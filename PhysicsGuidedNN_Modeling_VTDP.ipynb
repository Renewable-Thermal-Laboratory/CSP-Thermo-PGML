{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYuMZXny70b-",
        "outputId": "4154112d-47ef-483f-86d3-04dc889f11cc"
      },
      "outputs": [],
      "source": [
        "# cd drive/MyDrive/Colab\\ Notebooks/Theoretical_VTDP\n",
        "DATA_DIR = \"../data/Theoretical_VTDP\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rP-VCBDo9H-G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3T5Zu9hrZTz"
      },
      "source": [
        "# Combining All Data Files into One"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P4KvSmdeLeF2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg4-pTVqS1_x",
        "outputId": "b1ffd847-7691-4aec-9038-d481b221d6d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv\n",
            "h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv\n",
            "h6_flux88_abs0_surf1_491s - Sheet3_processed.csv\n",
            "h6_flux88_abs0_surf1_491s - Sheet3_processed.csv\n",
            "h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv\n",
            "h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv\n",
            "h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv\n",
            "h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv\n",
            "h6_flux88_abs20_surf1_613s - Sheet1_processed.csv\n",
            "h6_flux88_abs20_surf1_613s - Sheet1_processed.csv\n",
            "h6_flux88_abs0_surf0_800s - Sheet2_processed.csv\n",
            "h6_flux88_abs0_surf0_800s - Sheet2_processed.csv\n",
            "h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv\n",
            "h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv\n",
            "h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv\n",
            "h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv\n",
            "h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv\n",
            "h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv\n",
            "h6_flux88_abs92_surf0_598s - Sheet1_processed.csv\n",
            "h6_flux88_abs92_surf0_598s - Sheet1_processed.csv\n",
            "h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv\n",
            "h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv\n",
            "h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv\n",
            "h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv\n",
            "h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv\n",
            "h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv\n",
            "h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv\n",
            "h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv\n",
            "h6_flux88_abs20_surf0_699s - Sheet1_processed.csv\n",
            "h6_flux88_abs20_surf0_699s - Sheet1_processed.csv\n",
            "h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv\n",
            "h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv\n",
            "h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv\n",
            "h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv\n",
            "h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv\n",
            "h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv\n",
            "h6_flux73_abs0_surf0_868s - Sheet1_processed.csv\n",
            "h6_flux73_abs0_surf0_868s - Sheet1_processed.csv\n",
            "h6_flux73_abs0_surf1_754s - Sheet3_processed.csv\n",
            "h6_flux73_abs0_surf1_754s - Sheet3_processed.csv\n",
            "h6_flux88_abs0_surf1_790s - Sheet1_processed.csv\n",
            "h6_flux88_abs0_surf1_790s - Sheet1_processed.csv\n",
            "h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv\n",
            "h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv\n",
            "h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv\n",
            "h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv\n",
            "h6_flux73_abs0_surf1_868s - Sheet2_processed.csv\n",
            "h6_flux73_abs0_surf1_868s - Sheet2_processed.csv\n",
            "h3_flux88_abs0_surf0_644s-Sheet2_processed.csv\n",
            "h3_flux88_abs0_surf0_644s-Sheet2_processed.csv\n",
            "h3_flux88_abs0_surf0_719s-Sheet1_processed.csv\n",
            "h3_flux88_abs0_surf0_719s-Sheet1_processed.csv\n",
            "h6_flux88_abs0_surf0_800s - Sheet1_processed.csv\n",
            "h6_flux88_abs0_surf0_800s - Sheet1_processed.csv\n",
            "h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv\n",
            "h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv\n",
            "h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv\n",
            "h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv\n",
            "h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv\n",
            "h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv\n",
            "h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv\n",
            "h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv\n",
            "h2_flux88_abs0_surf0_431s - Sheet1_processed.csv\n",
            "h2_flux88_abs0_surf0_431s - Sheet1_processed.csv\n",
            "h3_flux88_abs90_surf0_747s-Sheet3_processed.csv\n",
            "h3_flux88_abs90_surf0_747s-Sheet3_processed.csv\n",
            "h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv\n",
            "h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv\n",
            "h2_flux88_abs0_surf0_585s - Sheet2_processed.csv\n",
            "h2_flux88_abs0_surf0_585s - Sheet2_processed.csv\n",
            "h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv\n",
            "h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv\n",
            "h6_flux88_abs0_surf0_389s - Sheet4_processed.csv\n",
            "h6_flux88_abs0_surf0_389s - Sheet4_processed.csv\n",
            "h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv\n",
            "h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv\n",
            "h6_flux88_abs0_surf1_545s - Sheet2_processed.csv\n",
            "h6_flux88_abs0_surf1_545s - Sheet2_processed.csv\n",
            "h2_flux88_abs92_surf0_115s - Sheet1_processed.csv\n",
            "h2_flux88_abs92_surf0_115s - Sheet1_processed.csv\n",
            "h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv\n",
            "h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv\n",
            "h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv\n",
            "h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv\n",
            "h6_flux78_abs0_surf0_745s - Sheet1_processed.csv\n",
            "h6_flux78_abs0_surf0_745s - Sheet1_processed.csv\n",
            "h6_flux88_abs20_surf0_389s - Sheet3_processed.csv\n",
            "h6_flux88_abs20_surf0_389s - Sheet3_processed.csv\n",
            "h3_flux88_abs25_surf0_439s-Sheet2_processed.csv\n",
            "h3_flux88_abs25_surf0_439s-Sheet2_processed.csv\n",
            "h3_flux88_abs90_surf0_749s-Sheet1_processed.csv\n",
            "h3_flux88_abs90_surf0_749s-Sheet1_processed.csv\n",
            "h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv\n",
            "h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv\n",
            "h6_flux88_abs20_surf0_651s - Sheet4_processed.csv\n",
            "h6_flux88_abs20_surf0_651s - Sheet4_processed.csv\n",
            "h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv\n",
            "h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv\n",
            "h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv\n",
            "h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv\n",
            "h2_flux88_abs25_surf0_493s - Sheet1_processed.csv\n",
            "h2_flux88_abs25_surf0_493s - Sheet1_processed.csv\n",
            "h6_flux88_abs20_surf1_830s - Sheet2_processed.csv\n",
            "h6_flux88_abs20_surf1_830s - Sheet2_processed.csv\n",
            "h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv\n",
            "h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv\n",
            "h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv\n",
            "h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv\n",
            "h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv\n",
            "h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv\n",
            "h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv\n",
            "h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv\n",
            "h3_flux88_abs0_surf0_825s-Sheer3_processed.csv\n",
            "h3_flux88_abs0_surf0_825s-Sheer3_processed.csv\n",
            "h6_flux88_abs0_surf0_689s - Sheet3_processed.csv\n",
            "h6_flux88_abs0_surf0_689s - Sheet3_processed.csv\n",
            "h3_flux88_abs25_surf0_660s-Sheet1_processed.csv\n",
            "h3_flux88_abs25_surf0_660s-Sheet1_processed.csv\n",
            "h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv\n",
            "h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv\n",
            "h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv\n",
            "h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv\n",
            "h6_flux78_abs0_surf0_523s - Sheet2_processed.csv\n",
            "h6_flux78_abs0_surf0_523s - Sheet2_processed.csv\n",
            "h6_flux73_abs0_surf1_660s - Sheet1_processed.csv\n",
            "h6_flux73_abs0_surf1_660s - Sheet1_processed.csv\n",
            "h6_flux88_abs92_surf0_628s - Sheet4_processed.csv\n",
            "h6_flux88_abs92_surf0_628s - Sheet4_processed.csv\n",
            "h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv\n",
            "h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv\n",
            "h6_flux73_abs0_surf0_511s - Sheet3_processed.csv\n",
            "h6_flux73_abs0_surf0_511s - Sheet3_processed.csv\n",
            "h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv\n",
            "h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv\n",
            "h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv\n",
            "h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv\n",
            "h3_flux88_abs90_surf0_692s-Sheet2_processed.csv\n",
            "h3_flux88_abs90_surf0_692s-Sheet2_processed.csv\n",
            "h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv\n",
            "h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv\n",
            "h6_flux88_abs20_surf0_781s - Sheet2_processed.csv\n",
            "h6_flux88_abs20_surf0_781s - Sheet2_processed.csv\n",
            "h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv\n",
            "h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv\n",
            "h6_flux78_abs0_surf0_450s - Sheet3_processed.csv\n",
            "h6_flux78_abs0_surf0_450s - Sheet3_processed.csv\n",
            "h6_flux88_abs92_surf0_648s - Sheet3_processed.csv\n",
            "h6_flux88_abs92_surf0_648s - Sheet3_processed.csv\n",
            "h6_flux88_abs92_surf0_630s - Sheet2_processed.csv\n",
            "h6_flux88_abs92_surf0_630s - Sheet2_processed.csv\n",
            "h6_flux73_abs0_surf0_429s - Sheet2_processed.csv\n",
            "h6_flux73_abs0_surf0_429s - Sheet2_processed.csv\n",
            "h6_flux88_abs20_surf1_675s - Sheet3_processed.csv\n",
            "h6_flux88_abs20_surf1_675s - Sheet3_processed.csv\n",
            "   Time  TC1_tip     TC2     TC3     TC4     TC5     TC6     TC7     TC8  \\\n",
            "0   762   359.97  359.35  359.98  360.20  359.55  361.57  358.99  359.88   \n",
            "1   763   360.02  359.44  359.96  360.20  359.55  361.55  359.16  360.04   \n",
            "2   764   360.12  359.60  360.00  360.26  359.58  361.59  359.36  360.33   \n",
            "3   765   360.24  359.76  360.07  360.34  359.64  361.64  359.55  360.59   \n",
            "4   766   360.38  359.92  360.13  360.47  359.70  361.73  359.73  360.83   \n",
            "\n",
            "      TC9  ...  Theoretical_Temps_10  Theoretical_Temps_11       h   flux  \\\n",
            "0  356.71  ...            334.837384            376.779889  0.1575  25900   \n",
            "1  357.16  ...            334.881585            376.824090  0.1575  25900   \n",
            "2  357.60  ...            334.925783            376.868287  0.1575  25900   \n",
            "3  358.04  ...            334.969976            376.912481  0.1575  25900   \n",
            "4  358.45  ...            335.014165            376.956670  0.1575  25900   \n",
            "\n",
            "   abs  surf  TC_9_5  TC_Bottom_rec_groove  TC_wall_ins_ext  \\\n",
            "0    3  0.98     NaN                   NaN              NaN   \n",
            "1    3  0.98     NaN                   NaN              NaN   \n",
            "2    3  0.98     NaN                   NaN              NaN   \n",
            "3    3  0.98     NaN                   NaN              NaN   \n",
            "4    3  0.98     NaN                   NaN              NaN   \n",
            "\n",
            "   TC_bottom_ins_groove  \n",
            "0                   NaN  \n",
            "1                   NaN  \n",
            "2                   NaN  \n",
            "3                   NaN  \n",
            "4                   NaN  \n",
            "\n",
            "[5 rows x 41 columns]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "pattern = r\"h(\\d+)_flux(\\d+)_abs(\\d+).*?_surf(\\d+).*?_(\\d+)s\"\n",
        "\n",
        "# % mapping tables for quick lookup\n",
        "h_map = {2:0.0375, 3:0.084, 6:0.1575}\n",
        "flux_map = {88: 25900, 78: 21250, 73: 19400}\n",
        "abs_val_map = {0: 3, 92: 100}\n",
        "surf_map = {0:0.98, 1:0.76}\n",
        "\n",
        "dataframes = []\n",
        "for filename in os.listdir(\"./\"):\n",
        "    if filename.endswith(\".csv\"):\n",
        "\n",
        "        match = re.search(pattern, filename)\n",
        "        if match:\n",
        "            h = int(match.group(1))\n",
        "            flux = int(match.group(2))\n",
        "            abs_val = int(match.group(3))\n",
        "            surf = int(match.group(4))\n",
        "            min_time = int(match.group(5))\n",
        "\n",
        "            # flux from 73 → 19.4, 88 → 25.9\n",
        "            if h in h_map:\n",
        "              h = h_map[h]\n",
        "            if flux in flux_map:\n",
        "              flux = flux_map[flux]\n",
        "\n",
        "            if abs_val in abs_val_map:\n",
        "              abs_val = abs_val_map[abs_val]\n",
        "\n",
        "            if surf in surf_map:\n",
        "              surf = surf_map[surf]\n",
        "\n",
        "\n",
        "\n",
        "            print(filename)\n",
        "            file_path = os.path.join(\"\", filename)\n",
        "            print(file_path)\n",
        "            data = pd.read_csv(file_path, encoding=\"utf-8-sig\")  # UTF-8 with BOM\n",
        "\n",
        "            if 'Time' in data.columns:\n",
        "                data = data[data['Time'] >= min_time].copy()\n",
        "\n",
        "            data[\"h\"] = h\n",
        "            data[\"flux\"] = flux\n",
        "            data[\"abs\"] = abs_val\n",
        "            data[\"surf\"] = surf\n",
        "\n",
        "            dataframes.append(data)\n",
        "\n",
        "# Combine all dataframes into one\n",
        "combined_data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "print(combined_data.head())\n",
        "\n",
        "combined_data.to_csv(\"PG_combined_data.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYMf52zCAsSR",
        "outputId": "05bc1f77-1894-4d5a-f58e-a63b9130efa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null Check:  Time                    0\n",
            "TC1_tip                 0\n",
            "TC2                     0\n",
            "TC3                     0\n",
            "TC4                     0\n",
            "TC5                     0\n",
            "TC6                     0\n",
            "TC7                     0\n",
            "TC8                     0\n",
            "TC9                     0\n",
            "TC10                    0\n",
            "Theoretical_Temps_1     0\n",
            "Theoretical_Temps_2     0\n",
            "Theoretical_Temps_3     0\n",
            "Theoretical_Temps_4     0\n",
            "Theoretical_Temps_5     0\n",
            "Theoretical_Temps_6     0\n",
            "Theoretical_Temps_7     0\n",
            "Theoretical_Temps_8     0\n",
            "Theoretical_Temps_9     0\n",
            "Theoretical_Temps_10    0\n",
            "h                       0\n",
            "flux                    0\n",
            "abs                     0\n",
            "surf                    0\n",
            "dtype: int64\n",
            "INPUTS: \n",
            "    Time       h   flux  abs  surf  Theoretical_Temps_1  Theoretical_Temps_2  \\\n",
            "0   762  0.1575  25900    3  0.98           313.462921           334.837384   \n",
            "1   763  0.1575  25900    3  0.98           313.502765           334.881585   \n",
            "2   764  0.1575  25900    3  0.98           313.542606           334.925783   \n",
            "3   765  0.1575  25900    3  0.98           313.582442           334.969976   \n",
            "4   766  0.1575  25900    3  0.98           313.622274           335.014165   \n",
            "\n",
            "   Theoretical_Temps_3  Theoretical_Temps_4  Theoretical_Temps_5  \\\n",
            "0           334.837384           334.837384           334.837384   \n",
            "1           334.881585           334.881585           334.881585   \n",
            "2           334.925783           334.925783           334.925783   \n",
            "3           334.969976           334.969976           334.969976   \n",
            "4           335.014165           335.014165           335.014165   \n",
            "\n",
            "   Theoretical_Temps_6  Theoretical_Temps_7  Theoretical_Temps_8  \\\n",
            "0           334.837384           334.837384           334.837384   \n",
            "1           334.881585           334.881585           334.881585   \n",
            "2           334.925783           334.925783           334.925783   \n",
            "3           334.969976           334.969976           334.969976   \n",
            "4           335.014165           335.014165           335.014165   \n",
            "\n",
            "   Theoretical_Temps_9  Theoretical_Temps_10  \n",
            "0           334.837384            334.837384  \n",
            "1           334.881585            334.881585  \n",
            "2           334.925783            334.925783  \n",
            "3           334.969976            334.969976  \n",
            "4           335.014165            335.014165  \n",
            "OUPUTS: \n",
            "    TC1_tip     TC2     TC3     TC4     TC5     TC6     TC7     TC8     TC9  \\\n",
            "0   359.97  359.35  359.98  360.20  359.55  361.57  358.99  359.88  356.71   \n",
            "1   360.02  359.44  359.96  360.20  359.55  361.55  359.16  360.04  357.16   \n",
            "2   360.12  359.60  360.00  360.26  359.58  361.59  359.36  360.33  357.60   \n",
            "3   360.24  359.76  360.07  360.34  359.64  361.64  359.55  360.59  358.04   \n",
            "4   360.38  359.92  360.13  360.47  359.70  361.73  359.73  360.83  358.45   \n",
            "\n",
            "     TC10  \n",
            "0  337.26  \n",
            "1  337.41  \n",
            "2  337.58  \n",
            "3  337.76  \n",
            "4  337.95  \n",
            "input size:  15\n",
            "output size:  10\n",
            "Amount of Training Data:  52916\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# so for this model, the training data will be\n",
        "# h (depth), flux (q0), abs (absorption coefficient k), surf (surface emisisivty epsilon), and varying time (tStep)\n",
        "# and y will be the thermal couples values FOR THAT TIME\n",
        "data = pd.read_csv(\"PG_combined_data.csv\")\n",
        "\n",
        "# removing these two columns, some data files have these, some don't. I'll worry about them later +\n",
        "# need to ask what they do again, idk why there is a 11th temp\n",
        "data.drop(columns=[\"TC_9_5\", \"TC_Bottom_rec_groove\",\"TC_wall_ins_ext\", \"TC_bottom_ins_groove\", \"Theoretical_Temps_11\"], axis=1, inplace=True)\n",
        "depth_cols = [c for c in data.columns if c.startswith(\"Depth_\")]\n",
        "\n",
        "\n",
        "data.drop(columns=depth_cols, inplace=True)\n",
        "\n",
        "# making sure no nulls in  other  rows after r emoving these two\n",
        "print(\"Null Check: \", data.isnull().sum())\n",
        "\n",
        "Theoretical_temps = [x for x in data.columns if x.startswith(\"Theoretical_Temps_\")]\n",
        "X = data[[\"Time\", \"h\", \"flux\", \"abs\", \"surf\"] + Theoretical_temps]\n",
        "print(\"INPUTS: \\n\", X.head())\n",
        "y = data.drop(columns=[\"Time\", \"h\", \"flux\", \"abs\", \"surf\"] + Theoretical_temps)\n",
        "print(\"OUPUTS: \\n\", y.head())\n",
        "\n",
        "y_columns = y.columns\n",
        "# MinMaxScaler -> normalization, standardize when data is not normal (not in our case)\n",
        "X_scaler = MinMaxScaler()\n",
        "y_scaler = MinMaxScaler()\n",
        "X = X_scaler.fit_transform(X)\n",
        "y = y_scaler.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "print(\"input size: \", input_size)\n",
        "\n",
        "output_size = y_train.shape[1]\n",
        "print(\"output size: \", output_size)\n",
        "\n",
        "print(\"Amount of Training Data: \", X_train.shape[0])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oofb_qxQvEPP",
        "outputId": "2e40d50a-2f26-49d3-811f-4d0971c830dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [50/500], Loss: 0.0405014343559742\n",
            "Epoch [100/500], Loss: 0.022291723638772964\n",
            "Epoch [150/500], Loss: 0.014255122281610966\n",
            "Epoch [200/500], Loss: 0.011769085191190243\n",
            "Epoch [250/500], Loss: 0.011114482767879963\n",
            "Epoch [300/500], Loss: 0.010584376752376556\n",
            "Epoch [350/500], Loss: 0.010056163184344769\n",
            "Epoch [400/500], Loss: 0.009541229344904423\n",
            "Epoch [450/500], Loss: 0.009085705503821373\n",
            "Epoch [500/500], Loss: 0.008708038367331028\n",
            "\n",
            "RMSE per output (°C):\n",
            "TC1_tip: 33.577 °C\n",
            "TC2: 33.319 °C\n",
            "TC3: 34.047 °C\n",
            "TC4: 36.990 °C\n",
            "TC5: 35.857 °C\n",
            "TC6: 34.020 °C\n",
            "TC7: 33.630 °C\n",
            "TC8: 34.467 °C\n",
            "TC9: 36.514 °C\n",
            "TC10: 38.978 °C\n",
            "\n",
            "Average RMSE across all outputs: 35.140 °C\n",
            "Model weights saved to 'thermal_model_weights.pth'\n"
          ]
        }
      ],
      "source": [
        "# basic neural net\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = NeuralNet(input_size, output_size)\n",
        "\n",
        "# coommon loss func n optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad() # resetting gradients\n",
        "    outputs = model(X_train)\n",
        "\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}\")\n",
        "\n",
        "# no more calculating grad descent/back propgation, only forward\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test).numpy()\n",
        "\n",
        "# change back predictions and true values\n",
        "y_pred_real = y_scaler.inverse_transform(y_pred)\n",
        "y_test_real = y_scaler.inverse_transform(y_test.numpy())\n",
        "\n",
        "# get RMSE per output\n",
        "rmse_per_output = np.sqrt(np.mean((y_pred_real - y_test_real)**2, axis=0))\n",
        "rmse_overall = np.mean(rmse_per_output)\n",
        "\n",
        "\n",
        "print(\"\\nRMSE per output (°C):\")\n",
        "for col, rmse in zip(y_columns, rmse_per_output):\n",
        "    print(f\"{col}: {rmse:.3f} °C\")\n",
        "\n",
        "print(f\"\\nAverage RMSE across all outputs: {rmse_overall:.3f} °C\")\n",
        "\n",
        "torch.save(model.state_dict(), \"thermal_model_weights.pth\")\n",
        "print(\"Model weights saved to 'thermal_model_weights.pth'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "170TXBzts9fS"
      },
      "source": [
        "# Using a Trained Model\n",
        "To use a trained NN, you need the weights as a .pt or .pth file AND the architecture of the NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyxUf5rBpOBY",
        "outputId": "b2cf7834-f4da-4de9-959c-89dff6b39e88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NeuralNet(\n",
              "  (fc1): Linear(in_features=15, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "input_size = 15\n",
        "output_size = y.shape[1]\n",
        "model = NeuralNet(input_size, output_size)\n",
        "model.load_state_dict(torch.load('thermal_model_weights.pth')) # load  weights\n",
        "model.eval() # eval mode, no trianing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "gU2c_fr5pPTQ",
        "outputId": "81645017-36db-48f7-c463-5aee360ce705"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "X has 5 features, but MinMaxScaler is expecting 15 features as input.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2ee9a78f1688>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1049.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1575\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25900\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.98\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# still have to scale d ata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnew_data_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2965\u001b[0;31m         \u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2967\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(estimator, X, reset)\u001b[0m\n\u001b[1;32m   2827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2830\u001b[0m             \u001b[0;34mf\"X has {n_features} features, but {estimator.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m             \u001b[0;34mf\"is expecting {estimator.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 5 features, but MinMaxScaler is expecting 15 features as input."
          ]
        }
      ],
      "source": [
        "# time (tStep), h (y), flux (q0), abs (k), surf (epsilon),\n",
        "new_data = [[1049.0, 0.1575, 25900, 3, 0.98]]\n",
        "# still have to scale d ata\n",
        "new_data = X_scaler.transform(new_data)\n",
        "new_data_tensor = torch.tensor(new_data, dtype=torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(new_data_tensor)\n",
        "\n",
        "# reverse normalziation to see the acutal outputs\n",
        "predictions = y_scaler.inverse_transform(predictions.numpy())\n",
        "print(\"Predictions:\", predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
