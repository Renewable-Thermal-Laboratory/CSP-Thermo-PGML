{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi_opgPH29S_",
        "outputId": "b5eb3ad0-75d7-4f51-9aa7-dbe553f967ac"
      },
      "outputs": [],
      "source": [
        "\n",
        "# cd drive/MyDrive/Colab\\ Notebooks/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnks7Rop3Bft",
        "outputId": "7b882ff3-0da1-4fa0-f44f-3aebc37525c3"
      },
      "outputs": [],
      "source": [
        "# ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mVGwbKW0Vn2"
      },
      "source": [
        "# Imports + Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HNxLFpgi0Vn4"
      },
      "outputs": [],
      "source": [
        "import os, re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5-TXbl4Y0Vn5"
      },
      "outputs": [],
      "source": [
        "# Set seeds for reproducibility\n",
        "import random\n",
        "SEED = 1\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior for CUDA\n",
        "torch.backends.cudnn.benchmark = False      # Disable auto-tuner that can introduce randomness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q96kJ_tT0Vn6"
      },
      "outputs": [],
      "source": [
        "# File Mapping\n",
        "h_map = {2: 0.0375, 3: 0.084, 6: 0.1575}\n",
        "flux_map = {88: 25900, 78: 21250, 73: 19400}\n",
        "abs_map = {0: 3, 92: 100}\n",
        "surf_map = {0: 0.98, 1: 0.76}\n",
        "pattern = r\"h(\\d+)_flux(\\d+)_abs(\\d+)(?:_[A-Za-z0-9]+)*_surf([01])(?:_[A-Za-z0-9]+)*[\\s_]+(\\d+)s\\b\"\n",
        "# Depending on where u store the data files\n",
        "# cd drive/MyDrive/Colab\\ Notebooks/Theoretical_VTDP\n",
        "DATA_DIR = \"../../data/Theoretical_VTDP\"\n",
        "# DATA_DIR = \"Theoretical_VTDP/\"\n",
        "DROP_COLS = [\"TC_9_5\", \"TC_Bottom_rec_groove\", \"TC_wall_ins_ext\", \"TC_bottom_ins_groove\", \"Theoretical_Temps_11\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEPwSUTU0Vn6"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s0OmPxTi0Vn7"
      },
      "outputs": [],
      "source": [
        "def parse_filename_params(filename):\n",
        "    m = re.search(pattern, filename)\n",
        "    if not m:\n",
        "        return None\n",
        "    h_raw    = int(m.group(1))\n",
        "    flux_raw = int(m.group(2))\n",
        "    abs_raw  = int(m.group(3))\n",
        "    surf_raw = int(m.group(4))\n",
        "    t        = int(m.group(5))\n",
        "\n",
        "    # keeping number if no entry exists\n",
        "    h    = h_map.get(h_raw,    h_raw)\n",
        "    flux = flux_map.get(flux_raw, flux_raw)\n",
        "    abs_ = abs_map.get(abs_raw,  abs_raw)\n",
        "    surf = surf_map.get(surf_raw)\n",
        "\n",
        "    return h, flux, abs_, surf, t\n",
        "\n",
        "def load_and_process_file(path, h, flux, abs_val, surf, min_time,filename):\n",
        "    df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
        "    df = df[df[\"Time\"] >= min_time].copy()\n",
        "    df.drop(columns=[col for col in df.columns if col in DROP_COLS or col.startswith(\"Depth_\")], inplace=True)\n",
        "    df[\"h\"] = h\n",
        "    df[\"flux\"] = flux\n",
        "    df[\"abs\"] = abs_val\n",
        "    df[\"surf\"] = surf\n",
        "    df[\"filename\"] = filename\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZon2gIG0Vn7"
      },
      "source": [
        "# Load and Combine Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJIZorgZ0Vn8",
        "outputId": "1b80bd5b-a6b3-4099-fa65-226c99bf2b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping: h2_flux88_abs25_wr_surfParAdded_169s - Sheet1_processed.csv\n",
            "Skipping: h2_flux88_abs25_wr_surfSimD_525s - Sheet1_processed.csv\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 66146 entries, 0 to 66145\n",
            "Data columns (total 25 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   Time                  66146 non-null  int64  \n",
            " 1   TC1_tip               66146 non-null  float64\n",
            " 2   TC2                   66146 non-null  float64\n",
            " 3   TC3                   66146 non-null  float64\n",
            " 4   TC4                   66146 non-null  float64\n",
            " 5   TC5                   66146 non-null  float64\n",
            " 6   TC6                   66146 non-null  float64\n",
            " 7   TC7                   66146 non-null  float64\n",
            " 8   TC8                   66146 non-null  float64\n",
            " 9   TC9                   66146 non-null  float64\n",
            " 10  TC10                  66146 non-null  float64\n",
            " 11  Theoretical_Temps_1   66146 non-null  float64\n",
            " 12  Theoretical_Temps_2   66146 non-null  float64\n",
            " 13  Theoretical_Temps_3   66146 non-null  float64\n",
            " 14  Theoretical_Temps_4   66146 non-null  float64\n",
            " 15  Theoretical_Temps_5   66146 non-null  float64\n",
            " 16  Theoretical_Temps_6   66146 non-null  float64\n",
            " 17  Theoretical_Temps_7   66146 non-null  float64\n",
            " 18  Theoretical_Temps_8   66146 non-null  float64\n",
            " 19  Theoretical_Temps_9   66146 non-null  float64\n",
            " 20  Theoretical_Temps_10  66146 non-null  float64\n",
            " 21  h                     66146 non-null  float64\n",
            " 22  flux                  66146 non-null  int64  \n",
            " 23  abs                   66146 non-null  int64  \n",
            " 24  surf                  66146 non-null  float64\n",
            "dtypes: float64(22), int64(3)\n",
            "memory usage: 12.6 MB\n",
            "None\n",
            "   Time  TC1_tip     TC2     TC3     TC4     TC5    TC6    TC7    TC8    TC9  \\\n",
            "0   571   362.19  358.26  333.06  183.66  124.89  82.55  64.62  52.62  42.62   \n",
            "1   572   362.24  358.30  332.63  183.57  124.75  82.47  64.61  52.71  42.62   \n",
            "2   573   362.55  358.94  332.55  183.51  124.68  82.44  64.79  52.91  42.80   \n",
            "3   574   362.82  359.44  332.70  183.64  124.69  82.52  65.04  53.29  43.01   \n",
            "4   575   363.11  359.92  332.86  183.88  124.72  82.53  65.26  53.63  43.24   \n",
            "\n",
            "   ...  Theoretical_Temps_5  Theoretical_Temps_6  Theoretical_Temps_7  \\\n",
            "0  ...           401.456645           401.456645           401.456645   \n",
            "1  ...           401.612500           401.612500           401.612500   \n",
            "2  ...           401.768277           401.768277           401.768277   \n",
            "3  ...           401.923974           401.923974           401.923974   \n",
            "4  ...           402.079593           402.079593           402.079593   \n",
            "\n",
            "   Theoretical_Temps_8  Theoretical_Temps_9  Theoretical_Temps_10       h  \\\n",
            "0           401.456645           401.456645            401.456645  0.0375   \n",
            "1           401.612500           401.612500            401.612500  0.0375   \n",
            "2           401.768277           401.768277            401.768277  0.0375   \n",
            "3           401.923974           401.923974            401.923974  0.0375   \n",
            "4           402.079593           402.079593            402.079593  0.0375   \n",
            "\n",
            "    flux  abs  surf  \n",
            "0  25900    3  0.98  \n",
            "1  25900    3  0.98  \n",
            "2  25900    3  0.98  \n",
            "3  25900    3  0.98  \n",
            "4  25900    3  0.98  \n",
            "\n",
            "[5 rows x 25 columns]\n"
          ]
        }
      ],
      "source": [
        "# dataframes = []\n",
        "# for fname in os.listdir(DATA_DIR):\n",
        "#     if not fname.endswith(\".csv\"):\n",
        "#         continue\n",
        "#     params = parse_filename_params(fname)\n",
        "#     if params is None or params[3] is None:\n",
        "#         print(\"Skipping:\", fname)\n",
        "#         continue\n",
        "#     path = os.path.join(DATA_DIR, fname)\n",
        "#     df = load_and_process_file(path, *params)\n",
        "#     dataframes.append(df)\n",
        "\n",
        "# data = pd.concat(dataframes, ignore_index=True)\n",
        "# print(data.info())\n",
        "# print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping (not h6): h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv\n",
            "Skipping (not h6): h2_flux88_abs0_surf0_431s - Sheet1_processed.csv\n",
            "Skipping (not h6): h2_flux88_abs0_surf0_585s - Sheet2_processed.csv\n",
            "Skipping (not h6): h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv\n",
            "Skipping (not h6): h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv\n",
            "Skipping (not h6): h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv\n",
            "Skipping (not h6): h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv\n",
            "Skipping (not h6): h2_flux88_abs25_surf0_493s - Sheet1_processed.csv\n",
            "Skipping (not h6): h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv\n",
            "Skipping (unmatched): h2_flux88_abs25_wr_surfParAdded_169s - Sheet1_processed.csv\n",
            "Skipping (unmatched): h2_flux88_abs25_wr_surfSimD_525s - Sheet1_processed.csv\n",
            "Skipping (not h6): h2_flux88_abs92_surf0_115s - Sheet1_processed.csv\n",
            "Skipping (not h6): h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv\n",
            "Skipping (not h6): h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs0_surf0_644s-Sheet2_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs0_surf0_719s-Sheet1_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs0_surf0_825s-Sheer3_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs25_surf0_439s-Sheet2_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs25_surf0_660s-Sheet1_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs90_surf0_692s-Sheet2_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs90_surf0_747s-Sheet3_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs90_surf0_749s-Sheet1_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv\n",
            "Skipping (not h6): h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 48233 entries, 0 to 48232\n",
            "Data columns (total 26 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   Time                  48233 non-null  int64  \n",
            " 1   TC1_tip               48233 non-null  float64\n",
            " 2   TC2                   48233 non-null  float64\n",
            " 3   TC3                   48233 non-null  float64\n",
            " 4   TC4                   48233 non-null  float64\n",
            " 5   TC5                   48233 non-null  float64\n",
            " 6   TC6                   48233 non-null  float64\n",
            " 7   TC7                   48233 non-null  float64\n",
            " 8   TC8                   48233 non-null  float64\n",
            " 9   TC9                   48233 non-null  float64\n",
            " 10  TC10                  48233 non-null  float64\n",
            " 11  Theoretical_Temps_1   48233 non-null  float64\n",
            " 12  Theoretical_Temps_2   48233 non-null  float64\n",
            " 13  Theoretical_Temps_3   48233 non-null  float64\n",
            " 14  Theoretical_Temps_4   48233 non-null  float64\n",
            " 15  Theoretical_Temps_5   48233 non-null  float64\n",
            " 16  Theoretical_Temps_6   48233 non-null  float64\n",
            " 17  Theoretical_Temps_7   48233 non-null  float64\n",
            " 18  Theoretical_Temps_8   48233 non-null  float64\n",
            " 19  Theoretical_Temps_9   48233 non-null  float64\n",
            " 20  Theoretical_Temps_10  48233 non-null  float64\n",
            " 21  h                     48233 non-null  float64\n",
            " 22  flux                  48233 non-null  int64  \n",
            " 23  abs                   48233 non-null  int64  \n",
            " 24  surf                  48233 non-null  float64\n",
            " 25  filename              48233 non-null  object \n",
            "dtypes: float64(22), int64(3), object(1)\n",
            "memory usage: 9.6+ MB\n",
            "None\n",
            "   Time  TC1_tip     TC2     TC3     TC4     TC5     TC6     TC7     TC8  \\\n",
            "0   429   359.39  358.73  358.61  359.15  358.48  360.28  358.15  358.81   \n",
            "1   430   359.41  358.78  358.62  359.17  358.48  360.30  358.28  359.01   \n",
            "2   431   359.48  358.90  358.67  359.21  358.49  360.33  358.45  359.24   \n",
            "3   432   359.55  359.03  358.74  359.29  358.53  360.38  358.65  359.43   \n",
            "4   433   359.65  359.22  358.84  359.39  358.62  360.46  358.90  359.61   \n",
            "\n",
            "      TC9  ...  Theoretical_Temps_6  Theoretical_Temps_7  Theoretical_Temps_8  \\\n",
            "0  356.29  ...           313.628513           313.628513           313.628513   \n",
            "1  356.33  ...           313.659721           313.659721           313.659721   \n",
            "2  356.47  ...           313.690927           313.690927           313.690927   \n",
            "3  356.65  ...           313.722129           313.722129           313.722129   \n",
            "4  356.93  ...           313.753329           313.753329           313.753329   \n",
            "\n",
            "   Theoretical_Temps_9  Theoretical_Temps_10       h   flux  abs  surf  \\\n",
            "0           313.628513            313.628513  0.1575  19400    3  0.98   \n",
            "1           313.659721            313.659721  0.1575  19400    3  0.98   \n",
            "2           313.690927            313.690927  0.1575  19400    3  0.98   \n",
            "3           313.722129            313.722129  0.1575  19400    3  0.98   \n",
            "4           313.753329            313.753329  0.1575  19400    3  0.98   \n",
            "\n",
            "                                           filename  \n",
            "0  h6_flux73_abs0_surf0_429s - Sheet2_processed.csv  \n",
            "1  h6_flux73_abs0_surf0_429s - Sheet2_processed.csv  \n",
            "2  h6_flux73_abs0_surf0_429s - Sheet2_processed.csv  \n",
            "3  h6_flux73_abs0_surf0_429s - Sheet2_processed.csv  \n",
            "4  h6_flux73_abs0_surf0_429s - Sheet2_processed.csv  \n",
            "\n",
            "[5 rows x 26 columns]\n"
          ]
        }
      ],
      "source": [
        "dataframes = []\n",
        "for fname in os.listdir(DATA_DIR):\n",
        "    if not fname.endswith(\".csv\"):\n",
        "        continue\n",
        "\n",
        "    params = parse_filename_params(fname)\n",
        "    if params is None or params[3] is None:\n",
        "        print(\"Skipping (unmatched):\", fname)\n",
        "        continue\n",
        "\n",
        "    h_val = params[0]\n",
        "    if h_val != h_map[6]:  # Only include h6 = 0.1575\n",
        "        print(\"Skipping (not h6):\", fname)\n",
        "        continue\n",
        "\n",
        "    path = os.path.join(DATA_DIR, fname)\n",
        "    df = load_and_process_file(path, *params,filename=fname)\n",
        "    dataframes.append(df)\n",
        "    data = pd.concat(dataframes, ignore_index=True)\n",
        "print(data.info())\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhvD7dmJ0Vn9"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kNe4WtPR0Vn-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Files: ['h6_flux88_abs20_surf1_675s - Sheet3_processed.csv', 'h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv', 'h6_flux73_abs0_surf0_429s - Sheet2_processed.csv', 'h6_flux88_abs20_surf0_389s - Sheet3_processed.csv', 'h6_flux88_abs92_surf0_598s - Sheet1_processed.csv', 'h6_flux73_abs0_surf0_511s - Sheet3_processed.csv', 'h6_flux88_abs0_surf1_790s - Sheet1_processed.csv', 'h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv', 'h6_flux88_abs20_surf1_613s - Sheet1_processed.csv', 'h6_flux73_abs0_surf1_754s - Sheet3_processed.csv', 'h6_flux88_abs0_surf0_800s - Sheet2_processed.csv', 'h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv', 'h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv', 'h6_flux88_abs92_surf0_630s - Sheet2_processed.csv', 'h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv', 'h6_flux88_abs20_surf0_781s - Sheet2_processed.csv', 'h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv', 'h6_flux88_abs0_surf0_800s - Sheet1_processed.csv', 'h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv', 'h6_flux88_abs0_surf0_689s - Sheet3_processed.csv', 'h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv', 'h6_flux73_abs0_surf1_868s - Sheet2_processed.csv', 'h6_flux73_abs0_surf1_660s - Sheet1_processed.csv', 'h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv', 'h6_flux88_abs0_surf1_491s - Sheet3_processed.csv', 'h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv', 'h6_flux78_abs0_surf0_450s - Sheet3_processed.csv', 'h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv', 'h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv', 'h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv', 'h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv', 'h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv', 'h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv', 'h6_flux88_abs0_surf0_389s - Sheet4_processed.csv', 'h6_flux78_abs0_surf0_745s - Sheet1_processed.csv', 'h6_flux88_abs92_surf0_648s - Sheet3_processed.csv', 'h6_flux88_abs20_surf0_699s - Sheet1_processed.csv', 'h6_flux88_abs0_surf1_545s - Sheet2_processed.csv', 'h6_flux73_abs0_surf0_868s - Sheet1_processed.csv', 'h6_flux78_abs0_surf0_523s - Sheet2_processed.csv', 'h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv', 'h6_flux88_abs92_surf0_628s - Sheet4_processed.csv', 'h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv', 'h6_flux88_abs20_surf1_830s - Sheet2_processed.csv', 'h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv', 'h6_flux88_abs20_surf0_651s - Sheet4_processed.csv']\n",
            "Val Files: ['h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv', 'h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv', 'h6_flux88_abs0_surf0_800s - Sheet2_processed.csv', 'h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv', 'h6_flux88_abs0_surf0_389s - Sheet4_processed.csv', 'h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv', 'h6_flux73_abs0_surf0_868s - Sheet1_processed.csv', 'h6_flux88_abs0_surf1_491s - Sheet3_processed.csv', 'h6_flux88_abs92_surf0_628s - Sheet4_processed.csv', 'h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv', 'h6_flux88_abs20_surf0_651s - Sheet4_processed.csv', 'h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv', 'h6_flux88_abs0_surf0_800s - Sheet1_processed.csv', 'h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv', 'h6_flux88_abs92_surf0_630s - Sheet2_processed.csv', 'h6_flux73_abs0_surf0_511s - Sheet3_processed.csv', 'h6_flux78_abs0_surf0_450s - Sheet3_processed.csv', 'h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv', 'h6_flux88_abs92_surf0_598s - Sheet1_processed.csv', 'h6_flux88_abs20_surf0_699s - Sheet1_processed.csv', 'h6_flux73_abs0_surf1_754s - Sheet3_processed.csv', 'h6_flux88_abs0_surf0_689s - Sheet3_processed.csv', 'h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv', 'h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv', 'h6_flux88_abs20_surf1_613s - Sheet1_processed.csv', 'h6_flux88_abs20_surf0_389s - Sheet3_processed.csv', 'h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv', 'h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv', 'h6_flux78_abs0_surf0_523s - Sheet2_processed.csv', 'h6_flux88_abs20_surf0_781s - Sheet2_processed.csv', 'h6_flux88_abs0_surf1_545s - Sheet2_processed.csv', 'h6_flux88_abs92_surf0_648s - Sheet3_processed.csv', 'h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv', 'h6_flux73_abs0_surf1_868s - Sheet2_processed.csv', 'h6_flux88_abs0_surf1_790s - Sheet1_processed.csv', 'h6_flux73_abs0_surf0_429s - Sheet2_processed.csv', 'h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv', 'h6_flux88_abs20_surf1_830s - Sheet2_processed.csv', 'h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv', 'h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv', 'h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv', 'h6_flux73_abs0_surf1_660s - Sheet1_processed.csv', 'h6_flux78_abs0_surf0_745s - Sheet1_processed.csv', 'h6_flux88_abs20_surf1_675s - Sheet3_processed.csv', 'h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv', 'h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv']\n",
            "Test Files: ['h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv', 'h6_flux73_abs0_surf0_868s - Sheet1_processed.csv', 'h6_flux88_abs0_surf0_800s - Sheet2_processed.csv', 'h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv', 'h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv', 'h6_flux73_abs0_surf0_429s - Sheet2_processed.csv', 'h6_flux88_abs20_surf1_830s - Sheet2_processed.csv', 'h6_flux88_abs0_surf1_545s - Sheet2_processed.csv', 'h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv', 'h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv', 'h6_flux88_abs20_surf0_699s - Sheet1_processed.csv', 'h6_flux88_abs0_surf0_389s - Sheet4_processed.csv', 'h6_flux88_abs0_surf0_689s - Sheet3_processed.csv', 'h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv', 'h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv', 'h6_flux78_abs0_surf0_745s - Sheet1_processed.csv', 'h6_flux88_abs0_surf1_491s - Sheet3_processed.csv', 'h6_flux88_abs20_surf0_651s - Sheet4_processed.csv', 'h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv', 'h6_flux73_abs0_surf1_868s - Sheet2_processed.csv', 'h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv', 'h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv', 'h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv', 'h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv', 'h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv', 'h6_flux88_abs92_surf0_598s - Sheet1_processed.csv', 'h6_flux88_abs92_surf0_628s - Sheet4_processed.csv', 'h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv', 'h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv', 'h6_flux88_abs20_surf0_781s - Sheet2_processed.csv', 'h6_flux88_abs92_surf0_630s - Sheet2_processed.csv', 'h6_flux78_abs0_surf0_450s - Sheet3_processed.csv', 'h6_flux73_abs0_surf1_754s - Sheet3_processed.csv', 'h6_flux88_abs92_surf0_648s - Sheet3_processed.csv', 'h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv', 'h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv', 'h6_flux88_abs20_surf1_675s - Sheet3_processed.csv', 'h6_flux88_abs0_surf1_790s - Sheet1_processed.csv', 'h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv', 'h6_flux88_abs0_surf0_800s - Sheet1_processed.csv', 'h6_flux73_abs0_surf0_511s - Sheet3_processed.csv', 'h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv', 'h6_flux78_abs0_surf0_523s - Sheet2_processed.csv', 'h6_flux73_abs0_surf1_660s - Sheet1_processed.csv', 'h6_flux88_abs20_surf1_613s - Sheet1_processed.csv', 'h6_flux88_abs20_surf0_389s - Sheet3_processed.csv']\n"
          ]
        }
      ],
      "source": [
        "# 1. Extract input (X) and output (y) columns\n",
        "theory_cols = [c for c in data.columns if c.startswith(\"Theoretical_Temps_\")]\n",
        "# Store filename separately\n",
        "filenames = data[\"filename\"].reset_index(drop=True)\n",
        "\n",
        "# Drop filename before assigning to X and y\n",
        "X = data[[\"Time\", \"h\", \"flux\", \"abs\", \"surf\"] + theory_cols].copy()\n",
        "X[\"filename\"] = filenames  # only attach to X, not y\n",
        "\n",
        "y = data.drop(columns=[\"Time\", \"h\", \"flux\", \"abs\", \"surf\"] + theory_cols + [\"filename\"], errors='ignore')\n",
        "\n",
        "\n",
        "X_train_raw, X_temp_raw, y_train_raw, y_temp_raw = train_test_split(\n",
        "    X.drop(columns=\"filename\"), y, test_size=0.3, random_state=SEED\n",
        ")\n",
        "X_val_raw, X_test_raw, y_val_raw, y_test_raw = train_test_split(\n",
        "    X_temp_raw, y_temp_raw, test_size=0.5, random_state=SEED\n",
        ")\n",
        "\n",
        "train_filenames = X.loc[X_train_raw.index, \"filename\"].unique().tolist()\n",
        "val_filenames   = X.loc[X_val_raw.index, \"filename\"].unique().tolist()\n",
        "test_filenames  = X.loc[X_test_raw.index, \"filename\"].unique().tolist()\n",
        "\n",
        "print(\"Train Files:\", train_filenames)\n",
        "print(\"Val Files:\", val_filenames)\n",
        "print(\"Test Files:\", test_filenames)\n",
        "\n",
        "\n",
        "# 2. Split into train, val, test before scaling\n",
        "# X_train_raw, X_temp_raw, y_train_raw, y_temp_raw = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
        "# X_val_raw, X_test_raw, y_val_raw, y_test_raw = train_test_split(X_temp_raw, y_temp_raw, test_size=0.5, random_state=SEED)\n",
        "\n",
        "# 3. Scale using only training set statistics\n",
        "X_scaler = MinMaxScaler()\n",
        "X_train_scaled = X_scaler.fit_transform(X_train_raw)\n",
        "X_val_scaled   = X_scaler.transform(X_val_raw)\n",
        "X_test_scaled  = X_scaler.transform(X_test_raw)\n",
        "\n",
        "y_scaler = MinMaxScaler()\n",
        "y_train_scaled = y_scaler.fit_transform(y_train_raw)\n",
        "y_val_scaled   = y_scaler.transform(y_val_raw)\n",
        "y_test_scaled  = y_scaler.transform(y_test_raw)\n",
        "\n",
        "joblib.dump(X_scaler, \"X_scaler.pkl\")\n",
        "joblib.dump(y_scaler, \"y_scaler.pkl\")\n",
        "\n",
        "# 4. Convert to PyTorch tensors for training\n",
        "X_train = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "X_val   = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
        "X_test  = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "\n",
        "y_train = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
        "y_val   = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
        "y_test  = torch.tensor(y_test_scaled, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqsJsRau0Vn_"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XYdshwqi0Vn_"
      },
      "outputs": [],
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(256, 128),  # Extra hidden layer added\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(128, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "model = NeuralNet(input_size, output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyN5g_Pe0Vn_"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt-6ScmG0VoA",
        "outputId": "c1d2ce0c-3cc6-405a-c186-4a12bf6e7cc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [50/500], Train Loss: 0.035900, Val Loss: 0.034571\n",
            "Epoch [100/500], Train Loss: 0.027925, Val Loss: 0.020330\n",
            "Epoch [150/500], Train Loss: 0.025020, Val Loss: 0.018929\n",
            "Epoch [200/500], Train Loss: 0.023317, Val Loss: 0.018317\n",
            "Epoch [250/500], Train Loss: 0.022364, Val Loss: 0.017795\n",
            "Epoch [300/500], Train Loss: 0.021683, Val Loss: 0.017329\n",
            "Epoch [350/500], Train Loss: 0.021199, Val Loss: 0.017204\n",
            "Epoch [400/500], Train Loss: 0.020740, Val Loss: 0.016849\n",
            "Epoch [450/500], Train Loss: 0.020533, Val Loss: 0.016641\n",
            "Epoch [500/500], Train Loss: 0.020288, Val Loss: 0.016865\n"
          ]
        }
      ],
      "source": [
        "# ✅ Define loss and optimizer\n",
        "mse_loss = nn.MSELoss()\n",
        "mae_loss = nn.L1Loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "EPOCHS = 500\n",
        "\n",
        "# ✅ For storing loss per epoch\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# ✅ Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass + loss\n",
        "    predictions = model(X_train)\n",
        "    train_loss = 0.7 * mse_loss(predictions, y_train) + 0.3 * mae_loss(predictions, y_train)\n",
        "    \n",
        "    # Backward pass + optimization\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation loss\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_preds = model(X_val)\n",
        "        val_loss = 0.7 * mse_loss(val_preds, y_val) + 0.3 * mae_loss(val_preds, y_val)\n",
        "\n",
        "    # Save losses\n",
        "    train_losses.append(train_loss.item())\n",
        "    val_losses.append(val_loss.item())\n",
        "\n",
        "    # Print progress every 50 epochs\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_loss.item():.6f}, Val Loss: {val_loss.item():.6f}\")\n",
        "torch.save(model.state_dict(), \"thermal_model_weights.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6RMSCgq0VoA"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ltk47Zh-0VoB",
        "outputId": "e8fc3bc0-07ba-4f71-b206-15ac19047b28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RMSE per output (°C):\n",
            "TC1_tip: 37.471 °C\n",
            "TC2: 37.215 °C\n",
            "TC3: 37.154 °C\n",
            "TC4: 36.826 °C\n",
            "TC5: 36.569 °C\n",
            "TC6: 36.505 °C\n",
            "TC7: 37.009 °C\n",
            "TC8: 37.505 °C\n",
            "TC9: 38.482 °C\n",
            "TC10: 38.353 °C\n",
            "\n",
            "Average RMSE across all outputs: 37.309 °C\n",
            "\n",
            "MAE per output (°C):\n",
            "TC1_tip: 11.274 °C\n",
            "TC2: 10.807 °C\n",
            "TC3: 11.589 °C\n",
            "TC4: 9.843 °C\n",
            "TC5: 8.994 °C\n",
            "TC6: 9.677 °C\n",
            "TC7: 10.563 °C\n",
            "TC8: 11.018 °C\n",
            "TC9: 11.964 °C\n",
            "TC10: 14.795 °C\n",
            "\n",
            "Average MAE across all outputs: 11.053 °C\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds = model(X_test).numpy()\n",
        "\n",
        "preds_real = y_scaler.inverse_transform(preds)\n",
        "y_real = y_scaler.inverse_transform(y_test.numpy())\n",
        "rmse = np.sqrt(np.mean((preds_real - y_real) ** 2, axis=0))\n",
        "\n",
        "print(\"\\nRMSE per output (°C):\")\n",
        "for col, val in zip(y.columns, rmse):\n",
        "    print(f\"{col}: {val:.3f} °C\")\n",
        "\n",
        "print(f\"\\nAverage RMSE across all outputs: {np.mean(rmse):.3f} °C\")\n",
        "\n",
        "mae = np.mean(np.abs(preds_real - y_real), axis=0)\n",
        "\n",
        "print(\"\\nMAE per output (°C):\")\n",
        "for col, val in zip(y.columns, mae):\n",
        "    print(f\"{col}: {val:.3f} °C\")\n",
        "\n",
        "print(f\"\\nAverage MAE across all outputs: {np.mean(mae):.3f} °C\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pMWK_uV0VoC"
      },
      "source": [
        "<!-- ----------------------------------------------------------------------------------------------------------------------------------- -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Train Loss', color='blue')\n",
        "plt.plot(val_losses, label='Validation Loss', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Predicted vs Actual Temperatures for 5 samples from the test set\n",
        "num_samples = 5\n",
        "for idx in range(num_samples):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(y_real[idx], label='Actual', marker='o')\n",
        "    plt.plot(preds_real[idx], label='Predicted', marker='x')\n",
        "    plt.title(f'Predicted vs Actual Temperatures (Sample {idx})')\n",
        "    plt.xlabel('Sensor Index')\n",
        "    plt.ylabel('Temperature (°C)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Scatterplot\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(y_real.flatten(), preds_real.flatten(), alpha=0.4)\n",
        "plt.plot([y_real.min(), y_real.max()], [y_real.min(), y_real.max()], color='red', linestyle='--')\n",
        "plt.xlabel(\"Actual Temperature (°C)\")\n",
        "plt.ylabel(\"Predicted Temperature (°C)\")\n",
        "plt.title(\"Scatterplot of Predicted vs Actual Temperatures (All Data)\")\n",
        "plt.grid(True)\n",
        "plt.axis(\"equal\")\n",
        "plt.show()\n",
        "\n",
        "#Residuals (Error) Plot\n",
        "residuals = preds_real - y_real\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(range(len(residuals.flatten())), residuals.flatten(), alpha=0.4)\n",
        "plt.hlines(0, 0, len(residuals.flatten()), colors='red', linestyles='--')\n",
        "plt.title(\"Residuals (Predicted - Actual) for All Data\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Residual (°C)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#Residual Distribution Histogram\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(residuals.flatten(), bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
        "plt.title(\"Distribution of Residuals\")\n",
        "plt.xlabel(\"Residual (°C)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#Bar Chart: RMSE per Sensor\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(y.columns, rmse, color='skyblue', edgecolor='black')\n",
        "plt.title(\"RMSE per Output Sensor\")\n",
        "plt.ylabel(\"RMSE (°C)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n",
        "\n",
        "#Bar Chart: MAE per Sensor\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(y.columns, mae, color='orange', edgecolor='black')\n",
        "plt.title(\"MAE per Output Sensor\")\n",
        "plt.ylabel(\"MAE (°C)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_and_clip_input(input_df, scaler):\n",
        "    min_vals = scaler.data_min_\n",
        "    max_vals = scaler.data_max_\n",
        "    columns = scaler.feature_names_in_\n",
        "    clipped = False\n",
        "    for i, col in enumerate(columns):\n",
        "        val = input_df.iloc[0, i]\n",
        "        if val < min_vals[i] or val > max_vals[i]:\n",
        "            print(f\"Warning: '{col}' value {val} is out of range [{min_vals[i]}, {max_vals[i]}]. Clipping.\")\n",
        "            input_df.iloc[0, i] = min(max(val, min_vals[i]), max_vals[i])\n",
        "            clipped = True\n",
        "    if not clipped:\n",
        "        print(\"All input values are within the training range.\")\n",
        "    return input_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All input values are within the training range.\n",
            "Model loaded. First layer weights (sample): tensor([ 0.1330, -0.1140, -0.0501,  0.1212, -0.2431], grad_fn=<SliceBackward0>)\n",
            "\n",
            "Predicted Temperatures: [[353.93594 351.8109  352.25842 354.5042  355.66156 354.88785 354.08032\n",
            "  357.99487 353.97968 336.12445]]\n"
          ]
        }
      ],
      "source": [
        "new_input = [\n",
        "    [  # Full input vector for inference\n",
        "        868,     # Time (seconds)\n",
        "        0.1575,     # Depth (h)\n",
        "        19400,      # Heat flux (q0)\n",
        "        3,          # Absorption coefficient (abs)\n",
        "        0.98        # Surface emissivity (surf)\n",
        "    ] + [\n",
        "        306.456710321429,327.074235329552,327.074235329552,327.074235329552,327.074235329552,327.074235329552,327.074235329552,327.074235329552,327.074235329552,327.074235329552\n",
        "    ]\n",
        "    \n",
        "    ]\n",
        "\n",
        "X_scaler = joblib.load(\"X_scaler.pkl\")\n",
        "y_scaler = joblib.load(\"y_scaler.pkl\")\n",
        "\n",
        "# Create DataFrame with correct columns\n",
        "theory_cols = [f\"Theoretical_Temps_{i+1}\" for i in range(10)]\n",
        "input_columns = [\"Time\", \"h\", \"flux\", \"abs\", \"surf\"] + theory_cols\n",
        "new_input_df = pd.DataFrame(new_input, columns=input_columns)\n",
        "\n",
        "# Clip out-of-range values before scaling\n",
        "new_input_df = check_and_clip_input(new_input_df, X_scaler)\n",
        "\n",
        "new_input_scaled = X_scaler.transform(new_input_df)\n",
        "new_tensor = torch.tensor(new_input_scaled, dtype=torch.float32)\n",
        "\n",
        "input_size = len(new_input[0])               # Should be 15\n",
        "output_size = len(y_scaler.scale_)           # Should be 10 if you're predicting TC1–TC10\n",
        "model = NeuralNet(input_size, output_size)\n",
        "model.load_state_dict(torch.load(\"thermal_model_weights.pth\"))\n",
        "print(\"Model loaded. First layer weights (sample):\", model.net[0].weight[0][:5])\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred = model(new_tensor).numpy()\n",
        "    \n",
        "real_pred = y_scaler.inverse_transform(pred)\n",
        "print(\"\\nPredicted Temperatures:\", real_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- 360.23\t359.53\t359.49\t359.84\t359.06\t360.78\t358.57\t359.13\t356.75\t333.78 -->"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
