{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7f64754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: Imports and Setup\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35101844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Configuration\n",
    "def get_data_directory():\n",
    "    \"\"\"Find the data directory dynamically\"\"\"\n",
    "    possible_paths = [\n",
    "        \"data/new_processed_reset\",\n",
    "        \"./data/new_processed_reset\", \n",
    "        \"../data/new_processed_reset\",\n",
    "        \"data\",\n",
    "        \"./data\",\n",
    "        \"../data\",\n",
    "        \"D:/Research Assistant work/Github Organization/ml models/ml_models/PhysicsGuidedNeuralNetwork/data/new_processed_reset\"\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Found data directory: {path}\")\n",
    "            return path\n",
    "    \n",
    "    # If no directory found, ask user to specify\n",
    "    print(\"Data directory not found. Please specify the path to your data directory:\")\n",
    "    user_path = input(\"Enter data directory path: \").strip()\n",
    "    if os.path.exists(user_path):\n",
    "        return user_path\n",
    "    else:\n",
    "        raise ValueError(f\"Specified data directory {user_path} does not exist!\")\n",
    "\n",
    "DROP_COLS = [\"TC_9_5\", \"TC_Bottom_rec_groove\", \"TC_wall_ins_ext\", \"TC_bottom_ins_groove\", \"Theoretical_Temps_11\"]\n",
    "\n",
    "# File mapping dictionaries\n",
    "h_map = {2: 0.0375, 3: 0.084, 6: 0.1575}\n",
    "flux_map = {88: 25900, 78: 21250, 73: 19400}\n",
    "abs_map = {0: 3, 92: 100}\n",
    "surf_map = {0: 0.98, 1: 0.76}\n",
    "pattern = r\"h(\\d+)_flux(\\d+)_abs(\\d+)(?:_[A-Za-z0-9]+)*_surf([01])(?:_[A-Za-z0-9]+)*[\\s_]+(\\d+)s\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32497ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Data Loading Functions\n",
    "def parse_filename_params(filename):\n",
    "    \"\"\"Parse filename to extract thermal parameters\"\"\"\n",
    "    m = re.search(pattern, filename)\n",
    "    if not m:\n",
    "        return None\n",
    "    h_raw = int(m.group(1))\n",
    "    flux_raw = int(m.group(2))\n",
    "    abs_raw = int(m.group(3))\n",
    "    surf_raw = int(m.group(4))\n",
    "    t = int(m.group(5))\n",
    "\n",
    "    h = h_map.get(h_raw, h_raw)\n",
    "    flux = flux_map.get(flux_raw, flux_raw)\n",
    "    abs_ = abs_map.get(abs_raw, abs_raw)\n",
    "    surf = surf_map.get(surf_raw)\n",
    "\n",
    "    return h, flux, abs_, surf, t\n",
    "\n",
    "def load_and_process_file(path, h, flux, abs_val, surf, filename, min_time=0):\n",
    "    \"\"\"Load and process individual CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "        df = df[df[\"Time\"] >= min_time].copy()\n",
    "        \n",
    "        # Drop unwanted columns\n",
    "        df.drop(columns=[col for col in df.columns if col in DROP_COLS or col.startswith(\"Depth_\")], \n",
    "                inplace=True, errors='ignore')\n",
    "        \n",
    "        # Add parameters\n",
    "        df[\"h\"] = h\n",
    "        df[\"flux\"] = flux\n",
    "        df[\"abs\"] = abs_val\n",
    "        df[\"surf\"] = surf\n",
    "        df[\"filename\"] = filename\n",
    "        \n",
    "        return df.iloc[1:] if len(df) > 1 else df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_data(data_dir, h_filter=None, min_time=0):\n",
    "    \"\"\"Load and combine all data files\"\"\"\n",
    "    dataframes = []\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        raise ValueError(f\"Data directory {data_dir} does not exist!\")\n",
    "    \n",
    "    print(f\"Loading data from: {data_dir}\")\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "    if not csv_files:\n",
    "        raise ValueError(\"No CSV files found in the data directory!\")\n",
    "    \n",
    "    for fname in csv_files:\n",
    "        params = parse_filename_params(fname)\n",
    "        if params is None or params[3] is None:\n",
    "            print(f\"Skipping (unmatched): {fname}\")\n",
    "            continue\n",
    "\n",
    "        # Apply h filter if specified\n",
    "        if h_filter is not None:\n",
    "            h_val = params[0]\n",
    "            if h_val != h_filter:\n",
    "                print(f\"Skipping (not h={h_filter}): {fname}\")\n",
    "                continue\n",
    "\n",
    "        path = os.path.join(data_dir, fname)\n",
    "        try:\n",
    "            df = load_and_process_file(path, *params[:4], filename=fname, min_time=min_time)\n",
    "            if not df.empty:\n",
    "                dataframes.append(df)\n",
    "                print(f\"Loaded: {fname} ({len(df)} rows)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {fname}: {e}\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        raise ValueError(\"No valid data files found after processing!\")\n",
    "    \n",
    "    data = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"\\nTotal data loaded: {len(data)} rows, {len(data.columns)} columns\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5af29f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4: Model Architecture\n",
    "class EnhancedThermalNet(nn.Module):\n",
    "    \"\"\"Enhanced thermal neural network with attention and residual connections\"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_dims=[512, 256, 256, 128], dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Input processing\n",
    "        self.input_layer = nn.Linear(input_size, hidden_dims[0])\n",
    "        self.input_norm = nn.LayerNorm(hidden_dims[0])\n",
    "        \n",
    "        # Feature attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_size, max(1, input_size // 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(max(1, input_size // 2), input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Hidden layers with residual connections\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.residual_projections = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            layer = nn.Linear(hidden_dims[i], hidden_dims[i + 1])\n",
    "            norm = nn.LayerNorm(hidden_dims[i + 1])\n",
    "            \n",
    "            if hidden_dims[i] != hidden_dims[i + 1]:\n",
    "                residual_proj = nn.Linear(hidden_dims[i], hidden_dims[i + 1])\n",
    "            else:\n",
    "                residual_proj = nn.Identity()\n",
    "            \n",
    "            self.layers.append(layer)\n",
    "            self.norms.append(norm)\n",
    "            self.residual_projections.append(residual_proj)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_size)\n",
    "        \n",
    "        # Activation and regularization\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using He initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply attention to input features\n",
    "        attention_weights = self.attention(x)\n",
    "        x_attended = x * attention_weights\n",
    "        \n",
    "        # Input processing\n",
    "        x = self.activation(self.input_norm(self.input_layer(x_attended)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Forward through hidden layers with residual connections\n",
    "        for i, (layer, norm, residual_proj) in enumerate(zip(self.layers, self.norms, self.residual_projections)):\n",
    "            residual = residual_proj(x)\n",
    "            x = layer(x)\n",
    "            x = norm(x)\n",
    "            x = self.activation(x)\n",
    "            x = x + residual\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c4035d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedLoss(nn.Module):\n",
    "    \"\"\"Physics-informed loss function with energy conservation constraint\"\"\"\n",
    "    def __init__(self, smoothness_weight=0.001, gradient_weight=0.0001, physics_weight=0.01):\n",
    "        super().__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.smoothness_weight = smoothness_weight\n",
    "        self.gradient_weight = gradient_weight\n",
    "        self.physics_weight = physics_weight\n",
    "        \n",
    "        # Physical constants\n",
    "        self.r = 2.0375 * 0.0254  # Convert inches to meters\n",
    "        self.A_rec = np.pi * (self.r ** 2)  # Receiver area\n",
    "        self.rho = 1836.31  # Density (kg/m³)\n",
    "        self.cp = 1512  # Specific heat capacity (J/kg·K)\n",
    "        \n",
    "        # Convert to tensors for GPU compatibility\n",
    "        self.A_rec_tensor = None\n",
    "        self.rho_tensor = None\n",
    "        self.cp_tensor = None\n",
    "    \n",
    "    def _init_tensors(self, device):\n",
    "        \"\"\"Initialize tensors on the correct device\"\"\"\n",
    "        if self.A_rec_tensor is None:\n",
    "            self.A_rec_tensor = torch.tensor(self.A_rec, device=device, dtype=torch.float32)\n",
    "            self.rho_tensor = torch.tensor(self.rho, device=device, dtype=torch.float32)\n",
    "            self.cp_tensor = torch.tensor(self.cp, device=device, dtype=torch.float32)\n",
    "    \n",
    "    def compute_physics_loss(self, predictions, targets, inputs):\n",
    "        \"\"\"\n",
    "        Compute physics-informed loss based on energy conservation\n",
    "        \n",
    "        Args:\n",
    "            predictions: Model predictions (batch_size, num_sensors)\n",
    "            targets: Target temperatures (batch_size, num_sensors)\n",
    "            inputs: Input features (batch_size, input_dim)\n",
    "                   Expected to contain: [Time_norm, Time², Time³, Time_sin, Time_cos, \n",
    "                                       h, flux, abs, surf, flux_abs_interaction, h_flux_interaction, \n",
    "                                       theoretical_temps...]\n",
    "        \"\"\"\n",
    "        device = predictions.device\n",
    "        self._init_tensors(device)\n",
    "        \n",
    "        batch_size = predictions.shape[0]\n",
    "        \n",
    "        # Extract relevant features from inputs\n",
    "        # Based on your preprocessing, the order should be:\n",
    "        # [Time_norm, Time², Time³, Time_sin, Time_cos, h, flux, abs, surf, flux_abs_interaction, h_flux_interaction, theoretical_temps...]\n",
    "        \n",
    "        try:\n",
    "            # Extract flux and h from inputs\n",
    "            flux = inputs[:, 6]  # flux is at index 6\n",
    "            h = inputs[:, 5]     # h is at index 5\n",
    "            \n",
    "            # Calculate incoming energy\n",
    "            # incoming_energy = q0 * A_rec = flux * A_rec\n",
    "            incoming_energy = flux * self.A_rec_tensor  # Shape: (batch_size,)\n",
    "            \n",
    "            # Calculate mass for each batch\n",
    "            # mass = rho * h * pi * r^2 = rho * h * A_rec\n",
    "            mass = self.rho_tensor * h * self.A_rec_tensor  # Shape: (batch_size,)\n",
    "            \n",
    "            # For physics loss, we need to compute temperature derivatives\n",
    "            # Since we don't have explicit time derivatives in batch training,\n",
    "            # we'll approximate using the difference between predicted and target temperatures\n",
    "            # This represents the temperature change that should be consistent with energy balance\n",
    "            \n",
    "            # Average temperature change across all sensors\n",
    "            temp_change = torch.mean(predictions - targets, dim=1)  # Shape: (batch_size,)\n",
    "            \n",
    "            # Assume unit time step (dt = 1) for simplification\n",
    "            # In practice, you might want to pass actual time differences\n",
    "            dt = torch.ones_like(temp_change)\n",
    "            \n",
    "            # Calculate total energy stored\n",
    "            # total_energy_stored = mass * cp * (Temp_final - Temp_initial) / (time_final - time_initial)\n",
    "            # Here we approximate with: mass * cp * temp_change / dt\n",
    "            total_energy_stored = mass * self.cp_tensor * temp_change / dt  # Shape: (batch_size,)\n",
    "            \n",
    "            # Physics constraint: incoming_energy should equal total_energy_stored\n",
    "            # Physics loss = |incoming_energy - total_energy_stored|\n",
    "            physics_loss = torch.mean(torch.abs(incoming_energy - total_energy_stored))\n",
    "            \n",
    "            return physics_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Physics loss computation failed: {e}\")\n",
    "            # Return zero physics loss if computation fails\n",
    "            return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "    \n",
    "    def forward(self, predictions, targets, inputs=None):\n",
    "        \"\"\"\n",
    "        Forward pass with combined loss\n",
    "        \n",
    "        Args:\n",
    "            predictions: Model predictions\n",
    "            targets: Target temperatures\n",
    "            inputs: Input features (required for physics loss)\n",
    "        \"\"\"\n",
    "        # Primary MSE loss\n",
    "        mse_loss = self.mse_loss(predictions, targets)\n",
    "        \n",
    "        # Smoothness penalty (spatial consistency)\n",
    "        if predictions.shape[1] > 1:\n",
    "            smoothness_loss = torch.mean(torch.abs(predictions[:, 1:] - predictions[:, :-1]))\n",
    "            gradient_loss = torch.mean(torch.abs(torch.diff(predictions, dim=1)))\n",
    "        else:\n",
    "            smoothness_loss = torch.tensor(0.0, device=predictions.device)\n",
    "            gradient_loss = torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "        # Physics loss\n",
    "        if inputs is not None:\n",
    "            physics_loss = self.compute_physics_loss(predictions, targets, inputs)\n",
    "        else:\n",
    "            physics_loss = torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = (mse_loss + \n",
    "                     self.smoothness_weight * smoothness_loss + \n",
    "                     self.gradient_weight * gradient_loss +\n",
    "                     self.physics_weight * physics_loss)\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82143980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: Data Preprocessing\n",
    "def preprocess_data(data, test_size=0.3, val_size=0.5, use_enhanced_features=True):\n",
    "    \"\"\"Enhanced data preprocessing\"\"\"\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    \n",
    "    # Time normalization\n",
    "    time_min = data[\"Time\"].min()\n",
    "    time_max = data[\"Time\"].max()\n",
    "    data[\"Time_norm\"] = (data[\"Time\"] - time_min) / (time_max - time_min)\n",
    "    \n",
    "    # Enhanced time features\n",
    "    if use_enhanced_features:\n",
    "        data[\"Time²\"] = data[\"Time_norm\"] ** 2\n",
    "        data[\"Time³\"] = data[\"Time_norm\"] ** 3\n",
    "        data[\"Time_sin\"] = np.sin(2 * np.pi * data[\"Time_norm\"])\n",
    "        data[\"Time_cos\"] = np.cos(2 * np.pi * data[\"Time_norm\"])\n",
    "        \n",
    "        # Feature interaction terms\n",
    "        data[\"flux_abs_interaction\"] = data[\"flux\"] * data[\"abs\"]\n",
    "        data[\"h_flux_interaction\"] = data[\"h\"] * data[\"flux\"]\n",
    "        \n",
    "        # Enhanced feature set\n",
    "        base_features = [\"Time_norm\", \"Time²\", \"Time³\", \"Time_sin\", \"Time_cos\", \n",
    "                        \"h\", \"flux\", \"abs\", \"surf\", \"flux_abs_interaction\", \"h_flux_interaction\"]\n",
    "    else:\n",
    "        data[\"Time²\"] = data[\"Time_norm\"] ** 2\n",
    "        base_features = [\"Time_norm\", \"Time²\", \"h\", \"flux\", \"abs\", \"surf\"]\n",
    "    \n",
    "    # Identify columns\n",
    "    theory_cols = [c for c in data.columns if c.startswith(\"Theoretical_Temps_\")]\n",
    "    tc_cols = [col for col in data.columns if (col.startswith(\"TC\") and not col.endswith(\"_rate\")) or col.startswith(\"TC_\")]\n",
    "    \n",
    "    # Filter out any columns that don't exist\n",
    "    theory_cols = [col for col in theory_cols if col in data.columns]\n",
    "    tc_cols = [col for col in tc_cols if col in data.columns]\n",
    "    \n",
    "    if not tc_cols:\n",
    "        print(\"Warning: No TC columns found. Creating dummy TC columns for demonstration.\")\n",
    "        tc_cols = ['TC_1', 'TC_2', 'TC_3', 'TC_4']\n",
    "        for col in tc_cols:\n",
    "            if col not in data.columns:\n",
    "                data[col] = 20 + np.random.normal(0, 5, len(data))\n",
    "    \n",
    "    # Prepare features and targets\n",
    "    feature_cols = base_features + theory_cols\n",
    "    # Filter out any feature columns that don't exist\n",
    "    feature_cols = [col for col in feature_cols if col in data.columns]\n",
    "    \n",
    "    X = data[feature_cols].copy()\n",
    "    y = data[tc_cols].copy()\n",
    "    filenames = data[\"filename\"] if \"filename\" in data.columns else pd.Series([\"unknown\"] * len(data))\n",
    "    \n",
    "    print(f\"Features: {len(feature_cols)}, Targets: {len(tc_cols)}\")\n",
    "    print(f\"Theory columns: {len(theory_cols)}, TC columns: {len(tc_cols)}\")\n",
    "    print(f\"Feature columns: {feature_cols}\")\n",
    "    print(f\"Target columns: {tc_cols}\")\n",
    "    \n",
    "    # Remove missing values\n",
    "    mask = ~(X.isnull().any(axis=1) | y.isnull().any(axis=1))\n",
    "    X = X[mask].reset_index(drop=True)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "    filenames = filenames[mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"After removing missing values: {len(X)} samples\")\n",
    "    \n",
    "    if len(X) < 10:\n",
    "        raise ValueError(\"Not enough valid samples for training!\")\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_temp, y_train, y_temp, _, _ = train_test_split(\n",
    "        X, y, filenames, test_size=test_size, random_state=SEED\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test, _, _ = train_test_split(\n",
    "        X_temp, y_temp, _, test_size=val_size, random_state=SEED\n",
    "    )\n",
    "    \n",
    "    # Scaling\n",
    "    X_scaler = StandardScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "    \n",
    "    X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "    X_val_scaled = X_scaler.transform(X_val)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "    \n",
    "    y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "    y_val_scaled = y_scaler.transform(y_val)\n",
    "    y_test_scaled = y_scaler.transform(y_test)\n",
    "    \n",
    "    # Save scalers and metadata\n",
    "    joblib.dump(X_scaler, \"X_scaler_enhanced.pkl\")\n",
    "    joblib.dump(y_scaler, \"y_scaler_enhanced.pkl\")\n",
    "    joblib.dump({\"time_min\": time_min, \"time_max\": time_max}, \"time_range_enhanced.pkl\")\n",
    "    joblib.dump({\"feature_cols\": feature_cols, \"tc_cols\": tc_cols}, \"column_info.pkl\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "    )\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(X_test_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=min(64, len(X_train)//4), shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=min(64, len(X_val)//4), shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=min(64, len(X_test)//4), shuffle=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dba64751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, epochs=1000, patience=50):\n",
    "    \"\"\"Train the model with physics-informed loss and early stopping\"\"\"\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, min_lr=1e-6)\n",
    "    \n",
    "    # Updated criterion with physics loss\n",
    "    criterion = PhysicsInformedLoss(\n",
    "        smoothness_weight=0.001,\n",
    "        gradient_weight=0.0001,\n",
    "        physics_weight=0.01  # Adjust this weight based on performance\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"Starting training for {epochs} epochs...\")\n",
    "    print(f\"Physics loss weight: {criterion.physics_weight}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss_epoch = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            \n",
    "            # Pass inputs to loss function for physics computation\n",
    "            loss = criterion(predictions, y_batch, X_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_epoch += loss.item()\n",
    "        \n",
    "        train_loss_epoch /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_epoch = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                predictions = model(X_batch)\n",
    "                \n",
    "                # Pass inputs to loss function for physics computation\n",
    "                loss = criterion(predictions, y_batch, X_batch)\n",
    "                val_loss_epoch += loss.item()\n",
    "        \n",
    "        val_loss_epoch /= len(val_loader)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss_epoch)\n",
    "        \n",
    "        # Track metrics\n",
    "        train_losses.append(train_loss_epoch)\n",
    "        val_losses.append(val_loss_epoch)\n",
    "        \n",
    "        # Early stopping and checkpointing\n",
    "        if val_loss_epoch < best_val_loss:\n",
    "            best_val_loss = val_loss_epoch\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss_epoch,\n",
    "                'train_loss': train_loss_epoch,\n",
    "            }, 'best_thermal_model_enhanced.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Logging with more detail\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Train Loss: {train_loss_epoch:.6f} \"\n",
    "                  f\"Val Loss: {val_loss_epoch:.6f} LR: {current_lr:.8f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint = torch.load('best_thermal_model_enhanced.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model with validation loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a54e3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7: Evaluation and Visualization\n",
    "def evaluate_model(model, test_loader, y_scaler, device):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            pred = model(X_batch).cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "            actuals.append(y_batch.numpy())\n",
    "    \n",
    "    predictions = np.concatenate(predictions)\n",
    "    actuals = np.concatenate(actuals)\n",
    "    \n",
    "    # Inverse transform\n",
    "    pred_real = y_scaler.inverse_transform(predictions)\n",
    "    actual_real = y_scaler.inverse_transform(actuals)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(np.mean((pred_real - actual_real) ** 2, axis=0))\n",
    "    mae = np.mean(np.abs(pred_real - actual_real), axis=0)\n",
    "    mape = np.mean(np.abs((actual_real - pred_real) / (actual_real + 1e-8)), axis=0) * 100\n",
    "    \n",
    "    # R² score\n",
    "    y_mean = np.mean(actual_real, axis=0)\n",
    "    ss_tot = np.sum((actual_real - y_mean) ** 2, axis=0)\n",
    "    ss_res = np.sum((actual_real - pred_real) ** 2, axis=0)\n",
    "    r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "    \n",
    "    return {\n",
    "        'predictions': pred_real,\n",
    "        'actuals': actual_real,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'r2': r2,\n",
    "        'overall_rmse': np.sqrt(np.mean((pred_real - actual_real) ** 2)),\n",
    "        'overall_mae': np.mean(np.abs(pred_real - actual_real)),\n",
    "        'overall_r2': np.mean(r2)\n",
    "    }\n",
    "\n",
    "def plot_results(train_losses, val_losses, results, tc_cols):\n",
    "    \"\"\"Plot training history and results\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training history\n",
    "    axes[0, 0].plot(train_losses, label='Training Loss', color='blue')\n",
    "    axes[0, 0].plot(val_losses, label='Validation Loss', color='orange')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # RMSE by sensor\n",
    "    axes[0, 1].bar(range(len(tc_cols)), results['rmse'])\n",
    "    axes[0, 1].set_xlabel('Sensor Index')\n",
    "    axes[0, 1].set_ylabel('RMSE (°C)')\n",
    "    axes[0, 1].set_title('RMSE by Sensor')\n",
    "    axes[0, 1].set_xticks(range(len(tc_cols)))\n",
    "    axes[0, 1].set_xticklabels([f'TC{i+1}' for i in range(len(tc_cols))], rotation=45)\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[1, 0].scatter(results['actuals'].flatten(), results['predictions'].flatten(), alpha=0.5)\n",
    "    min_val = min(results['actuals'].min(), results['predictions'].min())\n",
    "    max_val = max(results['actuals'].max(), results['predictions'].max())\n",
    "    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('Actual Temperature (°C)')\n",
    "    axes[1, 0].set_ylabel('Predicted Temperature (°C)')\n",
    "    axes[1, 0].set_title('Predicted vs Actual')\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # R² scores\n",
    "    axes[1, 1].bar(range(len(tc_cols)), results['r2'])\n",
    "    axes[1, 1].set_xlabel('Sensor Index')\n",
    "    axes[1, 1].set_ylabel('R² Score')\n",
    "    axes[1, 1].set_title('R² Score by Sensor')\n",
    "    axes[1, 1].set_xticks(range(len(tc_cols)))\n",
    "    axes[1, 1].set_xticklabels([f'TC{i+1}' for i in range(len(tc_cols))], rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3c3333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8: Inference Functions\n",
    "def predict_temperature(model, X_scaler, y_scaler, time_range_data, column_info, \n",
    "                       time, h, flux, abs_val, surf, theoretical_temps, device):\n",
    "    \"\"\"\n",
    "    Inference function to predict actual temperatures from input parameters\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if len(theoretical_temps) != 10:\n",
    "        raise ValueError(\"Expected 10 theoretical temperatures, got {}\".format(len(theoretical_temps)))\n",
    "    \n",
    "    # Get time normalization parameters\n",
    "    time_min = time_range_data['time_min']\n",
    "    time_max = time_range_data['time_max']\n",
    "    \n",
    "    # Normalize time\n",
    "    time_norm = (time - time_min) / (time_max - time_min)\n",
    "    \n",
    "    # Create enhanced time features\n",
    "    time_squared = time_norm ** 2\n",
    "    time_cubed = time_norm ** 3\n",
    "    time_sin = np.sin(2 * np.pi * time_norm)\n",
    "    time_cos = np.cos(2 * np.pi * time_norm)\n",
    "    \n",
    "    # Create interaction features\n",
    "    flux_abs_interaction = flux * abs_val\n",
    "    h_flux_interaction = h * flux\n",
    "    \n",
    "    # Prepare input features\n",
    "    input_features = [\n",
    "        time_norm, time_squared, time_cubed, time_sin, time_cos,\n",
    "        h, flux, abs_val, surf, flux_abs_interaction, h_flux_interaction\n",
    "    ]\n",
    "    input_features.extend(theoretical_temps)\n",
    "    \n",
    "    # Convert to numpy array and reshape\n",
    "    input_array = np.array(input_features).reshape(1, -1)\n",
    "    \n",
    "    # Scale the input features\n",
    "    input_scaled = X_scaler.transform(input_array)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction_scaled = model(input_tensor).cpu().numpy()\n",
    "    \n",
    "    # Inverse transform the prediction\n",
    "    prediction_real = y_scaler.inverse_transform(prediction_scaled)\n",
    "    \n",
    "    # Get TC column names\n",
    "    tc_cols = column_info['tc_cols']\n",
    "    \n",
    "    # Create result dictionary\n",
    "    result = {\n",
    "        'input_parameters': {\n",
    "            'time': time,\n",
    "            'h': h,\n",
    "            'flux': flux,\n",
    "            'abs': abs_val,\n",
    "            'surf': surf,\n",
    "            'theoretical_temps': theoretical_temps\n",
    "        },\n",
    "        'predicted_temperatures': {}\n",
    "    }\n",
    "    \n",
    "    # Map predictions to TC sensor names\n",
    "    for i, tc_name in enumerate(tc_cols):\n",
    "        result['predicted_temperatures'][tc_name] = float(prediction_real[0, i])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def load_inference_components():\n",
    "    \"\"\"Load all necessary components for inference\"\"\"\n",
    "    try:\n",
    "        # Load scalers and metadata\n",
    "        X_scaler = joblib.load(\"X_scaler_enhanced.pkl\")\n",
    "        y_scaler = joblib.load(\"y_scaler_enhanced.pkl\")\n",
    "        time_range_data = joblib.load(\"time_range_enhanced.pkl\")\n",
    "        column_info = joblib.load(\"column_info.pkl\")\n",
    "        \n",
    "        # Set device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load model\n",
    "        checkpoint = torch.load('best_thermal_model_enhanced.pth', map_location=device)\n",
    "        \n",
    "        # Recreate model\n",
    "        input_size = len(column_info['feature_cols'])\n",
    "        output_size = len(column_info['tc_cols'])\n",
    "        \n",
    "        model = EnhancedThermalNet(\n",
    "            input_size=input_size,\n",
    "            output_size=output_size,\n",
    "            hidden_dims=[512, 256, 256, 128],\n",
    "            dropout_rate=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        # Load trained weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        print(\"Inference components loaded successfully!\")\n",
    "        print(f\"Model input size: {input_size}\")\n",
    "        print(f\"Model output size: {output_size}\")\n",
    "        print(f\"TC sensors: {column_info['tc_cols']}\")\n",
    "        \n",
    "        return model, X_scaler, y_scaler, time_range_data, column_info, device\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading inference components: {e}\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "def example_inference():\n",
    "    \"\"\"Example of how to use the inference function\"\"\"\n",
    "    # Load inference components\n",
    "    model, X_scaler, y_scaler, time_range_data, column_info, device = load_inference_components()\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"Failed to load inference components. Make sure the model is trained and saved.\")\n",
    "        return\n",
    "    \n",
    "    # Example input parameters\n",
    "    time = 0  # 30 minutes\n",
    "    h = 0.1575   # Heat transfer coefficient\n",
    "    flux = 25900  # Heat flux\n",
    "    abs_val = 20  # Absorption coefficient\n",
    "    surf = 0.98   # Surface emissivity\n",
    "    \n",
    "    # Example theoretical temperatures (10 values)\n",
    "    theoretical_temps = [322.346598107413,344.707379405421,344.707598403347,342.463078051269,332.928870144283,324.216781541098,318.02660925491,315.821244548393,315.821244548393,315.821244548393]\n",
    "    \n",
    "    try:\n",
    "        # Make prediction\n",
    "        result = predict_temperature(\n",
    "            model, X_scaler, y_scaler, time_range_data, column_info,\n",
    "            time, h, flux, abs_val, surf, theoretical_temps, device\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TEMPERATURE PREDICTION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Time: {result['input_parameters']['time']} seconds\")\n",
    "        print(f\"Heat transfer coefficient (h): {result['input_parameters']['h']}\")\n",
    "        print(f\"Heat flux: {result['input_parameters']['flux']}\")\n",
    "        print(f\"Absorption coefficient: {result['input_parameters']['abs']}\")\n",
    "        print(f\"Surface emissivity: {result['input_parameters']['surf']}\")\n",
    "        print(\"\\nPredicted TC Temperatures:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for tc_name, temp in result['predicted_temperatures'].items():\n",
    "            print(f\"{tc_name}: {temp:.2f} °C\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "        return None\n",
    "\n",
    "def batch_predict_temperature(model, X_scaler, y_scaler, time_range_data, column_info, \n",
    "                            input_data, device):\n",
    "    \"\"\"Batch inference function for multiple predictions\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for data_point in input_data:\n",
    "        try:\n",
    "            result = predict_temperature(\n",
    "                model, X_scaler, y_scaler, time_range_data, column_info,\n",
    "                data_point['time'], data_point['h'], data_point['flux'],\n",
    "                data_point['abs'], data_point['surf'], data_point['theoretical_temps'],\n",
    "                device\n",
    "            )\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data point: {e}\")\n",
    "            results.append(None)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87015f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data directory: data/new_processed_reset\n",
      "Loading data from: data/new_processed_reset\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_699s - Sheet1_processed.csv (717 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv (532 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv (4621 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv (566 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv (621 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_630s - Sheet2_processed.csv (618 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv (516 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_675s - Sheet3_processed.csv (856 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv (826 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv (795 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv (932 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_585s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_598s - Sheet1_processed.csv (528 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_surf0_439s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv (912 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_719s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv (1087 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_644s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv (790 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_749s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_429s - Sheet2_processed.csv (880 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv (585 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv (4247 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv (611 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv (554 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_surf0_660s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_692s-Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv (589 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_431s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv (970 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv (803 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv (1411 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_830s - Sheet2_processed.csv (528 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv (787 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_surf0_493s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv (531 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_491s - Sheet3_processed.csv (702 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv (547 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_surf0_115s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv (3952 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_651s - Sheet4_processed.csv (554 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv (865 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv (870 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv (550 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_628s - Sheet4_processed.csv (539 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_389s - Sheet3_processed.csv (537 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_389s - Sheet4_processed.csv (2139 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv (887 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_825s-Sheer3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv (790 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_613s - Sheet1_processed.csv (550 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv (794 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_747s-Sheet3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv (4087 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv (849 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv (534 rows)\n",
      "\n",
      "Total data loaded: 48187 rows, 26 columns\n",
      "Starting data preprocessing...\n",
      "Features: 21, Targets: 10\n",
      "Theory columns: 10, TC columns: 10\n",
      "Feature columns: ['Time_norm', 'Time²', 'Time³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10']\n",
      "After removing missing values: 48187 samples\n",
      "Training samples: 33730\n",
      "Validation samples: 7228\n",
      "Test samples: 7229\n",
      "Using device: cpu\n",
      "Model created with 409549 parameters\n",
      "Starting training for 1000 epochs...\n",
      "Physics loss weight: 0.01\n",
      "Epoch [50/1000] Train Loss: 0.000527 Val Loss: 0.000326 LR: 0.00100000\n",
      "Epoch [100/1000] Train Loss: 0.000411 Val Loss: 0.000201 LR: 0.00100000\n",
      "Epoch [150/1000] Train Loss: 0.000255 Val Loss: 0.000134 LR: 0.00050000\n",
      "Epoch [200/1000] Train Loss: 0.000190 Val Loss: 0.000125 LR: 0.00025000\n",
      "Epoch [250/1000] Train Loss: 0.000166 Val Loss: 0.000101 LR: 0.00006250\n",
      "Epoch [300/1000] Train Loss: 0.000161 Val Loss: 0.000100 LR: 0.00006250\n",
      "Early stopping at epoch 329\n",
      "Loaded best model with validation loss: 0.000098\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Sensor          RMSE (°C)    MAE (°C)     MAPE (%)     R² Score    \n",
      "-----------------------------------------------------------------\n",
      "TC1_tip         0.985        0.658        0.22         0.999       \n",
      "TC2             1.055        0.676        0.24         0.999       \n",
      "TC3             0.891        0.571        0.20         1.000       \n",
      "TC4             0.977        0.619        0.23         0.999       \n",
      "TC5             1.421        0.565        0.21         0.999       \n",
      "TC6             1.292        0.634        0.28         0.999       \n",
      "TC7             1.408        0.687        0.30         0.999       \n",
      "TC8             1.579        0.730        0.32         0.999       \n",
      "TC9             1.708        1.059        0.36         0.999       \n",
      "TC10            3.626        2.078        0.67         0.994       \n",
      "-----------------------------------------------------------------\n",
      "Overall         1.675        0.828        0.30         0.999       \n"
     ]
    }
   ],
   "source": [
    "# Block 9: Main Execution\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        # Load data (with h6 filter like in your original code)\n",
    "        DATA_DIR=get_data_directory()\n",
    "        data = load_data(DATA_DIR, h_filter=h_map[6])  # Only h6 = 0.1575\n",
    "        \n",
    "        # Preprocess data\n",
    "        train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols = preprocess_data(\n",
    "            data, use_enhanced_features=True\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        model = EnhancedThermalNet(\n",
    "            input_size=train_loader.dataset.tensors[0].shape[1],\n",
    "            output_size=train_loader.dataset.tensors[1].shape[1],\n",
    "            hidden_dims=[512, 256, 256, 128],\n",
    "            dropout_rate=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "        \n",
    "        # Train model\n",
    "        train_losses, val_losses = train_model(\n",
    "            model, train_loader, val_loader, device, epochs=1000, patience=50\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        results = evaluate_model(model, test_loader, y_scaler, device)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"{'Sensor':<15} {'RMSE (°C)':<12} {'MAE (°C)':<12} {'MAPE (%)':<12} {'R² Score':<12}\")\n",
    "        print(\"-\"*65)\n",
    "        \n",
    "        for i, col in enumerate(tc_cols):\n",
    "            print(f\"{col:<15} {results['rmse'][i]:<12.3f} {results['mae'][i]:<12.3f} \"\n",
    "                  f\"{results['mape'][i]:<12.2f} {results['r2'][i]:<12.3f}\")\n",
    "        \n",
    "        print(\"-\"*65)\n",
    "        print(f\"{'Overall':<15} {results['overall_rmse']:<12.3f} {results['overall_mae']:<12.3f} \"\n",
    "              f\"{np.mean(results['mape']):<12.2f} {results['overall_r2']:<12.3f}\")\n",
    "\n",
    "        # Plot results\n",
    "        plot_results(train_losses, val_losses, results, tc_cols)\n",
    "        # return model, results, test_results, X_scaler, y_scaler, tc_cols, test_loader, device\n",
    "        return model, results, X_scaler, y_scaler, tc_cols\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, results, X_scaler, y_scaler, tc_cols = main()\n",
    "    # model, results, test_results, X_scaler, y_scaler, tc_cols, test_loader, device = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85bb0083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction components loaded successfully!\n",
      "\n",
      "Predicted Temperatures:\n",
      "TC1_tip: 360.980 °C\n",
      "TC2: 359.409 °C\n",
      "TC3: 360.902 °C\n",
      "TC4: 360.858 °C\n",
      "TC5: 360.609 °C\n",
      "TC6: 362.309 °C\n",
      "TC7: 360.845 °C\n",
      "TC8: 361.639 °C\n",
      "TC9: 359.356 °C\n",
      "TC10: 339.511 °C\n",
      "[360.97967529296875, 359.4092712402344, 360.9024353027344, 360.8578796386719, 360.6089172363281, 362.3086242675781, 360.84466552734375, 361.6388854980469, 359.3563537597656, 339.5113220214844]\n"
     ]
    }
   ],
   "source": [
    "# Block 10: Standalone Prediction Functionality\n",
    "def load_for_prediction():\n",
    "    \"\"\"Load all components needed for prediction\"\"\"\n",
    "    try:\n",
    "        # Load scalers and metadata\n",
    "        X_scaler = joblib.load(\"X_scaler_enhanced.pkl\")\n",
    "        y_scaler = joblib.load(\"y_scaler_enhanced.pkl\")\n",
    "        time_range_data = joblib.load(\"time_range_enhanced.pkl\")\n",
    "        column_info = joblib.load(\"column_info.pkl\")\n",
    "        \n",
    "        # Set device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load model\n",
    "        model = EnhancedThermalNet(\n",
    "            input_size=len(column_info['feature_cols']),\n",
    "            output_size=len(column_info['tc_cols']),\n",
    "            hidden_dims=[512, 256, 256, 128],\n",
    "            dropout_rate=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        checkpoint = torch.load('best_thermal_model_enhanced.pth', map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"Prediction components loaded successfully!\")\n",
    "        return model, X_scaler, y_scaler, time_range_data, column_info, device\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prediction components: {e}\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "def make_prediction(time, h, flux, abs_val, surf, theoretical_temps):\n",
    "    \"\"\"\n",
    "    Make a temperature prediction with given parameters\n",
    "    \n",
    "    Args:\n",
    "        time: Time value in seconds\n",
    "        h: Heat transfer coefficient (0.0375, 0.084, or 0.1575)\n",
    "        flux: Heat flux (19400, 21250, or 25900)\n",
    "        abs_val: Absorption coefficient (3 or 100)\n",
    "        surf: Surface emissivity (0.76 or 0.98)\n",
    "        theoretical_temps: List of 10 theoretical temperatures\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with predicted temperatures for each TC sensor\n",
    "    \"\"\"\n",
    "    # Load components if not already loaded\n",
    "    if not all(var in globals() for var in ['pred_model', 'pred_X_scaler', 'pred_y_scaler', \n",
    "                                           'pred_time_range', 'pred_column_info', 'pred_device']):\n",
    "        global pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device\n",
    "        pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device = load_for_prediction()\n",
    "        if pred_model is None:\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        # Validate inputs\n",
    "        if len(theoretical_temps) != 10:\n",
    "            raise ValueError(\"Exactly 10 theoretical temperatures required\")\n",
    "        \n",
    "        if h not in [0.0375, 0.084, 0.1575]:\n",
    "            print(f\"Warning: h value {h} not in expected values [0.0375, 0.084, 0.1575]\")\n",
    "            \n",
    "        if flux not in [19400, 21250, 25900]:\n",
    "            print(f\"Warning: flux value {flux} not in expected values [19400, 21250, 25900]\")\n",
    "            \n",
    "        if abs_val not in [3, 100]:\n",
    "            print(f\"Warning: abs value {abs_val} not in expected values [3, 100]\")\n",
    "            \n",
    "        if surf not in [0.76, 0.98]:\n",
    "            print(f\"Warning: surf value {surf} not in expected values [0.76, 0.98]\")\n",
    "        \n",
    "        # Make prediction\n",
    "        result = predict_temperature(\n",
    "            pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info,\n",
    "            time, h, flux, abs_val, surf, theoretical_temps, pred_device\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Prediction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (run this block after training once to load components)\n",
    "pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device = load_for_prediction()\n",
    "\n",
    "# Now you can make predictions anytime using:\n",
    "prediction = make_prediction(\n",
    "    time=44,\n",
    "    h=0.1575,\n",
    "    flux=21250,\n",
    "    abs_val=3,\n",
    "    surf=0.98,\n",
    "    theoretical_temps=[303.991791613348,324.34732303606,324.34732303606,324.34732303606,324.34732303606,324.34732303606,324.34732303606,324.34732303606,324.34732303606,324.34732303606]\n",
    "    \n",
    ")\n",
    "temp_arr=[]\n",
    "if prediction:\n",
    "    print(\"\\nPredicted Temperatures:\")\n",
    "    for tc_name, temp in prediction['predicted_temperatures'].items():\n",
    "        print(f\"{tc_name}: {temp:.3f} °C\")\n",
    "        temp_arr.append(temp)\n",
    "\n",
    "print(temp_arr)\n",
    "\n",
    "# Test batch inference\n",
    "        # print(\"\\n3. Testing batch prediction...\")\n",
    "        # try:\n",
    "        #     if model_inf is not None:\n",
    "        #         # Create batch of test data\n",
    "        #         batch_data = [\n",
    "        #             {\n",
    "        #                 'time': 1800,  # 30 minutes\n",
    "        #                 'h': 0.1575,\n",
    "        #                 'flux': 25900,\n",
    "        #                 'abs': 100,\n",
    "        #                 'surf': 0.98,\n",
    "        #                 'theoretical_temps': [25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0]\n",
    "        #             },\n",
    "        #             {\n",
    "        #                 'time': 3600,  # 60 minutes\n",
    "        #                 'h': 0.1575,\n",
    "        #                 'flux': 19400,\n",
    "        #                 'abs': 3,\n",
    "        #                 'surf': 0.76,\n",
    "        #                 'theoretical_temps': [28.0, 32.5, 37.2, 41.8, 46.5, 51.1, 55.8, 60.4, 65.1, 69.7]\n",
    "        #             }\n",
    "        #         ]\n",
    "                \n",
    "        #         batch_results = batch_predict_temperature(\n",
    "        #             model_inf, X_scaler_inf, y_scaler_inf, time_range_inf, column_info_inf,\n",
    "        #             batch_data, device_inf\n",
    "        #         )\n",
    "                \n",
    "        #         print(f\"Batch prediction completed for {len(batch_results)} samples\")\n",
    "                \n",
    "        #         for i, result in enumerate(batch_results):\n",
    "        #             if result:\n",
    "        #                 print(f\"\\nBatch Sample {i+1}:\")\n",
    "        #                 print(f\"  Time: {result['input_parameters']['time']/60:.1f} min\")\n",
    "        #                 print(f\"  Flux: {result['input_parameters']['flux']}\")\n",
    "        #                 print(f\"  Average predicted temp: {np.mean(list(result['predicted_temperatures'].values())):.2f} °C\")\n",
    "                \n",
    "        #         print(\"✓ Batch prediction test successful!\")\n",
    "                \n",
    "        #     else:\n",
    "        #         print(\"✗ Batch prediction test failed - model not loaded\")\n",
    "                \n",
    "        # except Exception as e:\n",
    "        #     print(f\"✗ Batch prediction test failed: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6dcc8430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vertical_profile(predicted, actual, filename=\"Sample Profile\"):\n",
    "    # Reverse order so TC10 (surface) is at the top\n",
    "    predicted = predicted[::-1]\n",
    "    actual = actual[::-1]\n",
    "    sensor_labels = [f\"TC{i}\" for i in range(10, 0, -1)]  # TC10 to TC1\n",
    "\n",
    "    total_height = 0.1575  # Total receiver height in meters\n",
    "    spacing = total_height / 9\n",
    "    depths = [0 - i * spacing for i in range(10)]  # TC10 at 0.0, TC1 at -total_height\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.plot(actual, depths, label=\"Actual\", color='blue', marker='o', linewidth=2)\n",
    "    plt.plot(predicted, depths, label=\"Predicted\", color='red', marker='x', linestyle='--', linewidth=2)\n",
    "\n",
    "    plt.gca().invert_yaxis()  # So 0 (surface) is at top\n",
    "\n",
    "    # Set clean numeric y-ticks\n",
    "    plt.yticks(depths, [f\"{d:.3f}\" for d in depths])\n",
    "    plt.ylim(min(depths) - spacing * 0.5, max(depths) + spacing * 0.5)\n",
    "\n",
    "    plt.xlabel(\"Temperature (°C)\")\n",
    "    plt.ylabel(\"Depth (m)\")\n",
    "    plt.title(f\"Vertical Profile: {filename}\")\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Add sensor labels next to each point\n",
    "    for i, label in enumerate(sensor_labels):\n",
    "        plt.text(\n",
    "            actual[i], depths[i], label,\n",
    "            ha='right', va='center', fontsize=8,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#cce6ff\", ec=\"navy\", lw=1, alpha=0.7)\n",
    "        )\n",
    "        plt.text(\n",
    "            predicted[i], depths[i], label,\n",
    "            ha='left', va='center', fontsize=8,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#ffcccc\", ec=\"darkred\", lw=1, alpha=0.7)\n",
    "        )\n",
    "\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{filename}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# actual = [361.36,360.52,360.93,362.1,361.66,363.36,361.29,361.44,360.32,337.33]\n",
    "# pred = [361.057861328125, 359.50201416015625, 360.8453063964844, 360.733642578125, 360.468505859375, 362.0776062011719, 360.5418395996094, 361.2062683105469, 358.7239990234375, 338.3153076171875]\n",
    "\n",
    "# plot_vertical_profile(pred, actual, filename=\"h6_flux78_abs0_surf0_newSalt_641s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4164e4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data directory: data/new_processed_reset\n",
      "\n",
      "=== Running Standalone Test Cross-Check ===\n",
      "Loading data from: data/new_processed_reset\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_699s - Sheet1_processed.csv (717 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv (532 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv (4621 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv (566 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv (621 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_630s - Sheet2_processed.csv (618 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv (516 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_675s - Sheet3_processed.csv (856 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv (826 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv (795 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv (932 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_585s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_598s - Sheet1_processed.csv (528 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_surf0_439s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv (912 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_719s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv (1087 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_644s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv (790 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_749s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_429s - Sheet2_processed.csv (880 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv (585 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv (4247 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv (611 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv (554 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_surf0_660s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_692s-Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv (589 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_431s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv (970 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv (803 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv (1411 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_830s - Sheet2_processed.csv (528 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv (787 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_surf0_493s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv (531 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_491s - Sheet3_processed.csv (702 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv (547 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_surf0_115s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv (3952 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_651s - Sheet4_processed.csv (554 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv (865 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv (870 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv (550 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_628s - Sheet4_processed.csv (539 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_389s - Sheet3_processed.csv (537 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_389s - Sheet4_processed.csv (2139 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv (887 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_825s-Sheer3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv (790 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_613s - Sheet1_processed.csv (550 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv (794 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_747s-Sheet3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv (4087 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv (849 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv (534 rows)\n",
      "\n",
      "Total data loaded: 48187 rows, 26 columns\n",
      "Starting data preprocessing...\n",
      "Features: 21, Targets: 10\n",
      "Theory columns: 10, TC columns: 10\n",
      "Feature columns: ['Time_norm', 'Time²', 'Time³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10']\n",
      "After removing missing values: 48187 samples\n",
      "Training samples: 33730\n",
      "Validation samples: 7228\n",
      "Test samples: 7229\n",
      "\n",
      "=== Cross-Checking 3 Test Samples ===\n",
      "\n",
      "Test Sample 5329:\n",
      "Inputs: Time=1631.00s, h=0.1575, flux=25900, abs=20, surf=0.98\n",
      "Theoretical Temps: ['400.89', '433.32', '433.32', '433.30', '420.60', '398.60', '380.57', '374.79', '374.79', '374.79']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (°C)  Actual (°C)     Error (°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         354.620         354.420         0.200          \n",
      "TC2             356.527         356.340         0.187          \n",
      "TC3             364.470         364.160         0.310          \n",
      "TC4             372.988         372.510         0.478          \n",
      "TC5             377.613         377.270         0.343          \n",
      "TC6             384.736         384.030         0.706          \n",
      "TC7             385.739         385.110         0.629          \n",
      "TC8             387.747         386.960         0.787          \n",
      "TC9             389.546         388.650         0.896          \n",
      "TC10            377.363         377.090         0.273          \n",
      "\n",
      "Test Sample 6382:\n",
      "Inputs: Time=315.00s, h=0.1575, flux=25900, abs=20, surf=0.98\n",
      "Theoretical Temps: ['348.50', '373.94', '373.94', '371.97', '358.60', '344.10', '333.61', '330.17', '330.17', '330.17']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (°C)  Actual (°C)     Error (°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         354.148         353.820         0.328          \n",
      "TC2             354.363         354.700         0.337          \n",
      "TC3             358.676         359.400         0.724          \n",
      "TC4             361.622         362.290         0.668          \n",
      "TC5             363.115         363.620         0.505          \n",
      "TC6             367.056         367.320         0.264          \n",
      "TC7             367.125         366.920         0.205          \n",
      "TC8             368.855         368.520         0.335          \n",
      "TC9             370.202         370.220         0.018          \n",
      "TC10            357.228         358.750         1.522          \n",
      "\n",
      "Test Sample 3900:\n",
      "Inputs: Time=734.00s, h=0.1575, flux=25900, abs=3, surf=0.76\n",
      "Theoretical Temps: ['350.56', '371.79', '371.79', '371.79', '371.79', '371.79', '371.79', '371.79', '371.79', '371.79']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (°C)  Actual (°C)     Error (°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         365.240         365.500         0.260          \n",
      "TC2             363.481         363.720         0.239          \n",
      "TC3             365.841         366.050         0.209          \n",
      "TC4             365.952         366.340         0.388          \n",
      "TC5             365.992         366.170         0.178          \n",
      "TC6             368.073         368.400         0.327          \n",
      "TC7             367.387         367.670         0.283          \n",
      "TC8             368.345         368.670         0.325          \n",
      "TC9             368.033         368.810         0.777          \n",
      "TC10            355.475         355.240         0.235          \n",
      "\n",
      "=== Average Errors (Test Set) ===\n",
      "Sensor          Avg Error (°C) \n",
      "------------------------------\n",
      "TC1_tip         0.262          \n",
      "TC2             0.254          \n",
      "TC3             0.414          \n",
      "TC4             0.511          \n",
      "TC5             0.342          \n",
      "TC6             0.432          \n",
      "TC7             0.372          \n",
      "TC8             0.482          \n",
      "TC9             0.563          \n",
      "TC10            0.677          \n",
      "Plot saved: h0.1575_flux25900.0_abs20.0_surf0.9800000190734863_time1631s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs20.0_surf0.9800000190734863_time315s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.7599999904632568_time734s.png\n"
     ]
    }
   ],
   "source": [
    "# Block 11: Standalone Testing\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive Agg backend to avoid tkinter dependency\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def cross_check_test_predictions(data_dir, h_filter=0.1575, num_samples=5, filter_condition=None):\n",
    "    \"\"\"Cross-check model predictions with rows from the test dataset.\"\"\"\n",
    "    # Load inference components\n",
    "    global pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device\n",
    "    if not all(var in globals() for var in ['pred_model', 'pred_X_scaler', 'pred_y_scaler', \n",
    "                                           'pred_time_range', 'pred_column_info', 'pred_device']):\n",
    "        pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device = load_for_prediction()\n",
    "        if pred_model is None:\n",
    "            raise ValueError(\"Failed to load inference components. Ensure model and scalers are saved.\")\n",
    "\n",
    "    # Load and preprocess data to get test_loader\n",
    "    data = load_data(data_dir, h_filter=h_filter)\n",
    "    train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols = preprocess_data(data, use_enhanced_features=True)\n",
    "    \n",
    "    # Extract test dataset from test_loader\n",
    "    X_test_scaled = np.concatenate([batch[0].numpy() for batch in test_loader], axis=0)\n",
    "    y_test_scaled = np.concatenate([batch[1].numpy() for batch in test_loader], axis=0)\n",
    "    \n",
    "    # Inverse transform to get original feature and target values\n",
    "    X_test = pred_X_scaler.inverse_transform(X_test_scaled)\n",
    "    y_test = pred_y_scaler.inverse_transform(y_test_scaled)\n",
    "    \n",
    "    # Create DataFrame for test data\n",
    "    feature_cols = pred_column_info['feature_cols']\n",
    "    test_df = pd.DataFrame(X_test, columns=feature_cols)\n",
    "    test_df[tc_cols] = y_test\n",
    "    \n",
    "    # Add original 'Time' (before normalization) for filtering\n",
    "    time_min = pred_time_range['time_min']\n",
    "    time_max = pred_time_range['time_max']\n",
    "    test_df['Time'] = test_df['Time_norm'] * (time_max - time_min) + time_min\n",
    "    \n",
    "    # Apply filter condition if provided (e.g., specific time or flux)\n",
    "    if filter_condition is not None:\n",
    "        test_df = test_df.query(filter_condition)\n",
    "        if test_df.empty:\n",
    "            raise ValueError(f\"No test rows match the condition: {filter_condition}\")\n",
    "    \n",
    "    # Sample rows for cross-checking (no random_state for true randomness)\n",
    "    num_samples = min(num_samples, len(test_df))\n",
    "    if num_samples == 0:\n",
    "        raise ValueError(\"No test samples available after filtering!\")\n",
    "    sample_rows = test_df.sample(n=num_samples,random_state=33) if filter_condition is None else test_df.head(num_samples)\n",
    "    \n",
    "    print(f\"\\n=== Cross-Checking {len(sample_rows)} Test Samples ===\")\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in sample_rows.iterrows():\n",
    "        # Prepare input features\n",
    "        time = row['Time']\n",
    "        h = row['h']\n",
    "        flux = row['flux']\n",
    "        abs_val = row['abs']\n",
    "        surf = row['surf']\n",
    "        theoretical_temps = [row[col] for col in [c for c in feature_cols if c.startswith('Theoretical_Temps_')]]\n",
    "        \n",
    "        # Make prediction\n",
    "        try:\n",
    "            pred_result = predict_temperature(\n",
    "                pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info,\n",
    "                time, h, flux, abs_val, surf, theoretical_temps, pred_device\n",
    "            )\n",
    "            \n",
    "            # Get actual temperatures\n",
    "            actual_temps = {col: row[col] for col in tc_cols}\n",
    "            \n",
    "            # Compare predictions with actuals\n",
    "            comparison = {\n",
    "                'index': idx,\n",
    "                'inputs': {\n",
    "                    'time': time,\n",
    "                    'h': h,\n",
    "                    'flux': flux,\n",
    "                    'abs': abs_val,\n",
    "                    'surf': surf,\n",
    "                    'theoretical_temps': theoretical_temps\n",
    "                },\n",
    "                'predicted_temps': pred_result['predicted_temperatures'],\n",
    "                'actual_temps': actual_temps,\n",
    "                'errors': {col: abs(pred_result['predicted_temperatures'][col] - actual_temps[col]) \n",
    "                          for col in tc_cols}\n",
    "            }\n",
    "            \n",
    "            results.append(comparison)\n",
    "            \n",
    "            # Print comparison\n",
    "            print(f\"\\nTest Sample {idx}:\")\n",
    "            print(f\"Inputs: Time={time:.2f}s, h={h:.4f}, flux={flux:.0f}, abs={abs_val:.0f}, surf={surf:.2f}\")\n",
    "            print(\"Theoretical Temps:\", [f\"{t:.2f}\" for t in theoretical_temps])\n",
    "            print(\"Predicted vs Actual Temperatures:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"{'Sensor':<15} {'Predicted (°C)':<15} {'Actual (°C)':<15} {'Error (°C)':<15}\")\n",
    "            print(\"-\" * 50)\n",
    "            for col in tc_cols:\n",
    "                pred_temp = pred_result['predicted_temperatures'][col]\n",
    "                actual_temp = actual_temps[col]\n",
    "                error = comparison['errors'][col]\n",
    "                print(f\"{col:<15} {pred_temp:<15.3f} {actual_temp:<15.3f} {error:<15.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test sample {idx}: {e}\")\n",
    "    \n",
    "    # Calculate average errors\n",
    "    if results:\n",
    "        avg_errors = {col: np.mean([r['errors'][col] for r in results]) for col in tc_cols}\n",
    "        print(\"\\n=== Average Errors (Test Set) ===\")\n",
    "        print(f\"{'Sensor':<15} {'Avg Error (°C)':<15}\")\n",
    "        print(\"-\" * 30)\n",
    "        for col, avg_error in avg_errors.items():\n",
    "            print(f\"{col:<15} {avg_error:<15.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_test_cross_check(data_dir, h_filter=0.1575, num_samples=5, filter_condition=None):\n",
    "    \"\"\"Run cross-checking on test data without requiring main execution.\"\"\"\n",
    "    try:\n",
    "        print(\"\\n=== Running Standalone Test Cross-Check ===\")\n",
    "        results = cross_check_test_predictions(data_dir, h_filter, num_samples, filter_condition)\n",
    "        \n",
    "        # Plot results using plot_vertical_profile\n",
    "        for result in results:\n",
    "            # Extract predicted and actual temperatures in order TC1_tip to TC10\n",
    "            tc_cols = ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10']\n",
    "            predicted = [result['predicted_temps'][col] for col in tc_cols]\n",
    "            actual = [result['actual_temps'][col] for col in tc_cols]\n",
    "            # Create filename based on input conditions\n",
    "            filename = (\n",
    "                f\"h{h_filter}_flux{result['inputs']['flux']}_\"\n",
    "                f\"abs{result['inputs']['abs']}_surf{result['inputs']['surf']}_\"\n",
    "                f\"time{result['inputs']['time']:.0f}s\"\n",
    "            )\n",
    "            try:\n",
    "                plot_vertical_profile(predicted, actual, filename=f\"Sample {result['index']} - {filename}\")\n",
    "                print(f\"Plot saved: {filename}.png\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error plotting sample {result['index']}: {e}\")\n",
    "        \n",
    "        # Save results to CSV\n",
    "        # pd.DataFrame(results).to_csv('test_cross_check_results.csv')\n",
    "        # print(\"Results saved to 'test_cross_check_results.csv'\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in standalone test cross-check: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_DIR = get_data_directory()\n",
    "    run_test_cross_check(DATA_DIR, h_filter=0.1575, num_samples=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
