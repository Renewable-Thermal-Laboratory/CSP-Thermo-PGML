{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f64754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: Imports and Setup\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35101844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Configuration\n",
    "def get_data_directory():\n",
    "    \"\"\"Find the data directory dynamically\"\"\"\n",
    "    possible_paths = [\n",
    "        \"data/new_processed_reset\",\n",
    "        \"./data/new_processed_reset\", \n",
    "        \"../data/new_processed_reset\",\n",
    "        \"data\",\n",
    "        \"./data\",\n",
    "        \"../data\",\n",
    "        \"D:/Research Assistant work/Github Organization/ml models/ml_models/PhysicsGuidedNeuralNetwork/data/new_processed_reset\"\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Found data directory: {path}\")\n",
    "            return path\n",
    "    \n",
    "    # If no directory found, ask user to specify\n",
    "    print(\"Data directory not found. Please specify the path to your data directory:\")\n",
    "    user_path = input(\"Enter data directory path: \").strip()\n",
    "    if os.path.exists(user_path):\n",
    "        return user_path\n",
    "    else:\n",
    "        raise ValueError(f\"Specified data directory {user_path} does not exist!\")\n",
    "\n",
    "DROP_COLS = [\"TC_9_5\", \"TC_Bottom_rec_groove\", \"TC_wall_ins_ext\", \"TC_bottom_ins_groove\", \"Theoretical_Temps_11\"]\n",
    "\n",
    "# File mapping dictionaries\n",
    "h_map = {2: 0.0375, 3: 0.084, 6: 0.1575}\n",
    "flux_map = {88: 25900, 78: 21250, 73: 19400}\n",
    "abs_map = {0: 3, 92: 100}\n",
    "surf_map = {0: 0.98, 1: 0.76}\n",
    "pattern = r\"h(\\d+)_flux(\\d+)_abs(\\d+)(?:_[A-Za-z0-9]+)*_surf([01])(?:_[A-Za-z0-9]+)*[\\s_]+(\\d+)s\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32497ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Data Loading Functions\n",
    "def parse_filename_params(filename):\n",
    "    \"\"\"Parse filename to extract thermal parameters\"\"\"\n",
    "    m = re.search(pattern, filename)\n",
    "    if not m:\n",
    "        return None\n",
    "    h_raw = int(m.group(1))\n",
    "    flux_raw = int(m.group(2))\n",
    "    abs_raw = int(m.group(3))\n",
    "    surf_raw = int(m.group(4))\n",
    "    t = int(m.group(5))\n",
    "\n",
    "    h = h_map.get(h_raw, h_raw)\n",
    "    flux = flux_map.get(flux_raw, flux_raw)\n",
    "    abs_ = abs_map.get(abs_raw, abs_raw)\n",
    "    surf = surf_map.get(surf_raw)\n",
    "\n",
    "    return h, flux, abs_, surf, t\n",
    "\n",
    "def load_and_process_file(path, h, flux, abs_val, surf, filename, min_time=0):\n",
    "    \"\"\"Load and process individual CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "        df = df[df[\"Time\"] >= min_time].copy()\n",
    "        \n",
    "        # Drop unwanted columns\n",
    "        df.drop(columns=[col for col in df.columns if col in DROP_COLS or col.startswith(\"Depth_\")], \n",
    "                inplace=True, errors='ignore')\n",
    "        \n",
    "        # Add parameters\n",
    "        df[\"h\"] = h\n",
    "        df[\"flux\"] = flux\n",
    "        df[\"abs\"] = abs_val\n",
    "        df[\"surf\"] = surf\n",
    "        df[\"filename\"] = filename\n",
    "        \n",
    "        return df.iloc[1:] if len(df) > 1 else df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_data(data_dir, h_filter=None, min_time=0):\n",
    "    \"\"\"Load and combine all data files\"\"\"\n",
    "    dataframes = []\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        raise ValueError(f\"Data directory {data_dir} does not exist!\")\n",
    "    \n",
    "    print(f\"Loading data from: {data_dir}\")\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "    if not csv_files:\n",
    "        raise ValueError(\"No CSV files found in the data directory!\")\n",
    "    \n",
    "    for fname in csv_files:\n",
    "        params = parse_filename_params(fname)\n",
    "        if params is None or params[3] is None:\n",
    "            print(f\"Skipping (unmatched): {fname}\")\n",
    "            continue\n",
    "\n",
    "        # Apply h filter if specified\n",
    "        if h_filter is not None:\n",
    "            h_val = params[0]\n",
    "            if h_val != h_filter:\n",
    "                print(f\"Skipping (not h={h_filter}): {fname}\")\n",
    "                continue\n",
    "\n",
    "        path = os.path.join(data_dir, fname)\n",
    "        try:\n",
    "            df = load_and_process_file(path, *params[:4], filename=fname, min_time=min_time)\n",
    "            if not df.empty:\n",
    "                dataframes.append(df)\n",
    "                print(f\"Loaded: {fname} ({len(df)} rows)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {fname}: {e}\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        raise ValueError(\"No valid data files found after processing!\")\n",
    "    \n",
    "    data = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"\\nTotal data loaded: {len(data)} rows, {len(data.columns)} columns\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5af29f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4: Model Architecture\n",
    "class EnhancedThermalNet(nn.Module):\n",
    "    \"\"\"Enhanced thermal neural network with attention and residual connections\"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_dims=[512, 256, 256, 128], dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Input processing\n",
    "        self.input_layer = nn.Linear(input_size, hidden_dims[0])\n",
    "        self.input_norm = nn.LayerNorm(hidden_dims[0])\n",
    "        \n",
    "        # Feature attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_size, max(1, input_size // 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(max(1, input_size // 2), input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Hidden layers with residual connections\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.residual_projections = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            layer = nn.Linear(hidden_dims[i], hidden_dims[i + 1])\n",
    "            norm = nn.LayerNorm(hidden_dims[i + 1])\n",
    "            \n",
    "            if hidden_dims[i] != hidden_dims[i + 1]:\n",
    "                residual_proj = nn.Linear(hidden_dims[i], hidden_dims[i + 1])\n",
    "            else:\n",
    "                residual_proj = nn.Identity()\n",
    "            \n",
    "            self.layers.append(layer)\n",
    "            self.norms.append(norm)\n",
    "            self.residual_projections.append(residual_proj)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_size)\n",
    "        \n",
    "        # Activation and regularization\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using He initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply attention to input features\n",
    "        attention_weights = self.attention(x)\n",
    "        x_attended = x * attention_weights\n",
    "        \n",
    "        # Input processing\n",
    "        x = self.activation(self.input_norm(self.input_layer(x_attended)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Forward through hidden layers with residual connections\n",
    "        for i, (layer, norm, residual_proj) in enumerate(zip(self.layers, self.norms, self.residual_projections)):\n",
    "            residual = residual_proj(x)\n",
    "            x = layer(x)\n",
    "            x = norm(x)\n",
    "            x = self.activation(x)\n",
    "            x = x + residual\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "\n",
    "class PhysicsInformedLoss(nn.Module):\n",
    "    \"\"\"Physics-informed loss function\"\"\"\n",
    "    def __init__(self, smoothness_weight=0.001, gradient_weight=0.0001):\n",
    "        super().__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.smoothness_weight = smoothness_weight\n",
    "        self.gradient_weight = gradient_weight\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # Primary MSE loss\n",
    "        mse_loss = self.mse_loss(predictions, targets)\n",
    "        \n",
    "        # Smoothness penalty (spatial consistency)\n",
    "        if predictions.shape[1] > 1:\n",
    "            smoothness_loss = torch.mean(torch.abs(predictions[:, 1:] - predictions[:, :-1]))\n",
    "            gradient_loss = torch.mean(torch.abs(torch.diff(predictions, dim=1)))\n",
    "        else:\n",
    "            smoothness_loss = torch.tensor(0.0, device=predictions.device)\n",
    "            gradient_loss = torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = (mse_loss + \n",
    "                     self.smoothness_weight * smoothness_loss + \n",
    "                     self.gradient_weight * gradient_loss)\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82143980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: Data Preprocessing\n",
    "def preprocess_data(data, test_size=0.3, val_size=0.5, use_enhanced_features=True):\n",
    "    \"\"\"Enhanced data preprocessing\"\"\"\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    \n",
    "    # Time normalization\n",
    "    time_min = data[\"Time\"].min()\n",
    "    time_max = data[\"Time\"].max()\n",
    "    data[\"Time_norm\"] = (data[\"Time\"] - time_min) / (time_max - time_min)\n",
    "    \n",
    "    # Enhanced time features\n",
    "    if use_enhanced_features:\n",
    "        data[\"Time²\"] = data[\"Time_norm\"] ** 2\n",
    "        data[\"Time³\"] = data[\"Time_norm\"] ** 3\n",
    "        data[\"Time_sin\"] = np.sin(2 * np.pi * data[\"Time_norm\"])\n",
    "        data[\"Time_cos\"] = np.cos(2 * np.pi * data[\"Time_norm\"])\n",
    "        \n",
    "        # Feature interaction terms\n",
    "        data[\"flux_abs_interaction\"] = data[\"flux\"] * data[\"abs\"]\n",
    "        data[\"h_flux_interaction\"] = data[\"h\"] * data[\"flux\"]\n",
    "        \n",
    "        # Enhanced feature set\n",
    "        base_features = [\"Time_norm\", \"Time²\", \"Time³\", \"Time_sin\", \"Time_cos\", \n",
    "                        \"h\", \"flux\", \"abs\", \"surf\", \"flux_abs_interaction\", \"h_flux_interaction\"]\n",
    "    else:\n",
    "        data[\"Time²\"] = data[\"Time_norm\"] ** 2\n",
    "        base_features = [\"Time_norm\", \"Time²\", \"h\", \"flux\", \"abs\", \"surf\"]\n",
    "    \n",
    "    # Identify columns\n",
    "    theory_cols = [c for c in data.columns if c.startswith(\"Theoretical_Temps_\")]\n",
    "    tc_cols = [col for col in data.columns if (col.startswith(\"TC\") and not col.endswith(\"_rate\")) or col.startswith(\"TC_\")]\n",
    "    \n",
    "    # Filter out any columns that don't exist\n",
    "    theory_cols = [col for col in theory_cols if col in data.columns]\n",
    "    tc_cols = [col for col in tc_cols if col in data.columns]\n",
    "    \n",
    "    if not tc_cols:\n",
    "        print(\"Warning: No TC columns found. Creating dummy TC columns for demonstration.\")\n",
    "        tc_cols = ['TC_1', 'TC_2', 'TC_3', 'TC_4']\n",
    "        for col in tc_cols:\n",
    "            if col not in data.columns:\n",
    "                data[col] = 20 + np.random.normal(0, 5, len(data))\n",
    "    \n",
    "    # Prepare features and targets\n",
    "    feature_cols = base_features + theory_cols\n",
    "    # Filter out any feature columns that don't exist\n",
    "    feature_cols = [col for col in feature_cols if col in data.columns]\n",
    "    \n",
    "    X = data[feature_cols].copy()\n",
    "    y = data[tc_cols].copy()\n",
    "    filenames = data[\"filename\"] if \"filename\" in data.columns else pd.Series([\"unknown\"] * len(data))\n",
    "    \n",
    "    print(f\"Features: {len(feature_cols)}, Targets: {len(tc_cols)}\")\n",
    "    print(f\"Theory columns: {len(theory_cols)}, TC columns: {len(tc_cols)}\")\n",
    "    print(f\"Feature columns: {feature_cols}\")\n",
    "    print(f\"Target columns: {tc_cols}\")\n",
    "    \n",
    "    # Remove missing values\n",
    "    mask = ~(X.isnull().any(axis=1) | y.isnull().any(axis=1))\n",
    "    X = X[mask].reset_index(drop=True)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "    filenames = filenames[mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"After removing missing values: {len(X)} samples\")\n",
    "    \n",
    "    if len(X) < 10:\n",
    "        raise ValueError(\"Not enough valid samples for training!\")\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_temp, y_train, y_temp, _, _ = train_test_split(\n",
    "        X, y, filenames, test_size=test_size, random_state=SEED\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test, _, _ = train_test_split(\n",
    "        X_temp, y_temp, _, test_size=val_size, random_state=SEED\n",
    "    )\n",
    "    \n",
    "    # Scaling\n",
    "    X_scaler = StandardScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "    \n",
    "    X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "    X_val_scaled = X_scaler.transform(X_val)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "    \n",
    "    y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "    y_val_scaled = y_scaler.transform(y_val)\n",
    "    y_test_scaled = y_scaler.transform(y_test)\n",
    "    \n",
    "    # Save scalers and metadata\n",
    "    joblib.dump(X_scaler, \"X_scaler_enhanced.pkl\")\n",
    "    joblib.dump(y_scaler, \"y_scaler_enhanced.pkl\")\n",
    "    joblib.dump({\"time_min\": time_min, \"time_max\": time_max}, \"time_range_enhanced.pkl\")\n",
    "    joblib.dump({\"feature_cols\": feature_cols, \"tc_cols\": tc_cols}, \"column_info.pkl\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "    )\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(X_test_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=min(64, len(X_train)//4), shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=min(64, len(X_val)//4), shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=min(64, len(X_test)//4), shuffle=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba64751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6: Training Function\n",
    "def train_model(model, train_loader, val_loader, device, epochs=1000, patience=50):\n",
    "    \"\"\"Train the model with early stopping\"\"\"\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, min_lr=1e-6)\n",
    "    criterion = PhysicsInformedLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"Starting training for {epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss_epoch = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_epoch += loss.item()\n",
    "        \n",
    "        train_loss_epoch /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_epoch = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                predictions = model(X_batch)\n",
    "                loss = criterion(predictions, y_batch)\n",
    "                val_loss_epoch += loss.item()\n",
    "        \n",
    "        val_loss_epoch /= len(val_loader)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss_epoch)\n",
    "        \n",
    "        # Track metrics\n",
    "        train_losses.append(train_loss_epoch)\n",
    "        val_losses.append(val_loss_epoch)\n",
    "        \n",
    "        # Early stopping and checkpointing\n",
    "        if val_loss_epoch < best_val_loss:\n",
    "            best_val_loss = val_loss_epoch\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss_epoch,\n",
    "                'train_loss': train_loss_epoch,\n",
    "            }, 'best_thermal_model_enhanced.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Logging\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Train Loss: {train_loss_epoch:.6f} \"\n",
    "                  f\"Val Loss: {val_loss_epoch:.6f} LR: {current_lr:.8f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint = torch.load('best_thermal_model_enhanced.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model with validation loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a54e3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7: Evaluation and Visualization\n",
    "def evaluate_model(model, test_loader, y_scaler, device):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            pred = model(X_batch).cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "            actuals.append(y_batch.numpy())\n",
    "    \n",
    "    predictions = np.concatenate(predictions)\n",
    "    actuals = np.concatenate(actuals)\n",
    "    \n",
    "    # Inverse transform\n",
    "    pred_real = y_scaler.inverse_transform(predictions)\n",
    "    actual_real = y_scaler.inverse_transform(actuals)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(np.mean((pred_real - actual_real) ** 2, axis=0))\n",
    "    mae = np.mean(np.abs(pred_real - actual_real), axis=0)\n",
    "    mape = np.mean(np.abs((actual_real - pred_real) / (actual_real + 1e-8)), axis=0) * 100\n",
    "    \n",
    "    # R² score\n",
    "    y_mean = np.mean(actual_real, axis=0)\n",
    "    ss_tot = np.sum((actual_real - y_mean) ** 2, axis=0)\n",
    "    ss_res = np.sum((actual_real - pred_real) ** 2, axis=0)\n",
    "    r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "    \n",
    "    return {\n",
    "        'predictions': pred_real,\n",
    "        'actuals': actual_real,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'r2': r2,\n",
    "        'overall_rmse': np.sqrt(np.mean((pred_real - actual_real) ** 2)),\n",
    "        'overall_mae': np.mean(np.abs(pred_real - actual_real)),\n",
    "        'overall_r2': np.mean(r2)\n",
    "    }\n",
    "\n",
    "def plot_results(train_losses, val_losses, results, tc_cols):\n",
    "    \"\"\"Plot training history and results\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training history\n",
    "    axes[0, 0].plot(train_losses, label='Training Loss', color='blue')\n",
    "    axes[0, 0].plot(val_losses, label='Validation Loss', color='orange')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # RMSE by sensor\n",
    "    axes[0, 1].bar(range(len(tc_cols)), results['rmse'])\n",
    "    axes[0, 1].set_xlabel('Sensor Index')\n",
    "    axes[0, 1].set_ylabel('RMSE (°C)')\n",
    "    axes[0, 1].set_title('RMSE by Sensor')\n",
    "    axes[0, 1].set_xticks(range(len(tc_cols)))\n",
    "    axes[0, 1].set_xticklabels([f'TC{i+1}' for i in range(len(tc_cols))], rotation=45)\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[1, 0].scatter(results['actuals'].flatten(), results['predictions'].flatten(), alpha=0.5)\n",
    "    min_val = min(results['actuals'].min(), results['predictions'].min())\n",
    "    max_val = max(results['actuals'].max(), results['predictions'].max())\n",
    "    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('Actual Temperature (°C)')\n",
    "    axes[1, 0].set_ylabel('Predicted Temperature (°C)')\n",
    "    axes[1, 0].set_title('Predicted vs Actual')\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # R² scores\n",
    "    axes[1, 1].bar(range(len(tc_cols)), results['r2'])\n",
    "    axes[1, 1].set_xlabel('Sensor Index')\n",
    "    axes[1, 1].set_ylabel('R² Score')\n",
    "    axes[1, 1].set_title('R² Score by Sensor')\n",
    "    axes[1, 1].set_xticks(range(len(tc_cols)))\n",
    "    axes[1, 1].set_xticklabels([f'TC{i+1}' for i in range(len(tc_cols))], rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3c3333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8: Inference Functions\n",
    "def predict_temperature(model, X_scaler, y_scaler, time_range_data, column_info, \n",
    "                       time, h, flux, abs_val, surf, theoretical_temps, device):\n",
    "    \"\"\"\n",
    "    Inference function to predict actual temperatures from input parameters\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if len(theoretical_temps) != 10:\n",
    "        raise ValueError(\"Expected 10 theoretical temperatures, got {}\".format(len(theoretical_temps)))\n",
    "    \n",
    "    # Get time normalization parameters\n",
    "    time_min = time_range_data['time_min']\n",
    "    time_max = time_range_data['time_max']\n",
    "    \n",
    "    # Normalize time\n",
    "    time_norm = (time - time_min) / (time_max - time_min)\n",
    "    \n",
    "    # Create enhanced time features\n",
    "    time_squared = time_norm ** 2\n",
    "    time_cubed = time_norm ** 3\n",
    "    time_sin = np.sin(2 * np.pi * time_norm)\n",
    "    time_cos = np.cos(2 * np.pi * time_norm)\n",
    "    \n",
    "    # Create interaction features\n",
    "    flux_abs_interaction = flux * abs_val\n",
    "    h_flux_interaction = h * flux\n",
    "    \n",
    "    # Prepare input features\n",
    "    input_features = [\n",
    "        time_norm, time_squared, time_cubed, time_sin, time_cos,\n",
    "        h, flux, abs_val, surf, flux_abs_interaction, h_flux_interaction\n",
    "    ]\n",
    "    input_features.extend(theoretical_temps)\n",
    "    \n",
    "    # Convert to numpy array and reshape\n",
    "    input_array = np.array(input_features).reshape(1, -1)\n",
    "    \n",
    "    # Scale the input features\n",
    "    input_scaled = X_scaler.transform(input_array)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction_scaled = model(input_tensor).cpu().numpy()\n",
    "    \n",
    "    # Inverse transform the prediction\n",
    "    prediction_real = y_scaler.inverse_transform(prediction_scaled)\n",
    "    \n",
    "    # Get TC column names\n",
    "    tc_cols = column_info['tc_cols']\n",
    "    \n",
    "    # Create result dictionary\n",
    "    result = {\n",
    "        'input_parameters': {\n",
    "            'time': time,\n",
    "            'h': h,\n",
    "            'flux': flux,\n",
    "            'abs': abs_val,\n",
    "            'surf': surf,\n",
    "            'theoretical_temps': theoretical_temps\n",
    "        },\n",
    "        'predicted_temperatures': {}\n",
    "    }\n",
    "    \n",
    "    # Map predictions to TC sensor names\n",
    "    for i, tc_name in enumerate(tc_cols):\n",
    "        result['predicted_temperatures'][tc_name] = float(prediction_real[0, i])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def load_inference_components():\n",
    "    \"\"\"Load all necessary components for inference\"\"\"\n",
    "    try:\n",
    "        # Load scalers and metadata\n",
    "        X_scaler = joblib.load(\"X_scaler_enhanced.pkl\")\n",
    "        y_scaler = joblib.load(\"y_scaler_enhanced.pkl\")\n",
    "        time_range_data = joblib.load(\"time_range_enhanced.pkl\")\n",
    "        column_info = joblib.load(\"column_info.pkl\")\n",
    "        \n",
    "        # Set device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load model\n",
    "        checkpoint = torch.load('best_thermal_model_enhanced.pth', map_location=device)\n",
    "        \n",
    "        # Recreate model\n",
    "        input_size = len(column_info['feature_cols'])\n",
    "        output_size = len(column_info['tc_cols'])\n",
    "        \n",
    "        model = EnhancedThermalNet(\n",
    "            input_size=input_size,\n",
    "            output_size=output_size,\n",
    "            hidden_dims=[512, 256, 256, 128],\n",
    "            dropout_rate=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        # Load trained weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        print(\"Inference components loaded successfully!\")\n",
    "        print(f\"Model input size: {input_size}\")\n",
    "        print(f\"Model output size: {output_size}\")\n",
    "        print(f\"TC sensors: {column_info['tc_cols']}\")\n",
    "        \n",
    "        return model, X_scaler, y_scaler, time_range_data, column_info, device\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading inference components: {e}\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "def example_inference():\n",
    "    \"\"\"Example of how to use the inference function\"\"\"\n",
    "    # Load inference components\n",
    "    model, X_scaler, y_scaler, time_range_data, column_info, device = load_inference_components()\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"Failed to load inference components. Make sure the model is trained and saved.\")\n",
    "        return\n",
    "    \n",
    "    # Example input parameters\n",
    "    time = 0  # 30 minutes\n",
    "    h = 0.1575   # Heat transfer coefficient\n",
    "    flux = 25900  # Heat flux\n",
    "    abs_val = 100  # Absorption coefficient\n",
    "    surf = 0.98   # Surface emissivity\n",
    "    \n",
    "    # Example theoretical temperatures (10 values)\n",
    "    theoretical_temps = [25.5, 28.2, 31.8, 35.4, 39.1, 42.7, 46.3, 49.9, 53.6, 57.2]\n",
    "    \n",
    "    try:\n",
    "        # Make prediction\n",
    "        result = predict_temperature(\n",
    "            model, X_scaler, y_scaler, time_range_data, column_info,\n",
    "            time, h, flux, abs_val, surf, theoretical_temps, device\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TEMPERATURE PREDICTION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Time: {result['input_parameters']['time']} seconds\")\n",
    "        print(f\"Heat transfer coefficient (h): {result['input_parameters']['h']}\")\n",
    "        print(f\"Heat flux: {result['input_parameters']['flux']}\")\n",
    "        print(f\"Absorption coefficient: {result['input_parameters']['abs']}\")\n",
    "        print(f\"Surface emissivity: {result['input_parameters']['surf']}\")\n",
    "        print(\"\\nPredicted TC Temperatures:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for tc_name, temp in result['predicted_temperatures'].items():\n",
    "            print(f\"{tc_name}: {temp:.2f} °C\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "        return None\n",
    "\n",
    "def batch_predict_temperature(model, X_scaler, y_scaler, time_range_data, column_info, \n",
    "                            input_data, device):\n",
    "    \"\"\"Batch inference function for multiple predictions\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for data_point in input_data:\n",
    "        try:\n",
    "            result = predict_temperature(\n",
    "                model, X_scaler, y_scaler, time_range_data, column_info,\n",
    "                data_point['time'], data_point['h'], data_point['flux'],\n",
    "                data_point['abs'], data_point['surf'], data_point['theoretical_temps'],\n",
    "                device\n",
    "            )\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data point: {e}\")\n",
    "            results.append(None)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87015f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data directory: data/new_processed_reset\n",
      "Loading data from: data/new_processed_reset\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_431s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_585s - Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_surf0_493s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_surf0_115s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_644s-Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_719s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_825s-Sheer3_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_surf0_439s-Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_surf0_660s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_692s-Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_747s-Sheet3_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_749s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_429s - Sheet2_processed.csv (880 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv (865 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv (790 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv (826 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv (531 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv (566 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv (932 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv (912 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv (787 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv (887 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv (803 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv (795 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv (550 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv (554 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv (547 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv (532 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_389s - Sheet4_processed.csv (2139 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv (1411 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv (585 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv (1087 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv (3952 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv (870 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv (4087 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_491s - Sheet3_processed.csv (702 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv (794 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv (970 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv (790 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv (611 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv (534 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_389s - Sheet3_processed.csv (537 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_651s - Sheet4_processed.csv (554 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_699s - Sheet1_processed.csv (717 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv (849 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv (4247 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv (4621 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_613s - Sheet1_processed.csv (550 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_675s - Sheet3_processed.csv (856 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_830s - Sheet2_processed.csv (528 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv (589 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_598s - Sheet1_processed.csv (528 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_628s - Sheet4_processed.csv (539 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_630s - Sheet2_processed.csv (618 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv (621 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv (516 rows)\n",
      "\n",
      "Total data loaded: 48187 rows, 26 columns\n",
      "Starting data preprocessing...\n",
      "Features: 21, Targets: 10\n",
      "Theory columns: 10, TC columns: 10\n",
      "Feature columns: ['Time_norm', 'Time²', 'Time³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10']\n",
      "After removing missing values: 48187 samples\n",
      "Training samples: 33730\n",
      "Validation samples: 7228\n",
      "Test samples: 7229\n",
      "Using device: cuda\n",
      "Model created with 409549 parameters\n",
      "Starting training for 1000 epochs...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m     model, results, X_scaler, y_scaler, tc_cols \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 28\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel created with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[0;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[0;32m     33\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, y_scaler, device)\n",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, device, epochs, patience)\u001b[0m\n\u001b[0;32m     23\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, y_batch)\n\u001b[1;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Block 9: Main Execution\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        # Load data (with h6 filter like in your original code)\n",
    "        DATA_DIR=get_data_directory()\n",
    "        data = load_data(DATA_DIR, h_filter=h_map[6])  # Only h6 = 0.1575\n",
    "        \n",
    "        # Preprocess data\n",
    "        train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols = preprocess_data(\n",
    "            data, use_enhanced_features=True\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        model = EnhancedThermalNet(\n",
    "            input_size=train_loader.dataset.tensors[0].shape[1],\n",
    "            output_size=train_loader.dataset.tensors[1].shape[1],\n",
    "            hidden_dims=[512, 256, 256, 128],\n",
    "            dropout_rate=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "        \n",
    "        # Train model\n",
    "        train_losses, val_losses = train_model(\n",
    "            model, train_loader, val_loader, device, epochs=1000, patience=50\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        results = evaluate_model(model, test_loader, y_scaler, device)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"{'Sensor':<15} {'RMSE (°C)':<12} {'MAE (°C)':<12} {'MAPE (%)':<12} {'R² Score':<12}\")\n",
    "        print(\"-\"*65)\n",
    "        \n",
    "        for i, col in enumerate(tc_cols):\n",
    "            print(f\"{col:<15} {results['rmse'][i]:<12.3f} {results['mae'][i]:<12.3f} \"\n",
    "                  f\"{results['mape'][i]:<12.2f} {results['r2'][i]:<12.3f}\")\n",
    "        \n",
    "        print(\"-\"*65)\n",
    "        print(f\"{'Overall':<15} {results['overall_rmse']:<12.3f} {results['overall_mae']:<12.3f} \"\n",
    "              f\"{np.mean(results['mape']):<12.2f} {results['overall_r2']:<12.3f}\")\n",
    "        \n",
    "        # Plot results\n",
    "        plot_results(train_losses, val_losses, results, tc_cols)\n",
    "        \n",
    "        return model, results, X_scaler, y_scaler, tc_cols\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, results, X_scaler, y_scaler, tc_cols = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
