{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f64754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: Imports and Setup\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35101844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Configuration\n",
    "def get_data_directory():\n",
    "    \"\"\"Find the data directory dynamically\"\"\"\n",
    "    possible_paths = [\n",
    "        # \"data/new_processed_reset\",\n",
    "        # \"./data/new_processed_reset\", \n",
    "        # \"../data/new_processed_reset\",\n",
    "        # \"data\",\n",
    "        # \"./data\",\n",
    "        # \"../data\",\n",
    "        # \"D:/Research Assistant work/Github Organization/ml models/ml_models/PhysicsGuidedNeuralNetwork/data/new_processed_reset\"\n",
    "        \"data/new_processed_fix_new\",\n",
    "        \"./data/new_processed_fix_new\", \n",
    "        \"../data/new_processed_fix_new\",\n",
    "        \"data\",\n",
    "        \"./data\",\n",
    "        \"../data\",\n",
    "        \"D:/Research Assistant work/Github Organization/ml models/ml_models/PhysicsGuidedNeuralNetwork/data/new_processed_fix_new\"\n",
    "\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Found data directory: {path}\")\n",
    "            return path\n",
    "    \n",
    "    # If no directory found, ask user to specify\n",
    "    print(\"Data directory not found. Please specify the path to your data directory:\")\n",
    "    user_path = input(\"Enter data directory path: \").strip()\n",
    "    if os.path.exists(user_path):\n",
    "        return user_path\n",
    "    else:\n",
    "        raise ValueError(f\"Specified data directory {user_path} does not exist!\")\n",
    "\n",
    "DROP_COLS = [\"TC_9_5\", \"TC_Bottom_rec_groove\", \"TC_wall_ins_ext\", \"TC_bottom_ins_groove\", \"Theoretical_Temps_11\"]\n",
    "\n",
    "# File mapping dictionaries\n",
    "h_map = {2: 0.0375, 3: 0.084, 6: 0.1575}\n",
    "flux_map = {88: 25900, 78: 21250, 73: 19400}\n",
    "abs_map = {0: 3, 92: 100}\n",
    "surf_map = {0: 0.98, 1: 0.76}\n",
    "pattern = r\"h(\\d+)_flux(\\d+)_abs(\\d+)(?:_[A-Za-z0-9]+)*_surf([01])(?:_[A-Za-z0-9]+)*[\\s_]+(\\d+)s\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32497ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Data Loading Functions\n",
    "def parse_filename_params(filename):\n",
    "    \"\"\"Parse filename to extract thermal parameters\"\"\"\n",
    "    m = re.search(pattern, filename)\n",
    "    if not m:\n",
    "        return None\n",
    "    h_raw = int(m.group(1))\n",
    "    flux_raw = int(m.group(2))\n",
    "    abs_raw = int(m.group(3))\n",
    "    surf_raw = int(m.group(4))\n",
    "    t = int(m.group(5))\n",
    "\n",
    "    h = h_map.get(h_raw, h_raw)\n",
    "    flux = flux_map.get(flux_raw, flux_raw)\n",
    "    abs_ = abs_map.get(abs_raw, abs_raw)\n",
    "    surf = surf_map.get(surf_raw)\n",
    "\n",
    "    return h, flux, abs_, surf, t\n",
    "\n",
    "def load_and_process_file(path, h, flux, abs_val, surf, filename, min_time=0):\n",
    "    \"\"\"Load and process individual CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "        df = df[df[\"Time\"] >= min_time].copy()\n",
    "        \n",
    "        # Drop unwanted columns\n",
    "        df.drop(columns=[col for col in df.columns if col in DROP_COLS or col.startswith(\"Depth_\")], \n",
    "                inplace=True, errors='ignore')\n",
    "        \n",
    "        # Add parameters\n",
    "        df[\"h\"] = h\n",
    "        df[\"flux\"] = flux\n",
    "        df[\"abs\"] = abs_val\n",
    "        df[\"surf\"] = surf\n",
    "        df[\"filename\"] = filename\n",
    "        \n",
    "        return df.iloc[1:] if len(df) > 1 else df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_data(data_dir, h_filter=None, min_time=0):\n",
    "    \"\"\"Load and combine all data files\"\"\"\n",
    "    dataframes = []\n",
    "    total_files = 0\n",
    "    loaded_files = 0\n",
    "    skipped_files = 0\n",
    "    unmatched_files = 0\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        raise ValueError(f\"Data directory {data_dir} does not exist!\")\n",
    "    \n",
    "    print(f\"Loading data from: {data_dir}\")\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "    if not csv_files:\n",
    "        raise ValueError(\"No CSV files found in the data directory!\")\n",
    "    \n",
    "    for fname in csv_files:\n",
    "        total_files += 1\n",
    "        params = parse_filename_params(fname)\n",
    "        if params is None or params[3] is None:\n",
    "            print(f\"Skipping (unmatched): {fname}\")\n",
    "            unmatched_files += 1\n",
    "            continue\n",
    "\n",
    "        h_val = params[0]\n",
    "        if h_filter is not None:\n",
    "            if isinstance(h_filter, list):\n",
    "                if h_val not in h_filter:\n",
    "                    print(f\"Skipping (h={h_val} not in {h_filter}): {fname}\")\n",
    "                    skipped_files += 1\n",
    "                    continue\n",
    "            else:\n",
    "                if h_val != h_filter:\n",
    "                    print(f\"Skipping (not h={h_filter}): {fname}\")\n",
    "                    skipped_files += 1\n",
    "                    continue\n",
    "\n",
    "        path = os.path.join(data_dir, fname)\n",
    "        try:\n",
    "            df = load_and_process_file(path, *params[:4], filename=fname, min_time=min_time)\n",
    "            if not df.empty:\n",
    "                dataframes.append(df)\n",
    "                print(f\"Loaded: {fname} ({len(df)} rows)\")\n",
    "                loaded_files += 1\n",
    "            else:\n",
    "                print(f\"File {fname} has no data after processing.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {fname}: {e}\")\n",
    "            skipped_files += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"FILE PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total files scanned     : {total_files}\")\n",
    "    print(f\"Files successfully loaded: {loaded_files}\")\n",
    "    print(f\"Files skipped (filtered): {skipped_files}\")\n",
    "    print(f\"Files skipped (unmatched): {unmatched_files}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if not dataframes:\n",
    "        raise ValueError(\"No valid data files found after processing!\")\n",
    "    \n",
    "    data = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"\\nTotal data loaded: {len(data)} rows, {len(data.columns)} columns\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5af29f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4: Model Architecture\n",
    "class EnhancedThermalNet(nn.Module):\n",
    "    \"\"\"Enhanced thermal neural network with attention and residual connections\"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_dims=[512, 256, 256, 128], dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Input processing\n",
    "        self.input_layer = nn.Linear(input_size, hidden_dims[0])\n",
    "        self.input_norm = nn.LayerNorm(hidden_dims[0])\n",
    "        \n",
    "        # Feature attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_size, max(1, input_size // 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(max(1, input_size // 2), input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Hidden layers with residual connections\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.residual_projections = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            layer = nn.Linear(hidden_dims[i], hidden_dims[i + 1])\n",
    "            norm = nn.LayerNorm(hidden_dims[i + 1])\n",
    "            \n",
    "            if hidden_dims[i] != hidden_dims[i + 1]:\n",
    "                residual_proj = nn.Linear(hidden_dims[i], hidden_dims[i + 1])\n",
    "            else:\n",
    "                residual_proj = nn.Identity()\n",
    "            \n",
    "            self.layers.append(layer)\n",
    "            self.norms.append(norm)\n",
    "            self.residual_projections.append(residual_proj)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_size)\n",
    "        \n",
    "        # Activation and regularization\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using He initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply attention to input features\n",
    "        attention_weights = self.attention(x)\n",
    "        x_attended = x * attention_weights\n",
    "        \n",
    "        # Input processing\n",
    "        x = self.activation(self.input_norm(self.input_layer(x_attended)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Forward through hidden layers with residual connections\n",
    "        for i, (layer, norm, residual_proj) in enumerate(zip(self.layers, self.norms, self.residual_projections)):\n",
    "            residual = residual_proj(x)\n",
    "            x = layer(x)\n",
    "            x = norm(x)\n",
    "            x = self.activation(x)\n",
    "            x = x + residual\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c4035d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class PhysicsInformedLoss(nn.Module):\n",
    "    \"\"\"Enhanced Physics-informed loss function with professor's exact physics formula\"\"\"\n",
    "    def __init__(self, smoothness_weight=0.005, gradient_weight=0.0001, physics_weight=0.5, \n",
    "                 conservation_violation_penalty=100.0):\n",
    "        super().__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.smoothness_weight = smoothness_weight\n",
    "        self.gradient_weight = gradient_weight\n",
    "        self.physics_weight = physics_weight\n",
    "        self.conservation_violation_penalty = conservation_violation_penalty\n",
    "        \n",
    "        # Physical constants (keeping your professor's exact values)\n",
    "        self.r = 2.0375 * 0.0254  # Convert inches to meters\n",
    "        self.A_rec = np.pi * (self.r ** 2)  # Receiver area\n",
    "        self.rho = 1836.31  # Density (kg/mÂ³)\n",
    "        self.cp = 1512  # Specific heat capacity (J/kgÂ·K)\n",
    "        \n",
    "        # Convert to tensors for GPU compatibility\n",
    "        self.A_rec_tensor = None\n",
    "        self.rho_tensor = None\n",
    "        self.cp_tensor = None\n",
    "        \n",
    "        # Tracking conservation violations\n",
    "        self.violation_count = 0\n",
    "        self.total_batches = 0\n",
    "    \n",
    "    def _init_tensors(self, device):\n",
    "        \"\"\"Initialize tensors on the correct device\"\"\"\n",
    "        if self.A_rec_tensor is None:\n",
    "            self.A_rec_tensor = torch.tensor(self.A_rec, device=device, dtype=torch.float32)\n",
    "            self.rho_tensor = torch.tensor(self.rho, device=device, dtype=torch.float32)\n",
    "            self.cp_tensor = torch.tensor(self.cp, device=device, dtype=torch.float32)\n",
    "    \n",
    "    def compute_physics_loss(self, predictions, targets, inputs):\n",
    "        \"\"\"\n",
    "        Compute physics-informed loss using professor's exact energy conservation formula\n",
    "        \n",
    "        KEEPING YOUR EXACT PHYSICS:\n",
    "        Energy Conservation Law: Energy_stored â‰¤ Energy_incoming\n",
    "        - Energy_incoming = flux * A_rec (no efficiency factor as per your code)\n",
    "        - Energy_stored = mass * cp * temp_change / dt\n",
    "        - mass = rho * h * A_rec\n",
    "        \n",
    "        The key fixes are in NUMERICAL HANDLING and VIOLATION INTERPRETATION, not physics\n",
    "        \"\"\"\n",
    "        device = predictions.device\n",
    "        self._init_tensors(device)\n",
    "        \n",
    "        batch_size = predictions.shape[0]\n",
    "        self.total_batches += 1\n",
    "        \n",
    "        try:\n",
    "            # Extract relevant features from inputs (keeping your exact indices)\n",
    "            flux = inputs[:, 6]  # flux is at index 6\n",
    "            h = inputs[:, 5]     # h is at index 5\n",
    "            \n",
    "            # Calculate incoming energy (EXACT same formula as your professor's)\n",
    "            incoming_energy = flux * self.A_rec_tensor\n",
    "            \n",
    "            # Calculate mass for each batch (EXACT same formula)\n",
    "            mass = self.rho_tensor * h * self.A_rec_tensor\n",
    "            \n",
    "            # Calculate temperature change (keeping your exact approach)\n",
    "            theoretical_start_idx = 11\n",
    "            num_sensors = predictions.shape[1]\n",
    "            \n",
    "            if inputs.shape[1] >= theoretical_start_idx + num_sensors:\n",
    "                theoretical_temps = inputs[:, theoretical_start_idx:theoretical_start_idx + num_sensors]\n",
    "                temp_change = torch.mean(predictions - theoretical_temps, dim=1)\n",
    "            else:\n",
    "                # Fallback: use difference between prediction and target\n",
    "                temp_change = torch.mean(predictions - targets, dim=1)\n",
    "            \n",
    "            # Calculate total energy stored (EXACT same formula, assuming unit time step)\n",
    "            dt = torch.ones_like(temp_change)\n",
    "            total_energy_stored = mass * self.cp_tensor * temp_change / dt\n",
    "            \n",
    "            # Energy conservation constraint checking (EXACT same logic)\n",
    "            conservation_loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "            \n",
    "            # Case 1: CRITICAL VIOLATION - Energy stored > Energy incoming\n",
    "            # This violates the fundamental law of energy conservation\n",
    "            violation_mask = total_energy_stored > incoming_energy\n",
    "            # Case 1.5: SOFT MARGIN penalty â€” predictions that are close to violating\n",
    "            margin = 0.05 * incoming_energy  # 5% margin\n",
    "            soft_violation_mask = (total_energy_stored > incoming_energy - margin) & (~violation_mask)\n",
    "            if torch.any(soft_violation_mask):\n",
    "                soft_excess = total_energy_stored[soft_violation_mask] - incoming_energy[soft_violation_mask]\n",
    "                soft_relative_violation = soft_excess / (incoming_energy[soft_violation_mask] + 1e-6)\n",
    "                soft_penalty = torch.mean(soft_relative_violation ** 2)\n",
    "                conservation_loss = conservation_loss + 0.1 * self.conservation_violation_penalty * soft_penalty\n",
    "\n",
    "            # Case 2: ACCEPTABLE - Energy stored â‰¤ Energy incoming\n",
    "            acceptable_mask = total_energy_stored <= incoming_energy\n",
    "            if torch.any(acceptable_mask):\n",
    "                # Very mild penalty to encourage efficiency but not violate physics\n",
    "                energy_difference = incoming_energy[acceptable_mask] - total_energy_stored[acceptable_mask]\n",
    "                efficiency_penalty = torch.mean(energy_difference) * 0.01\n",
    "                conservation_loss = conservation_loss + efficiency_penalty\n",
    "            \n",
    "            # Case 3: Near perfect energy transfer (within small tolerance)\n",
    "            balance_tolerance = 0.01 * torch.abs(incoming_energy)\n",
    "            energy_difference = torch.abs(incoming_energy - total_energy_stored)\n",
    "            balance_mask = energy_difference <= balance_tolerance\n",
    "            if torch.any(balance_mask):\n",
    "                # Small reward for achieving good energy balance\n",
    "                balance_reward = torch.mean(energy_difference[balance_mask])\n",
    "                conservation_loss = conservation_loss - 0.05 * balance_reward\n",
    "            \n",
    "            return conservation_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Physics loss computation failed: {e}\")\n",
    "            return torch.tensor(100.0, device=device, requires_grad=True)\n",
    "    \n",
    "    def get_violation_rate(self):\n",
    "        \"\"\"Get the rate of energy conservation violations\"\"\"\n",
    "        if self.total_batches == 0:\n",
    "            return 0.0\n",
    "        # Using actual batch size from data instead of assuming 32\n",
    "        total_samples = self.total_batches * 64  # Your batch size appears to be 64\n",
    "        return self.violation_count / total_samples if total_samples > 0 else 0.0\n",
    "    \n",
    "    def reset_violation_tracking(self):\n",
    "        \"\"\"Reset violation tracking counters\"\"\"\n",
    "        self.violation_count = 0\n",
    "        self.total_batches = 0\n",
    "    \n",
    "    def forward(self, predictions, targets, inputs=None):\n",
    "        \"\"\"\n",
    "        Forward pass with enhanced physics-informed loss\n",
    "        KEEPING YOUR EXACT STRUCTURE\n",
    "        \"\"\"\n",
    "        # Primary MSE loss\n",
    "        mse_loss = self.mse_loss(predictions, targets)\n",
    "        \n",
    "        # Smoothness penalty (spatial consistency)\n",
    "        if predictions.shape[1] > 1:\n",
    "            smoothness_loss = torch.mean(torch.abs(predictions[:, 1:] - predictions[:, :-1]))\n",
    "            gradient_loss = torch.mean(torch.abs(torch.diff(predictions, dim=1)))\n",
    "        else:\n",
    "            smoothness_loss = torch.tensor(0.0, device=predictions.device)\n",
    "            gradient_loss = torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "        # Enhanced physics loss with conservation checking\n",
    "        if inputs is not None:\n",
    "            physics_loss = self.compute_physics_loss(predictions, targets, inputs)\n",
    "        else:\n",
    "            # High penalty if no inputs provided for physics computation\n",
    "            physics_loss = torch.tensor(50.0, device=predictions.device, requires_grad=True)\n",
    "        \n",
    "        # Combined loss (EXACT same structure as your code)\n",
    "        total_loss = (mse_loss + \n",
    "                     self.smoothness_weight * smoothness_loss + \n",
    "                     self.gradient_weight * gradient_loss +\n",
    "                     self.physics_weight * physics_loss)\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a87a5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, test_size=0.3, val_size=0.5, use_enhanced_features=True, max_tc_map=None):\n",
    "    \"\"\"\n",
    "    Enhanced data preprocessing to handle variable number of TC sensors.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame containing all measurements\n",
    "        max_tc_map: dict mapping h value to max TC sensor index, e.g. {0.0525: 3, 0.1050: 5, 0.1575: 10}\n",
    "    \"\"\"\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    \n",
    "    if max_tc_map is None:\n",
    "        # Updated to use actual h values rather than raw numbers\n",
    "        max_tc_map = {\n",
    "            0.0375:2, 0.084:3, 0.1575:6\n",
    "        }\n",
    "\n",
    "    # Check if all h values in data are mapped\n",
    "    unique_h_vals = data[\"h\"].unique()\n",
    "    for h_val in unique_h_vals:\n",
    "        if h_val not in max_tc_map:\n",
    "            raise ValueError(f\"No TC mapping defined for h={h_val}. Update max_tc_map.\")\n",
    "\n",
    "    # Process each h value separately\n",
    "    all_datasets = []\n",
    "    tc_cols_list = []\n",
    "    \n",
    "    for h_val in unique_h_vals:\n",
    "        h_data = data[data[\"h\"] == h_val].copy()\n",
    "        max_tc = max_tc_map[h_val]\n",
    "        print(f\"Processing h={h_val}, using first {max_tc} TC sensors\")\n",
    "\n",
    "        # Time normalization\n",
    "        time_min = h_data[\"Time\"].min()\n",
    "        time_max = h_data[\"Time\"].max()\n",
    "        h_data[\"Time_norm\"] = (h_data[\"Time\"] - time_min) / (time_max - time_min)\n",
    "\n",
    "        # Enhanced features\n",
    "        if use_enhanced_features:\n",
    "            h_data[\"TimeÂ²\"] = h_data[\"Time_norm\"] ** 2\n",
    "            h_data[\"TimeÂ³\"] = h_data[\"Time_norm\"] ** 3\n",
    "            h_data[\"Time_sin\"] = np.sin(2 * np.pi * h_data[\"Time_norm\"])\n",
    "            h_data[\"Time_cos\"] = np.cos(2 * np.pi * h_data[\"Time_norm\"])\n",
    "            h_data[\"flux_abs_interaction\"] = h_data[\"flux\"] * h_data[\"abs\"]\n",
    "            h_data[\"h_flux_interaction\"] = h_data[\"h\"] * h_data[\"flux\"]\n",
    "            base_features = [\"Time_norm\", \"TimeÂ²\", \"TimeÂ³\", \"Time_sin\", \"Time_cos\", \n",
    "                           \"h\", \"flux\", \"abs\", \"surf\", \n",
    "                           \"flux_abs_interaction\", \"h_flux_interaction\"]\n",
    "        else:\n",
    "            h_data[\"TimeÂ²\"] = h_data[\"Time_norm\"] ** 2\n",
    "            base_features = [\"Time_norm\", \"TimeÂ²\", \"h\", \"flux\", \"abs\", \"surf\"]\n",
    "\n",
    "        # Theoretical columns\n",
    "        theory_cols = [c for c in h_data.columns if c.startswith(\"Theoretical_Temps_\")]\n",
    "        theory_cols = [col for col in theory_cols if col in h_data.columns]\n",
    "\n",
    "        # TC columns (take only the first N for this h)\n",
    "        all_tc_cols = [col for col in h_data.columns if col.startswith(\"TC\")]\n",
    "        all_tc_cols.sort(key=lambda x: int(re.search(r\"\\d+\", x).group()))\n",
    "        tc_cols = all_tc_cols[:max_tc]\n",
    "        tc_cols_list.append(tc_cols)  # Save for verification\n",
    "\n",
    "        print(f\"For h={h_val}, using TC columns: {tc_cols}\")\n",
    "\n",
    "        # Features and targets\n",
    "        feature_cols = base_features + theory_cols\n",
    "        feature_cols = [col for col in feature_cols if col in h_data.columns]\n",
    "\n",
    "        X = h_data[feature_cols].copy()\n",
    "        y = h_data[tc_cols].copy()\n",
    "        filenames = h_data.get(\"filename\", pd.Series([\"unknown\"] * len(h_data)))\n",
    "\n",
    "        # Drop rows with missing values\n",
    "        mask = ~(X.isnull().any(axis=1) | y.isnull().any(axis=1))\n",
    "        X = X[mask].reset_index(drop=True)\n",
    "        y = y[mask].reset_index(drop=True)\n",
    "        filenames = filenames[mask].reset_index(drop=True)\n",
    "\n",
    "        all_datasets.append((X, y, filenames))\n",
    "\n",
    "    # Combine all datasets\n",
    "    X_combined = pd.concat([d[0] for d in all_datasets])\n",
    "    y_combined = pd.concat([d[1] for d in all_datasets])\n",
    "    filenames_combined = pd.concat([d[2] for d in all_datasets])\n",
    "\n",
    "    # Verify we have consistent TC columns across all h values\n",
    "    if len(set(tuple(cols) for cols in tc_cols_list)) > 1:\n",
    "        print(\"Warning: Different TC columns used for different h values\")\n",
    "    tc_cols = tc_cols_list[0]  # Use first set (or implement more sophisticated handling)\n",
    "\n",
    "    # Split\n",
    "    X_train, X_temp, y_train, y_temp, _, _ = train_test_split(\n",
    "        X_combined, y_combined, filenames_combined, test_size=test_size, random_state=SEED\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test, _, _ = train_test_split(\n",
    "        X_temp, y_temp, _, test_size=val_size, random_state=SEED\n",
    "    )\n",
    "\n",
    "    # Scaling\n",
    "    X_scaler = StandardScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "    X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "    X_val_scaled = X_scaler.transform(X_val)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "    y_val_scaled = y_scaler.transform(y_val)\n",
    "    y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "    # Save scalers & metadata\n",
    "    joblib.dump(X_scaler, \"X_scaler_enhanced.pkl\")\n",
    "    joblib.dump(y_scaler, \"y_scaler_enhanced.pkl\")\n",
    "    joblib.dump({\"feature_cols\": feature_cols, \"tc_cols\": tc_cols}, \"column_info.pkl\")\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "                                torch.tensor(y_train_scaled, dtype=torch.float32))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val_scaled, dtype=torch.float32),\n",
    "                              torch.tensor(y_val_scaled, dtype=torch.float32))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test_scaled, dtype=torch.float32),\n",
    "                               torch.tensor(y_test_scaled, dtype=torch.float32))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=min(64, len(X_train)//4), shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=min(64, len(X_val)//4), shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=min(64, len(X_test)//4), shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82143980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Block 5: Data Preprocessing\n",
    "# def preprocess_data(data, test_size=0.3, val_size=0.5, use_enhanced_features=True):\n",
    "#     \"\"\"Enhanced data preprocessing\"\"\"\n",
    "#     print(\"Starting data preprocessing...\")\n",
    "    \n",
    "#     # Time normalization\n",
    "#     time_min = data[\"Time\"].min()\n",
    "#     time_max = data[\"Time\"].max()\n",
    "#     data[\"Time_norm\"] = (data[\"Time\"] - time_min) / (time_max - time_min)\n",
    "    \n",
    "#     # Enhanced time features\n",
    "#     if use_enhanced_features:\n",
    "#         data[\"TimeÂ²\"] = data[\"Time_norm\"] ** 2\n",
    "#         data[\"TimeÂ³\"] = data[\"Time_norm\"] ** 3\n",
    "#         data[\"Time_sin\"] = np.sin(2 * np.pi * data[\"Time_norm\"])\n",
    "#         data[\"Time_cos\"] = np.cos(2 * np.pi * data[\"Time_norm\"])\n",
    "        \n",
    "#         # Feature interaction terms\n",
    "#         data[\"flux_abs_interaction\"] = data[\"flux\"] * data[\"abs\"]\n",
    "#         data[\"h_flux_interaction\"] = data[\"h\"] * data[\"flux\"]\n",
    "        \n",
    "#         # Enhanced feature set\n",
    "#         base_features = [\"Time_norm\", \"TimeÂ²\", \"TimeÂ³\", \"Time_sin\", \"Time_cos\", \n",
    "#                         \"h\", \"flux\", \"abs\", \"surf\", \"flux_abs_interaction\", \"h_flux_interaction\"]\n",
    "#     else:\n",
    "#         data[\"TimeÂ²\"] = data[\"Time_norm\"] ** 2\n",
    "#         base_features = [\"Time_norm\", \"TimeÂ²\", \"h\", \"flux\", \"abs\", \"surf\"]\n",
    "    \n",
    "#     # Identify columns\n",
    "#     theory_cols = [c for c in data.columns if c.startswith(\"Theoretical_Temps_\")]\n",
    "#     tc_cols = [col for col in data.columns if (col.startswith(\"TC\") and not col.endswith(\"_rate\")) or col.startswith(\"TC_\")]\n",
    "    \n",
    "#     # Filter out any columns that don't exist\n",
    "#     theory_cols = [col for col in theory_cols if col in data.columns]\n",
    "#     tc_cols = [col for col in tc_cols if col in data.columns]\n",
    "    \n",
    "#     if not tc_cols:\n",
    "#         print(\"Warning: No TC columns found. Creating dummy TC columns for demonstration.\")\n",
    "#         tc_cols = ['TC_1', 'TC_2', 'TC_3', 'TC_4']\n",
    "#         for col in tc_cols:\n",
    "#             if col not in data.columns:\n",
    "#                 data[col] = 20 + np.random.normal(0, 5, len(data))\n",
    "    \n",
    "#     # Prepare features and targets\n",
    "#     feature_cols = base_features + theory_cols\n",
    "#     # Filter out any feature columns that don't exist\n",
    "#     feature_cols = [col for col in feature_cols if col in data.columns]\n",
    "    \n",
    "#     X = data[feature_cols].copy()\n",
    "#     y = data[tc_cols].copy()\n",
    "#     filenames = data[\"filename\"] if \"filename\" in data.columns else pd.Series([\"unknown\"] * len(data))\n",
    "    \n",
    "#     print(f\"Features: {len(feature_cols)}, Targets: {len(tc_cols)}\")\n",
    "#     print(f\"Theory columns: {len(theory_cols)}, TC columns: {len(tc_cols)}\")\n",
    "#     print(f\"Feature columns: {feature_cols}\")\n",
    "#     print(f\"Target columns: {tc_cols}\")\n",
    "    \n",
    "#     # Remove missing values\n",
    "#     mask = ~(X.isnull().any(axis=1) | y.isnull().any(axis=1))\n",
    "#     X = X[mask].reset_index(drop=True)\n",
    "#     y = y[mask].reset_index(drop=True)\n",
    "#     filenames = filenames[mask].reset_index(drop=True)\n",
    "    \n",
    "#     print(f\"After removing missing values: {len(X)} samples\")\n",
    "    \n",
    "#     if len(X) < 10:\n",
    "#         raise ValueError(\"Not enough valid samples for training!\")\n",
    "    \n",
    "#     # Train-test split\n",
    "#     X_train, X_temp, y_train, y_temp, _, _ = train_test_split(\n",
    "#         X, y, filenames, test_size=test_size, random_state=SEED\n",
    "#     )\n",
    "#     X_val, X_test, y_val, y_test, _, _ = train_test_split(\n",
    "#         X_temp, y_temp, _, test_size=val_size, random_state=SEED\n",
    "#     )\n",
    "    \n",
    "#     # Scaling\n",
    "#     X_scaler = StandardScaler()\n",
    "#     y_scaler = MinMaxScaler()\n",
    "    \n",
    "#     X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "#     X_val_scaled = X_scaler.transform(X_val)\n",
    "#     X_test_scaled = X_scaler.transform(X_test)\n",
    "    \n",
    "#     y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "#     y_val_scaled = y_scaler.transform(y_val)\n",
    "#     y_test_scaled = y_scaler.transform(y_test)\n",
    "    \n",
    "#     # Save scalers and metadata\n",
    "#     joblib.dump(X_scaler, \"X_scaler_enhanced.pkl\")\n",
    "#     joblib.dump(y_scaler, \"y_scaler_enhanced.pkl\")\n",
    "#     joblib.dump({\"time_min\": time_min, \"time_max\": time_max}, \"time_range_enhanced.pkl\")\n",
    "#     joblib.dump({\"feature_cols\": feature_cols, \"tc_cols\": tc_cols}, \"column_info.pkl\")\n",
    "    \n",
    "#     # Create DataLoaders\n",
    "#     train_dataset = TensorDataset(\n",
    "#         torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "#         torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "#     )\n",
    "#     val_dataset = TensorDataset(\n",
    "#         torch.tensor(X_val_scaled, dtype=torch.float32),\n",
    "#         torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "#     )\n",
    "#     test_dataset = TensorDataset(\n",
    "#         torch.tensor(X_test_scaled, dtype=torch.float32),\n",
    "#         torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "#     )\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=min(64, len(X_train)//4), shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=min(64, len(X_val)//4), shuffle=False)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=min(64, len(X_test)//4), shuffle=False)\n",
    "    \n",
    "#     print(f\"Training samples: {len(X_train)}\")\n",
    "#     print(f\"Validation samples: {len(X_val)}\")\n",
    "#     print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "#     return train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dba64751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_conservation_monitoring(model, train_loader, val_loader, device, epochs=1000, patience=50):\n",
    "    \"\"\"Enhanced training with conservation violation monitoring - KEEPING YOUR EXACT PHYSICS\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, min_lr=1e-6)\n",
    "    \n",
    "    # Enhanced criterion with conservation violation penalty (KEEPING YOUR EXACT VALUES)\n",
    "    criterion = PhysicsInformedLoss(\n",
    "        smoothness_weight=0.005,\n",
    "        gradient_weight=0.0001,\n",
    "        physics_weight=0.5,\n",
    "        conservation_violation_penalty=100.0  # Keeping your exact penalty\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    violation_rates = []\n",
    "    \n",
    "    print(f\"Starting training with professor's energy conservation formula...\")\n",
    "    print(f\"Physics loss weight: {criterion.physics_weight}\")\n",
    "    print(f\"Conservation violation penalty: {criterion.conservation_violation_penalty}\")\n",
    "    print(f\"PHYSICS: Energy_stored = rho * h * A_rec * cp * temp_change / dt\")\n",
    "    print(f\"PHYSICS: Energy_incoming = flux * A_rec\")\n",
    "    print(f\"CONSTRAINT: Energy_stored â‰¤ Energy_incoming\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Reset violation tracking for this epoch\n",
    "        criterion.reset_violation_tracking()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss_epoch = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            \n",
    "            # Pass inputs to loss function for physics computation\n",
    "            loss = criterion(predictions, y_batch, X_batch)\n",
    "            \n",
    "            # Check for invalid loss values\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"Warning: Invalid loss detected at epoch {epoch}\")\n",
    "                continue\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_epoch += loss.item()\n",
    "        \n",
    "        train_loss_epoch /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_epoch = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                predictions = model(X_batch)\n",
    "                \n",
    "                loss = criterion(predictions, y_batch, X_batch)\n",
    "                val_loss_epoch += loss.item()\n",
    "        \n",
    "        val_loss_epoch /= len(val_loader)\n",
    "        \n",
    "        # Track violation rate\n",
    "        violation_rate = criterion.get_violation_rate()\n",
    "        violation_rates.append(violation_rate)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss_epoch)\n",
    "        \n",
    "        # Track metrics\n",
    "        train_losses.append(train_loss_epoch)\n",
    "        val_losses.append(val_loss_epoch)\n",
    "        \n",
    "        # Early stopping with conservation violation consideration\n",
    "        adjusted_val_loss = val_loss_epoch + (violation_rate * 100)  # Heavily penalize violations\n",
    "        \n",
    "        if adjusted_val_loss < best_val_loss:\n",
    "            best_val_loss = adjusted_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss_epoch,\n",
    "                'train_loss': train_loss_epoch,\n",
    "                'violation_rate': violation_rate,\n",
    "            }, 'best_thermal_model_enhanced.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # IMPROVED LOGGING: Less frequent, more informative\n",
    "        if (epoch + 1) % 100 == 0 or (epoch < 50 and (epoch + 1) % 10 == 0):\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Categorize violation rate for better understanding\n",
    "            if violation_rate < 0.01:\n",
    "                status = \"âœ… EXCELLENT\"\n",
    "            elif violation_rate < 0.05:\n",
    "                status = \"âš ï¸  ACCEPTABLE\" \n",
    "            else:\n",
    "                status = \"ðŸš¨ CONCERNING\"\n",
    "                \n",
    "            print(f\"Epoch [{epoch+1:4d}/{epochs}] \"\n",
    "                  f\"Train: {train_loss_epoch:.6f} \"\n",
    "                  f\"Val: {val_loss_epoch:.6f} \"\n",
    "                  f\"Violations: {violation_rate:.4f} ({status}) \"\n",
    "                  f\"LR: {current_lr:.8f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint = torch.load('best_thermal_model_enhanced.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING COMPLETED\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Best validation loss: {checkpoint['val_loss']:.6f}\")\n",
    "    print(f\"Final violation rate: {checkpoint['violation_rate']:.4f}\")\n",
    "    \n",
    "    # IMPROVED INTERPRETATION of violations\n",
    "    final_rate = checkpoint['violation_rate']\n",
    "    if final_rate < 0.01:\n",
    "        print(\"ðŸŽ‰ EXCELLENT: Physics constraints well learned!\")\n",
    "        print(\"   Model respects energy conservation in >99% of cases\")\n",
    "    elif final_rate < 0.05:\n",
    "        print(\"âœ… GOOD: Acceptable physics compliance\")\n",
    "        print(\"   Small violations may be due to:\")\n",
    "        print(\"   - Measurement noise in training data\")\n",
    "        print(\"   - Numerical precision limits\")\n",
    "        print(\"   - Heat losses not captured in simplified model\")\n",
    "    else:\n",
    "        print(\"âš ï¸  NEEDS ATTENTION: High violation rate\")\n",
    "        print(\"   Consider:\")\n",
    "        print(\"   - Increasing physics_weight parameter\")\n",
    "        print(\"   - Checking data preprocessing\")\n",
    "        print(\"   - Validating theoretical temperature calculations\")\n",
    "        print(\"   - Reviewing input feature scaling\")\n",
    "    \n",
    "    return train_losses, val_losses, violation_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def temperature_to_energy_per_sensor(temps, depths, rho, A_rec, cp, dt=1.0):\n",
    "#     \"\"\"\n",
    "#     Convert temperature array (samples x sensors) to energy stored per sensor.\n",
    "#     temps: np.array shape (N_samples, N_sensors) (Â°C or K difference)\n",
    "#     depths: np.array shape (N_sensors,) or scalar (m)\n",
    "#     Returns energy (Joules) same shape as temps.\n",
    "#     \"\"\"\n",
    "#     if np.isscalar(depths):\n",
    "#         depths = np.full((temps.shape[1],), depths)\n",
    "#     mass = rho * depths * A_rec  # kg per sensor\n",
    "#     energy = temps * (mass * cp) / dt  # energy = mass * cp * delta_T / dt\n",
    "#     return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def temperature_to_total_energy_binned(temp_changes, sensor_depths, rho, A_rec, cp, dt=1.0):\n",
    "#     \"\"\"\n",
    "#     Convert temperature CHANGE array to total energy stored using bins between TC sensors.\n",
    "    \n",
    "#     Args:\n",
    "#         temp_changes: np.array shape (N_samples, N_sensors) - temperature CHANGES in Â°C or K\n",
    "#         sensor_depths: np.array shape (N_sensors,) - depths in meters [deepest to surface]\n",
    "#         rho: float - density (kg/mÂ³)\n",
    "#         A_rec: float - receiver cross-sectional area (mÂ²)\n",
    "#         cp: float - specific heat capacity (J/kgÂ·K)\n",
    "#         dt: float - time step (s)\n",
    "    \n",
    "#     Returns:\n",
    "#         total_energy: np.array shape (N_samples,) - total energy for all bins\n",
    "#     \"\"\"\n",
    "#     N_samples, N_sensors = temp_changes.shape\n",
    "    \n",
    "#     if len(sensor_depths) != N_sensors:\n",
    "#         raise ValueError(f\"Mismatch: {N_sensors} sensors but {len(sensor_depths)} depths provided\")\n",
    "    \n",
    "#     # Sort depths to ensure proper ordering (deepest to surface)\n",
    "#     sorted_indices = np.argsort(sensor_depths)[::-1]  # Sort descending (deepest first)\n",
    "#     sorted_depths = sensor_depths[sorted_indices]\n",
    "#     sorted_temp_changes = temp_changes[:, sorted_indices]\n",
    "    \n",
    "#     # Calculate bins between consecutive sensors\n",
    "#     N_bins = N_sensors - 1\n",
    "#     total_energy_all_samples = np.zeros(N_samples)\n",
    "    \n",
    "#     for sample_idx in range(N_samples):\n",
    "#         sample_total_energy = 0.0\n",
    "        \n",
    "#         for bin_idx in range(N_bins):\n",
    "#             # Bin between sensor bin_idx and bin_idx+1\n",
    "#             depth_start = sorted_depths[bin_idx]      # Deeper sensor\n",
    "#             depth_end = sorted_depths[bin_idx + 1]    # Shallower sensor\n",
    "            \n",
    "#             # Bin thickness (depth difference)\n",
    "#             bin_thickness = abs(depth_start - depth_end)\n",
    "            \n",
    "#             # Average temperature CHANGE for this bin (linear interpolation between sensors)\n",
    "#             temp_change_start = sorted_temp_changes[sample_idx, bin_idx]\n",
    "#             temp_change_end = sorted_temp_changes[sample_idx, bin_idx + 1]\n",
    "#             avg_temp_change = (temp_change_start + temp_change_end) / 2.0\n",
    "            \n",
    "#             # Mass of material in this bin\n",
    "#             bin_volume = bin_thickness * A_rec\n",
    "#             bin_mass = rho * bin_volume\n",
    "            \n",
    "#             # Energy stored in this bin: mass * cp * temp_change / dt\n",
    "#             bin_energy = bin_mass * cp * avg_temp_change / dt\n",
    "            \n",
    "#             sample_total_energy += bin_energy\n",
    "        \n",
    "#         total_energy_all_samples[sample_idx] = sample_total_energy\n",
    "    \n",
    "#     return total_energy_all_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8766b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_energy_metrics(pred_temps, actual_temps, sensor_depths, rho, A_rec, cp, dt=1.0):\n",
    "#     \"\"\"\n",
    "#     Calculate total energy stored metrics for predicted vs actual temperatures.\n",
    "    \n",
    "#     Args:\n",
    "#         pred_temps: np.array shape (N_samples, N_sensors) - predicted temperatures (temperature changes)\n",
    "#         actual_temps: np.array shape (N_samples, N_sensors) - actual temperatures (temperature changes)\n",
    "#         sensor_depths: np.array shape (N_sensors,) - sensor depths\n",
    "#         rho, A_rec, cp, dt: physical constants\n",
    "    \n",
    "#     Returns:\n",
    "#         dict with energy metrics\n",
    "#     \"\"\"\n",
    "#     # Calculate total energy for predictions and actuals using the binned approach\n",
    "#     total_energy_pred = temperature_to_total_energy_binned(\n",
    "#         pred_temps, sensor_depths, rho, A_rec, cp, dt\n",
    "#     )\n",
    "#     total_energy_actual = temperature_to_total_energy_binned(\n",
    "#         actual_temps, sensor_depths, rho, A_rec, cp, dt\n",
    "#     )\n",
    "    \n",
    "#     # Calculate metrics\n",
    "#     energy_diff = total_energy_actual - total_energy_pred\n",
    "#     rmse_energy = np.sqrt(np.mean(energy_diff ** 2))\n",
    "#     mae_energy = np.mean(np.abs(energy_diff))\n",
    "#     mape_energy = np.mean(np.abs(energy_diff / (total_energy_actual + 1e-8))) * 100\n",
    "    \n",
    "#     # RÂ² score\n",
    "#     ss_tot = np.sum((total_energy_actual - np.mean(total_energy_actual)) ** 2)\n",
    "#     ss_res = np.sum(energy_diff ** 2)\n",
    "#     r2_energy = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "    \n",
    "#     return {\n",
    "#         'total_energy_predicted': total_energy_pred,\n",
    "#         'total_energy_actual': total_energy_actual,\n",
    "#         'rmse_energy': rmse_energy,\n",
    "#         'mae_energy': mae_energy, \n",
    "#         'mape_energy': mape_energy,\n",
    "#         'r2_energy': r2_energy,\n",
    "#         'mean_actual_energy': np.mean(total_energy_actual),\n",
    "#         'std_actual_energy': np.std(total_energy_actual)\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a54e3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7: Evaluation and Visualization\n",
    "def evaluate_model(model, test_loader, y_scaler, device, sensor_depths, sensor_names):\n",
    "    \"\"\"Comprehensive model evaluation with temperature and energy stored metrics.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            pred = model(X_batch).cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "            actuals.append(y_batch.numpy())\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "    \n",
    "    # Inverse transform to real temperatures\n",
    "    pred_real = y_scaler.inverse_transform(predictions)\n",
    "    actual_real = y_scaler.inverse_transform(actuals)\n",
    "    \n",
    "    # Temperature-based metrics\n",
    "    rmse_temp = np.sqrt(np.mean((pred_real - actual_real) ** 2, axis=0))\n",
    "    mae_temp = np.mean(np.abs(pred_real - actual_real), axis=0)\n",
    "    mape_temp = np.mean(np.abs((actual_real - pred_real) / (actual_real + 1e-8)), axis=0) * 100\n",
    "    y_mean = np.mean(actual_real, axis=0)\n",
    "    ss_tot = np.sum((actual_real - y_mean) ** 2, axis=0)\n",
    "    ss_res = np.sum((actual_real - pred_real) ** 2, axis=0)\n",
    "    r2_temp = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "    \n",
    "    overall_rmse_temp = np.sqrt(np.mean((pred_real - actual_real) ** 2))\n",
    "    overall_mae_temp = np.mean(np.abs(pred_real - actual_real))\n",
    "    overall_r2_temp = np.mean(r2_temp)\n",
    "    \n",
    "    # Constants for energy calculation\n",
    "    rho = 1836.31  # kg/mÂ³\n",
    "    r = 2.0375 * 0.0254  # radius in meters (converted from inches)\n",
    "    A_rec = np.pi * r**2  # receiver cross-sectional area (mÂ²)\n",
    "    cp = 1512  # J/kgÂ·K\n",
    "    dt = 1.0  # time step (s)\n",
    "    \n",
    "    # Convert temps to energy stored per sensor (Joules)\n",
    "    # energy_pred = temperature_to_energy_per_sensor(pred_real, sensor_depths, rho, A_rec, cp, dt)\n",
    "    # energy_actual = temperature_to_energy_per_sensor(actual_real, sensor_depths, rho, A_rec, cp, dt)\n",
    "    \n",
    "    # Energy-based metrics\n",
    "    # rmse_energy = np.sqrt(np.mean((energy_pred - energy_actual) ** 2, axis=0))\n",
    "    # mae_energy = np.mean(np.abs(energy_pred - energy_actual), axis=0)\n",
    "    # mape_energy = np.mean(np.abs((energy_actual - energy_pred) / (energy_actual + 1e-8)), axis=0) * 100\n",
    "    # y_mean_energy = np.mean(energy_actual, axis=0)\n",
    "    # ss_tot_energy = np.sum((energy_actual - y_mean_energy) ** 2, axis=0)\n",
    "    # ss_res_energy = np.sum((energy_actual - energy_pred) ** 2, axis=0)\n",
    "    # r2_energy = 1 - (ss_res_energy / (ss_tot_energy + 1e-8))\n",
    "    \n",
    "    # overall_rmse_energy = np.sqrt(np.mean((energy_pred - energy_actual) ** 2))\n",
    "    # overall_mae_energy = np.mean(np.abs(energy_pred - energy_actual))\n",
    "    # overall_r2_energy = np.mean(r2_energy)\n",
    "\n",
    "    # # --- NEW: Calculate typical energy stored magnitude for actual data ---\n",
    "    # mean_energy_per_sensor = np.mean(energy_actual, axis=0)\n",
    "    # median_energy_per_sensor = np.median(energy_actual, axis=0)\n",
    "    # overall_mean_energy = np.mean(energy_actual)\n",
    "    \n",
    "    # print(\"\\n--- Energy Stored Magnitude Summary ---\")\n",
    "    # for i, sensor in enumerate(sensor_names):\n",
    "    #     print(f\"{sensor}: Mean = {mean_energy_per_sensor[i]:.1f} W, Median = {median_energy_per_sensor[i]:.1f} W\")\n",
    "    # print(f\"Overall mean energy stored (all sensors): {overall_mean_energy:.1f} W\")\n",
    "    \n",
    "\n",
    "    # ----\n",
    "    # Your model has TC1 deepest, TC10 surface\n",
    "    # So reverse all metrics so index 0 corresponds to TC10 at surface for display\n",
    "    # ----\n",
    "\n",
    "    rmse_temp_rev = rmse_temp[::-1]\n",
    "    mae_temp_rev = mae_temp[::-1]\n",
    "    mape_temp_rev = mape_temp[::-1]\n",
    "    r2_temp_rev = r2_temp[::-1]\n",
    "\n",
    "    # rmse_energy_rev = rmse_energy[::-1]\n",
    "    # mae_energy_rev = mae_energy[::-1]\n",
    "    # mape_energy_rev = mape_energy[::-1]\n",
    "    # r2_energy_rev = r2_energy[::-1]\n",
    "\n",
    "    sensor_names_rev = sensor_names[::-1]  # TC10...TC1\n",
    "    \n",
    "    # Return all results including reversed arrays for display\n",
    "    return {\n",
    "        'predictions': pred_real,\n",
    "        'actuals': actual_real,\n",
    "        'rmse_temp': rmse_temp_rev,\n",
    "        'mae_temp': mae_temp_rev,\n",
    "        'mape_temp': mape_temp_rev,\n",
    "        'r2_temp': r2_temp_rev,\n",
    "        'overall_rmse_temp': overall_rmse_temp,\n",
    "        'overall_mae_temp': overall_mae_temp,\n",
    "        'overall_r2_temp': overall_r2_temp,\n",
    "        # 'overall_mean_energy': overall_mean_energy,\n",
    "        # 'rmse_energy': rmse_energy_rev,\n",
    "        # 'mae_energy': mae_energy_rev,\n",
    "        # 'mape_energy': mape_energy_rev,\n",
    "        # 'r2_energy': r2_energy_rev,\n",
    "        # 'overall_rmse_energy': overall_rmse_energy,\n",
    "        # 'overall_mae_energy': overall_mae_energy,\n",
    "        # 'overall_r2_energy': overall_r2_energy,\n",
    "        'sensor_names_rev': sensor_names_rev,\n",
    "    }\n",
    "\n",
    "def plot_results(train_losses, val_losses, results, tc_cols, violation_rates=None):\n",
    "    \"\"\"Plot training history and results with optional conservation monitoring\"\"\"\n",
    "    \n",
    "    # Determine subplot layout based on whether we have violation rates\n",
    "    if violation_rates is not None:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 10))  # 2x3 grid to include violation rates\n",
    "    else:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))   # Original 2x2 grid\n",
    "    \n",
    "    # Training history\n",
    "    axes[0, 0].plot(train_losses, label='Training Loss', color='blue')\n",
    "    axes[0, 0].plot(val_losses, label='Validation Loss', color='orange')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # RMSE by sensor (temperature)\n",
    "    sensor_names_rev = results.get('sensor_names_rev', tc_cols[::-1])\n",
    "    axes[0, 1].bar(range(len(sensor_names_rev)), results['rmse_temp'])\n",
    "    axes[0, 1].set_xlabel('Sensor')\n",
    "    axes[0, 1].set_ylabel('RMSE (Â°C)')\n",
    "    axes[0, 1].set_title('RMSE by Sensor (Temperature)')\n",
    "    axes[0, 1].set_xticks(range(len(sensor_names_rev)))\n",
    "    axes[0, 1].set_xticklabels(sensor_names_rev, rotation=45)\n",
    "    \n",
    "    # Conservation violation rates (if provided)\n",
    "    if violation_rates is not None:\n",
    "        axes[0, 2].plot(violation_rates, color='red', linewidth=2)\n",
    "        axes[0, 2].set_xlabel('Epoch')\n",
    "        axes[0, 2].set_ylabel('Violation Rate')\n",
    "        axes[0, 2].set_title('Energy Conservation Violation Rate')\n",
    "        axes[0, 2].grid(True)\n",
    "        \n",
    "        # Add horizontal line at acceptable threshold (e.g., 1%)\n",
    "        axes[0, 2].axhline(y=0.01, color='orange', linestyle='--', alpha=0.7, \n",
    "                          label='Acceptable Threshold (1%)')\n",
    "        axes[0, 2].legend()\n",
    "        \n",
    "        # Color-code the background based on violation severity\n",
    "        if len(violation_rates) > 0:\n",
    "            final_rate = violation_rates[-1]\n",
    "            if final_rate > 0.05:  # > 5% violations\n",
    "                axes[0, 2].set_facecolor('#ffeeee')  # Light red background\n",
    "            elif final_rate > 0.01:  # > 1% violations\n",
    "                axes[0, 2].set_facecolor('#fff8ee')  # Light yellow background\n",
    "            else:  # < 1% violations\n",
    "                axes[0, 2].set_facecolor('#eeffee')  # Light green background\n",
    "\n",
    "    # Scatter plot\n",
    "    axes[1, 0].scatter(results['actuals'].flatten(), results['predictions'].flatten(), alpha=0.5)\n",
    "    min_val = min(results['actuals'].min(), results['predictions'].min())\n",
    "    max_val = max(results['actuals'].max(), results['predictions'].max())\n",
    "    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('Actual Temperature (Â°C)')\n",
    "    axes[1, 0].set_ylabel('Predicted Temperature (Â°C)')\n",
    "    axes[1, 0].set_title('Predicted vs Actual')\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # RÂ² scores by sensor (temperature)\n",
    "    axes[1, 1].bar(range(len(sensor_names_rev)), results['r2_temp'])\n",
    "    axes[1, 1].set_xlabel('Sensor')\n",
    "    axes[1, 1].set_ylabel('RÂ² Score')\n",
    "    axes[1, 1].set_title('RÂ² Score by Sensor (Temperature)')\n",
    "    axes[1, 1].set_xticks(range(len(sensor_names_rev)))\n",
    "    axes[1, 1].set_xticklabels(sensor_names_rev, rotation=45)\n",
    "    \n",
    "    # Additional conservation analysis (if violation rates provided)\n",
    "    if violation_rates is not None:\n",
    "        violation_stats = {\n",
    "            'Final Rate': violation_rates[-1] if violation_rates else 0,\n",
    "            'Max Rate': max(violation_rates) if violation_rates else 0,\n",
    "            'Avg Rate': np.mean(violation_rates) if violation_rates else 0,\n",
    "            'Min Rate': min(violation_rates) if violation_rates else 0\n",
    "        }\n",
    "        \n",
    "        bars = axes[1, 2].bar(violation_stats.keys(), violation_stats.values())\n",
    "        axes[1, 2].set_ylabel('Violation Rate')\n",
    "        axes[1, 2].set_title('Conservation Violation Statistics')\n",
    "        axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        for i, (key, value) in enumerate(violation_stats.items()):\n",
    "            if value > 0.05:\n",
    "                bars[i].set_color('red')\n",
    "            elif value > 0.01:\n",
    "                bars[i].set_color('orange')\n",
    "            else:\n",
    "                bars[i].set_color('green')\n",
    "        \n",
    "        for i, (key, value) in enumerate(violation_stats.items()):\n",
    "            axes[1, 2].text(i, value + max(violation_stats.values()) * 0.01, \n",
    "                           f'{value:.4f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if violation_rates is not None and len(violation_rates) > 0:\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"CONSERVATION MONITORING SUMMARY\")\n",
    "        print(\"=\"*40)\n",
    "        final_rate = violation_rates[-1]\n",
    "        max_rate = max(violation_rates)\n",
    "        avg_rate = np.mean(violation_rates)\n",
    "        \n",
    "        print(f\"Final violation rate: {final_rate:.4f} ({final_rate*100:.2f}%)\")\n",
    "        print(f\"Maximum violation rate: {max_rate:.4f} ({max_rate*100:.2f}%)\")\n",
    "        print(f\"Average violation rate: {avg_rate:.4f} ({avg_rate*100:.2f}%)\")\n",
    "        \n",
    "        if final_rate < 0.01:\n",
    "            print(\"âœ… EXCELLENT: Energy conservation successfully learned!\")\n",
    "        elif final_rate < 0.05:\n",
    "            print(\"âš ï¸  WARNING: Moderate violation rate. Consider tuning physics_weight.\")\n",
    "        else:\n",
    "            print(\"âŒ CRITICAL: High violation rate. Increase conservation_violation_penalty!\")\n",
    "        print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3c3333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8: Inference Functions\n",
    "def predict_temperature(model, X_scaler, y_scaler, time_range_data, column_info, \n",
    "                       time, h, flux, abs_val, surf, theoretical_temps, device):\n",
    "    \"\"\"\n",
    "    Inference function to predict actual temperatures from input parameters\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if len(theoretical_temps) != 10:\n",
    "        raise ValueError(\"Expected 10 theoretical temperatures, got {}\".format(len(theoretical_temps)))\n",
    "    \n",
    "    # Get time normalization parameters\n",
    "    time_min = time_range_data['time_min']\n",
    "    time_max = time_range_data['time_max']\n",
    "    \n",
    "    # Normalize time\n",
    "    time_norm = (time - time_min) / (time_max - time_min)\n",
    "    \n",
    "    # Create enhanced time features\n",
    "    time_squared = time_norm ** 2\n",
    "    time_cubed = time_norm ** 3\n",
    "    time_sin = np.sin(2 * np.pi * time_norm)\n",
    "    time_cos = np.cos(2 * np.pi * time_norm)\n",
    "    \n",
    "    # Create interaction features\n",
    "    flux_abs_interaction = flux * abs_val\n",
    "    h_flux_interaction = h * flux\n",
    "    \n",
    "    # Prepare input features\n",
    "    input_features = [\n",
    "        time_norm, time_squared, time_cubed, time_sin, time_cos,\n",
    "        h, flux, abs_val, surf, flux_abs_interaction, h_flux_interaction\n",
    "    ]\n",
    "    input_features.extend(theoretical_temps)\n",
    "    \n",
    "    # Convert to numpy array and reshape\n",
    "    input_array = np.array(input_features).reshape(1, -1)\n",
    "    \n",
    "    # Scale the input features\n",
    "    input_scaled = X_scaler.transform(input_array)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction_scaled = model(input_tensor).cpu().numpy()\n",
    "    \n",
    "    # Inverse transform the prediction\n",
    "    prediction_real = y_scaler.inverse_transform(prediction_scaled)\n",
    "    \n",
    "    # Get TC column names\n",
    "    tc_cols = column_info['tc_cols']\n",
    "    \n",
    "    # Create result dictionary\n",
    "    result = {\n",
    "        'input_parameters': {\n",
    "            'time': time,\n",
    "            'h': h,\n",
    "            'flux': flux,\n",
    "            'abs': abs_val,\n",
    "            'surf': surf,\n",
    "            'theoretical_temps': theoretical_temps\n",
    "        },\n",
    "        'predicted_temperatures': {}\n",
    "    }\n",
    "    \n",
    "    # Map predictions to TC sensor names\n",
    "    for i, tc_name in enumerate(tc_cols):\n",
    "        result['predicted_temperatures'][tc_name] = float(prediction_real[0, i])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def load_inference_components():\n",
    "    \"\"\"Load all necessary components for inference\"\"\"\n",
    "    try:\n",
    "        # Load scalers and metadata\n",
    "        X_scaler = joblib.load(\"X_scaler_enhanced.pkl\")\n",
    "        y_scaler = joblib.load(\"y_scaler_enhanced.pkl\")\n",
    "        time_range_data = joblib.load(\"time_range_enhanced.pkl\")\n",
    "        column_info = joblib.load(\"column_info.pkl\")\n",
    "        \n",
    "        # Set device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load model\n",
    "        checkpoint = torch.load('best_thermal_model_enhanced.pth', map_location=device)\n",
    "        \n",
    "        # Recreate model\n",
    "        input_size = len(column_info['feature_cols'])\n",
    "        output_size = len(column_info['tc_cols'])\n",
    "        \n",
    "        model = EnhancedThermalNet(\n",
    "            input_size=input_size,\n",
    "            output_size=output_size,\n",
    "            hidden_dims=[512, 256, 256, 128],\n",
    "            dropout_rate=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        # Load trained weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        print(\"Inference components loaded successfully!\")\n",
    "        print(f\"Model input size: {input_size}\")\n",
    "        print(f\"Model output size: {output_size}\")\n",
    "        print(f\"TC sensors: {column_info['tc_cols']}\")\n",
    "        \n",
    "        return model, X_scaler, y_scaler, time_range_data, column_info, device\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading inference components: {e}\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "def example_inference():\n",
    "    \"\"\"Example of how to use the inference function\"\"\"\n",
    "    # Load inference components\n",
    "    model, X_scaler, y_scaler, time_range_data, column_info, device = load_inference_components()\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"Failed to load inference components. Make sure the model is trained and saved.\")\n",
    "        return\n",
    "    \n",
    "    # Example input parameters\n",
    "    time = 0  # 30 minutes\n",
    "    h = 0.1575   # Heat transfer coefficient\n",
    "    flux = 25900  # Heat flux\n",
    "    abs_val = 20  # Absorption coefficient\n",
    "    surf = 0.98   # Surface emissivity\n",
    "    \n",
    "    # Example theoretical temperatures (10 values)\n",
    "    theoretical_temps = [322.346598107413,344.707379405421,344.707598403347,342.463078051269,332.928870144283,324.216781541098,318.02660925491,315.821244548393,315.821244548393,315.821244548393]\n",
    "    \n",
    "    try:\n",
    "        # Make prediction\n",
    "        result = predict_temperature(\n",
    "            model, X_scaler, y_scaler, time_range_data, column_info,\n",
    "            time, h, flux, abs_val, surf, theoretical_temps, device\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TEMPERATURE PREDICTION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Time: {result['input_parameters']['time']} seconds\")\n",
    "        print(f\"Heat transfer coefficient (h): {result['input_parameters']['h']}\")\n",
    "        print(f\"Heat flux: {result['input_parameters']['flux']}\")\n",
    "        print(f\"Absorption coefficient: {result['input_parameters']['abs']}\")\n",
    "        print(f\"Surface emissivity: {result['input_parameters']['surf']}\")\n",
    "        print(\"\\nPredicted TC Temperatures:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for tc_name, temp in result['predicted_temperatures'].items():\n",
    "            print(f\"{tc_name}: {temp:.2f} Â°C\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "        return None\n",
    "\n",
    "def batch_predict_temperature(model, X_scaler, y_scaler, time_range_data, column_info, \n",
    "                            input_data, device):\n",
    "    \"\"\"Batch inference function for multiple predictions\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for data_point in input_data:\n",
    "        try:\n",
    "            result = predict_temperature(\n",
    "                model, X_scaler, y_scaler, time_range_data, column_info,\n",
    "                data_point['time'], data_point['h'], data_point['flux'],\n",
    "                data_point['abs'], data_point['surf'], data_point['theoretical_temps'],\n",
    "                device\n",
    "            )\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data point: {e}\")\n",
    "            results.append(None)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d2535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/new_processed_fix_new with h_filter=[0.1575, 0.0375]\n",
      "Error loading cleaned_h6_flux88_abs20_surf0_699s - Sheet1_processed.csv: operands could not be broadcast together with shapes (718,) (2,) \n",
      "Error loading cleaned_h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv: operands could not be broadcast together with shapes (623,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv: operands could not be broadcast together with shapes (533,) (2,) \n",
      "Error loading cleaned_h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv: operands could not be broadcast together with shapes (577,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv: operands could not be broadcast together with shapes (511,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv: operands could not be broadcast together with shapes (4622,) (2,) \n",
      "Error loading cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv: operands could not be broadcast together with shapes (567,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv: operands could not be broadcast together with shapes (622,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs92_surf0_630s - Sheet2_processed.csv: operands could not be broadcast together with shapes (619,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv: operands could not be broadcast together with shapes (517,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs20_surf1_675s - Sheet3_processed.csv: operands could not be broadcast together with shapes (857,) (2,) \n",
      "Error loading cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv: operands could not be broadcast together with shapes (827,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv: operands could not be broadcast together with shapes (515,) (2,) \n",
      "Error loading cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv: operands could not be broadcast together with shapes (796,) (2,) \n",
      "Error loading cleaned_h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv: operands could not be broadcast together with shapes (577,) (2,) \n",
      "Error loading cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv: operands could not be broadcast together with shapes (933,) (2,) \n",
      "Error loading cleaned_h2_flux88_abs0_surf0_585s - Sheet2_processed.csv: operands could not be broadcast together with shapes (565,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs92_surf0_598s - Sheet1_processed.csv: operands could not be broadcast together with shapes (529,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs25_surf0_439s-Sheet2_processed.csv: operands could not be broadcast together with shapes (587,) (2,) \n",
      "Error loading cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv: operands could not be broadcast together with shapes (913,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs0_surf0_719s-Sheet1_processed.csv: operands could not be broadcast together with shapes (531,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv: operands could not be broadcast together with shapes (1088,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs0_surf0_644s-Sheet2_processed.csv: operands could not be broadcast together with shapes (554,) (2,) \n",
      "Error loading cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv: operands could not be broadcast together with shapes (791,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv: operands could not be broadcast together with shapes (542,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs90_surf0_749s-Sheet1_processed.csv: operands could not be broadcast together with shapes (541,) (2,) \n",
      "Error loading cleaned_h6_flux73_abs0_surf0_429s - Sheet2_processed.csv: operands could not be broadcast together with shapes (881,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv: operands could not be broadcast together with shapes (586,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv: operands could not be broadcast together with shapes (4248,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv: operands could not be broadcast together with shapes (612,) (2,) \n",
      "Error loading cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv: operands could not be broadcast together with shapes (555,) (2,) \n",
      "Error loading cleaned_h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv: operands could not be broadcast together with shapes (502,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs25_surf0_660s-Sheet1_processed.csv: operands could not be broadcast together with shapes (552,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs90_surf0_692s-Sheet2_processed.csv: operands could not be broadcast together with shapes (623,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv: operands could not be broadcast together with shapes (568,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv: operands could not be broadcast together with shapes (590,) (2,) \n",
      "Error loading cleaned_h2_flux88_abs0_surf0_431s - Sheet1_processed.csv: operands could not be broadcast together with shapes (561,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv: operands could not be broadcast together with shapes (971,) (2,) \n",
      "Error loading cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv: operands could not be broadcast together with shapes (804,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv: operands could not be broadcast together with shapes (1412,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs20_surf1_830s - Sheet2_processed.csv: operands could not be broadcast together with shapes (529,) (2,) \n",
      "Error loading cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv: operands could not be broadcast together with shapes (788,) (2,) \n",
      "Error loading cleaned_h2_flux88_abs25_surf0_493s - Sheet1_processed.csv: operands could not be broadcast together with shapes (573,) (2,) \n",
      "Error loading cleaned_h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv: operands could not be broadcast together with shapes (549,) (2,) \n",
      "Error loading cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv: operands could not be broadcast together with shapes (532,) (2,) \n",
      "Error loading cleaned_h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv: operands could not be broadcast together with shapes (704,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv: operands could not be broadcast together with shapes (512,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs0_surf1_491s - Sheet3_processed.csv: operands could not be broadcast together with shapes (703,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv: operands could not be broadcast together with shapes (548,) (2,) \n",
      "Error loading cleaned_h2_flux88_abs92_surf0_115s - Sheet1_processed.csv: operands could not be broadcast together with shapes (501,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv: operands could not be broadcast together with shapes (3953,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv: operands could not be broadcast together with shapes (575,) (2,) \n",
      "Error loading cleaned_h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv: operands could not be broadcast together with shapes (541,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs20_surf0_651s - Sheet4_processed.csv: operands could not be broadcast together with shapes (555,) (2,) \n",
      "Error loading cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv: operands could not be broadcast together with shapes (866,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv: operands could not be broadcast together with shapes (817,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv: operands could not be broadcast together with shapes (871,) (2,) \n",
      "Error loading cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv: operands could not be broadcast together with shapes (551,) (2,) \n",
      "Error loading cleaned_h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv: operands could not be broadcast together with shapes (509,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv: operands could not be broadcast together with shapes (515,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs92_surf0_628s - Sheet4_processed.csv: operands could not be broadcast together with shapes (540,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs20_surf0_389s - Sheet3_processed.csv: operands could not be broadcast together with shapes (538,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv: operands could not be broadcast together with shapes (613,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs0_surf0_389s - Sheet4_processed.csv: operands could not be broadcast together with shapes (2140,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv: operands could not be broadcast together with shapes (556,) (2,) \n",
      "Error loading cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv: operands could not be broadcast together with shapes (888,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs0_surf0_825s-Sheer3_processed.csv: operands could not be broadcast together with shapes (853,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv: operands could not be broadcast together with shapes (791,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv: operands could not be broadcast together with shapes (556,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs20_surf1_613s - Sheet1_processed.csv: operands could not be broadcast together with shapes (551,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv: operands could not be broadcast together with shapes (795,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs90_surf0_747s-Sheet3_processed.csv: operands could not be broadcast together with shapes (511,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv: operands could not be broadcast together with shapes (4088,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv: operands could not be broadcast together with shapes (850,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv: operands could not be broadcast together with shapes (506,) (2,) \n",
      "Error loading cleaned_h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv: operands could not be broadcast together with shapes (623,) (2,) \n",
      "Error loading cleaned_h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv: operands could not be broadcast together with shapes (535,) (2,) \n",
      "No data for h_filter, trying without filter...\n",
      "Loaded 66146 rows with columns: ['Time', 'TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10', 'Theoretical_Temps_11', 'h', 'flux', 'abs', 'surf', 'filename', 'TC_9_5']\n",
      "Unique h values: [0.1575 0.0375 0.084 ]\n",
      "Starting data preprocessing...\n",
      "Features: 22, Targets: 11\n",
      "Theory columns: 11, TC columns: 11\n",
      "Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10', 'Theoretical_Temps_11']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10', 'TC_9_5']\n",
      "After removing missing values: 3417 samples\n",
      "Training samples: 2391\n",
      "Validation samples: 513\n",
      "Test samples: 513\n",
      "Using device: cpu\n",
      "Model created with 245515 parameters\n",
      "Starting training with professor's energy conservation formula...\n",
      "Physics loss weight: 0.5\n",
      "Conservation violation penalty: 100.0\n",
      "PHYSICS: Energy_stored = rho * h * A_rec * cp * temp_change / dt\n",
      "PHYSICS: Energy_incoming = flux * A_rec\n",
      "CONSTRAINT: Energy_stored â‰¤ Energy_incoming\n",
      "Epoch [  10/1000] Train: 0.018427 Val: 0.005866 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  20/1000] Train: 0.011290 Val: 0.004489 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  30/1000] Train: 0.009420 Val: 0.005203 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  40/1000] Train: 0.008329 Val: 0.004218 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  50/1000] Train: 0.008023 Val: 0.003923 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [ 100/1000] Train: 0.006297 Val: 0.003248 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00050000\n",
      "Epoch [ 200/1000] Train: 0.005504 Val: 0.002871 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00012500\n",
      "Epoch [ 300/1000] Train: 0.004466 Val: 0.001710 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00012500\n",
      "Epoch [ 400/1000] Train: 0.003893 Val: 0.001048 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00006250\n",
      "Epoch [ 500/1000] Train: 0.003778 Val: 0.001047 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00001563\n",
      "Early stopping at epoch 588\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "============================================================\n",
      "Best validation loss: 0.000794\n",
      "Final violation rate: 0.0000\n",
      "ðŸŽ‰ EXCELLENT: Physics constraints well learned!\n",
      "   Model respects energy conservation in >99% of cases\n",
      "Error in main execution: operands could not be broadcast together with shapes (513,11) (10,) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/nd/_q4p5fk90w5bcfnfkgkc8mhc0000gn/T/ipykernel_3884/701093163.py\", line 37, in main\n",
      "    results = evaluate_model(model, test_loader, y_scaler, device, sensor_depths, sensor_names)\n",
      "  File \"/var/folders/nd/_q4p5fk90w5bcfnfkgkc8mhc0000gn/T/ipykernel_3884/2652729663.py\", line 43, in evaluate_model\n",
      "    energy_pred = temperature_to_energy_per_sensor(pred_real, sensor_depths, rho, A_rec, cp, dt)\n",
      "  File \"/var/folders/nd/_q4p5fk90w5bcfnfkgkc8mhc0000gn/T/ipykernel_3884/3911901887.py\", line 11, in temperature_to_energy_per_sensor\n",
      "    energy = temps * (mass * cp) / dt  # energy = mass * cp * delta_T / dt\n",
      "             ~~~~~~^~~~~~~~~~~~~\n",
      "ValueError: operands could not be broadcast together with shapes (513,11) (10,) \n"
     ]
    }
   ],
   "source": [
    "# # Block 9: Main Execution [Best working]\n",
    "# def main():\n",
    "#     \"\"\"Main execution function\"\"\"\n",
    "#     try:\n",
    "#         # Load data (with h6 filter like in your original code)\n",
    "#         DATA_DIR = get_data_directory()\n",
    "#         data = load_data(DATA_DIR, h_filter=[h_map[6],h_map[2]])  # Only h6 = 0.1575\n",
    "        \n",
    "#         # Preprocess data\n",
    "#         train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols = preprocess_data(\n",
    "#             data, use_enhanced_features=True\n",
    "#         )\n",
    "        \n",
    "#         # Create model\n",
    "#         device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#         print(f\"Using device: {device}\")\n",
    "        \n",
    "#         model = EnhancedThermalNet(\n",
    "#             input_size=train_loader.dataset.tensors[0].shape[1],\n",
    "#             output_size=train_loader.dataset.tensors[1].shape[1],\n",
    "#             hidden_dims=[512, 256, 256, 128],\n",
    "#             dropout_rate=0.2\n",
    "#         ).to(device)\n",
    "        \n",
    "#         print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "        \n",
    "#         # Train model with enhanced conservation monitoring\n",
    "#         train_losses, val_losses, violation_rates = train_model_with_conservation_monitoring(\n",
    "#             model, train_loader, val_loader, device, epochs=1000, patience=50\n",
    "#         )\n",
    "        \n",
    "#         # Evaluate model\n",
    "#         sensor_depths = np.array([0.0, 0.018, 0.035, 0.053, 0.070, 0.099, 0.105, 0.123, 0.140, 0.158])\n",
    "#         # sensor names as TC1 deepest -> TC10 surface (reverse for display in evaluate_model)\n",
    "#         sensor_names = [f\"TC{i}\" for i in range(1, 11)] \n",
    "#         # results = evaluate_model(model, test_loader, y_scaler, device)\n",
    "#         results = evaluate_model(model, test_loader, y_scaler, device, sensor_depths, sensor_names)\n",
    "\n",
    "\n",
    "#        # Use reversed sensor names in print because evaluate_model returns reversed metrics for display\n",
    "#         print(\"\\n\" + \"=\"*50)\n",
    "#         print(\"EVALUATION RESULTS (Temperature Â°C)\")\n",
    "#         print(\"=\"*50)\n",
    "#         print(f\"{'Sensor':<15} {'RMSE (Â°C)':<12} {'MAE (Â°C)':<12} {'MAPE (%)':<12} {'RÂ² Score':<12}\")\n",
    "#         print(\"-\"*65)\n",
    "#         for i, name in enumerate(results['sensor_names_rev']):\n",
    "#             print(f\"{name:<15} {results['rmse_temp'][i]:<12.3f} {results['mae_temp'][i]:<12.3f} \"\n",
    "#                   f\"{results['mape_temp'][i]:<12.2f} {results['r2_temp'][i]:<12.3f}\")\n",
    "#         print(\"-\"*65)\n",
    "#         print(f\"{'Overall':<15} {results['overall_rmse_temp']:<12.3f} {results['overall_mae_temp']:<12.3f} \"\n",
    "#               f\"{'':<12} {results['overall_r2_temp']:<12.3f}\")\n",
    "        \n",
    "#         print(\"\\n\" + \"=\"*50)\n",
    "#         print(\"EVALUATION RESULTS (Energy stored, Watts)\")\n",
    "#         print(\"=\"*58)\n",
    "#         print(f\"{'Sensor':<15} {'RMSE (W)':<15} {'MAE (W)':<15} {'MAPE (%)':<12} {'RÂ² Score':<12}\")\n",
    "#         print(\"-\"*58)\n",
    "#         for i, name in enumerate(results['sensor_names_rev']):\n",
    "#             print(f\"{name:<15} {results['rmse_energy'][i]:<15.3f} {results['mae_energy'][i]:<15.3f} \"\n",
    "#                   f\"{results['mape_energy'][i]:<12.2f} {results['r2_energy'][i]:<12.3f}\")\n",
    "#         print(\"-\"*58)\n",
    "#         print(f\"{'Overall':<15} {results['overall_rmse_energy']:<15.3f} {results['overall_mae_energy']:<15.3f} \"\n",
    "#               f\"{'':<12} {results['overall_r2_energy']:<12.3f}\")\n",
    "        \n",
    "#         # Physics conservation monitoring\n",
    "#         print(\"\\n\" + \"=\"*50)\n",
    "#         print(\"PHYSICS CONSERVATION MONITORING\")\n",
    "#         print(\"=\"*50)\n",
    "#         final_violation_rate = violation_rates[-1] if violation_rates else 0.0\n",
    "#         print(f\"Final energy conservation violation rate: {final_violation_rate:.4f}\")\n",
    "#         if final_violation_rate > 0.01:\n",
    "#             print(\"âš ï¸  WARNING: High violation rate detected!\")\n",
    "#             print(\"   Consider increasing conservation_violation_penalty or physics_weight\")\n",
    "#         else:\n",
    "#             print(\"âœ… Energy conservation constraint successfully learned!\")\n",
    "\n",
    "#         if 'overall_mean_energy' in results:\n",
    "#             relative_rmse = results['overall_rmse_energy'] / results['overall_mean_energy'] * 100\n",
    "#             print(f\"\\nRelative RMSE error in energy stored: {relative_rmse:.2f}%\")\n",
    "        \n",
    "#         plot_results(train_losses, val_losses, results, tc_cols, violation_rates)\n",
    "        \n",
    "#         return model, results, X_scaler, y_scaler, tc_cols, violation_rates\n",
    "                \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in main execution: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return None, None, None, None, None, None\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     model, results, X_scaler, y_scaler, tc_cols, violation_rates = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a3233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset for h=0.0375 (h2)\n",
      "Loading data from data/new_processed_fix_new with h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs20_surf0_699s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs92_surf0_630s - Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs20_surf1_675s - Sheet3_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs92_surf0_598s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs25_surf0_439s-Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs0_surf0_719s-Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs0_surf0_644s-Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs90_surf0_749s-Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux73_abs0_surf0_429s - Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs25_surf0_660s-Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs90_surf0_692s-Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs20_surf1_830s - Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs0_surf1_491s - Sheet3_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs20_surf0_651s - Sheet4_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs92_surf0_628s - Sheet4_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs20_surf0_389s - Sheet3_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs0_surf0_389s - Sheet4_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs0_surf0_825s-Sheer3_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs20_surf1_613s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs90_surf0_747s-Sheet3_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv matches h_filter=0.0375\n",
      "No data in cleaned_h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv matches h_filter=0.0375\n",
      "Loaded 6782 rows with columns: ['Time', 'TC1_tip', 'TC2', 'TC3', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10', 'Theoretical_Temps_11', 'h', 'flux', 'abs', 'surf', 'filename']\n",
      "Unique h values: [0.0375]\n",
      "Starting data preprocessing...\n",
      "Features: 14, Targets: 3\n",
      "Theory columns: 3, TC columns: 3\n",
      "Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3']\n",
      "After removing missing values: 6782 samples\n",
      "Training samples: 4747\n",
      "Validation samples: 1017\n",
      "Test samples: 1018\n",
      "Using device: cpu\n",
      "Model created with 240387 parameters for 3 sensors\n",
      "Starting training with 3 sensors for h=0.0375...\n",
      "Epoch [  10/1000] Train: 0.015560 Val: 0.009709 Violations: 0.6657 (ðŸš¨ CONCERNING) LR: 0.00100000\n",
      "Epoch [  20/1000] Train: 0.007576 Val: 0.003278 Violations: 0.6684 (ðŸš¨ CONCERNING) LR: 0.00100000\n",
      "Epoch [  30/1000] Train: 0.005485 Val: 0.002371 Violations: 0.6707 (ðŸš¨ CONCERNING) LR: 0.00100000\n",
      "Epoch [  40/1000] Train: 0.004713 Val: 0.001970 Violations: 0.6712 (ðŸš¨ CONCERNING) LR: 0.00100000\n",
      "Epoch [  50/1000] Train: 0.004921 Val: 0.001968 Violations: 0.6720 (ðŸš¨ CONCERNING) LR: 0.00100000\n",
      "Early stopping at epoch 50\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "Best validation loss: 0.057972\n",
      "Final violation rate: 0.5376\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS FOR h2 (Temperature Â°C)\n",
      "==================================================\n",
      "Sensor          RMSE (Â°C)    MAE (Â°C)     MAPE (%)     RÂ² Score    \n",
      "-----------------------------------------------------------------\n",
      "TC3             30.188       25.262       7.22         -1.832      \n",
      "TC2             6.354        5.139        1.37         0.167       \n",
      "TC1             7.064        6.202        1.68         0.031       \n",
      "-----------------------------------------------------------------\n",
      "Overall         18.272       12.201                    -0.545      \n",
      "\n",
      "==================================================\n",
      "PHYSICS CONSERVATION MONITORING FOR h2\n",
      "==================================================\n",
      "Final energy conservation violation rate: 0.6720\n",
      "\n",
      "Processing dataset for h=0.084 (h3)\n",
      "Loading data from data/new_processed_fix_new with h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs20_surf0_699s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs92_surf0_630s - Sheet2_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs20_surf1_675s - Sheet3_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h2_flux88_abs0_surf0_585s - Sheet2_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs92_surf0_598s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux73_abs0_surf0_429s - Sheet2_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h2_flux88_abs0_surf0_431s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs20_surf1_830s - Sheet2_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h2_flux88_abs25_surf0_493s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs0_surf1_491s - Sheet3_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h2_flux88_abs92_surf0_115s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs20_surf0_651s - Sheet4_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs92_surf0_628s - Sheet4_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs20_surf0_389s - Sheet3_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs0_surf0_389s - Sheet4_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs20_surf1_613s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv matches h_filter=0.084\n",
      "No data in cleaned_h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv matches h_filter=0.084\n",
      "Loaded 11131 rows with columns: ['Time', 'TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10', 'Theoretical_Temps_11', 'h', 'flux', 'abs', 'surf', 'filename']\n",
      "Unique h values: [0.084]\n",
      "Starting data preprocessing...\n",
      "Features: 16, Targets: 5\n",
      "Theory columns: 5, TC columns: 5\n",
      "Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5']\n",
      "After removing missing values: 11131 samples\n",
      "Training samples: 7791\n",
      "Validation samples: 1670\n",
      "Test samples: 1670\n",
      "Using device: cpu\n",
      "Model created with 241669 parameters for 5 sensors\n",
      "Starting training with 5 sensors for h=0.084...\n",
      "Epoch [  10/1000] Train: 0.005371 Val: 0.001907 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  20/1000] Train: 0.003625 Val: 0.002063 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  30/1000] Train: 0.003467 Val: 0.001558 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  40/1000] Train: 0.003342 Val: 0.001320 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  50/1000] Train: 0.003006 Val: 0.001270 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [ 100/1000] Train: 0.002265 Val: 0.000979 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00050000\n",
      "Epoch [ 200/1000] Train: 0.002068 Val: 0.000907 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00050000\n",
      "Epoch [ 300/1000] Train: 0.001923 Val: 0.000881 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00012500\n",
      "Epoch [ 400/1000] Train: 0.001942 Val: 0.000730 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00001563\n",
      "Early stopping at epoch 426\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "Best validation loss: 0.000697\n",
      "Final violation rate: 0.0000\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS FOR h3 (Temperature Â°C)\n",
      "==================================================\n",
      "Sensor          RMSE (Â°C)    MAE (Â°C)     MAPE (%)     RÂ² Score    \n",
      "-----------------------------------------------------------------\n",
      "TC5             0.552        0.426        0.12         0.998       \n",
      "TC4             0.357        0.260        0.07         0.997       \n",
      "TC3             0.227        0.176        0.05         0.997       \n",
      "TC2             0.252        0.215        0.06         0.998       \n",
      "TC1             0.231        0.178        0.05         0.999       \n",
      "-----------------------------------------------------------------\n",
      "Overall         0.347        0.251                     0.998       \n",
      "\n",
      "==================================================\n",
      "PHYSICS CONSERVATION MONITORING FOR h3\n",
      "==================================================\n",
      "Final energy conservation violation rate: 0.0000\n",
      "\n",
      "Processing dataset for h=0.1575 (h6)\n",
      "Loading data from data/new_processed_fix_new with h_filter=0.1575\n",
      "No data in cleaned_h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h2_flux88_abs0_surf0_585s - Sheet2_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs25_surf0_439s-Sheet2_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs0_surf0_719s-Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs0_surf0_644s-Sheet2_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs90_surf0_749s-Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs25_surf0_660s-Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs90_surf0_692s-Sheet2_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h2_flux88_abs0_surf0_431s - Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h2_flux88_abs25_surf0_493s - Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h2_flux88_abs92_surf0_115s - Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs0_surf0_825s-Sheer3_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs90_surf0_747s-Sheet3_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv matches h_filter=0.1575\n",
      "No data in cleaned_h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv matches h_filter=0.1575\n",
      "Loaded 48233 rows with columns: ['Time', 'TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10', 'Theoretical_Temps_11', 'h', 'flux', 'abs', 'surf', 'filename', 'TC_9_5']\n",
      "Unique h values: [0.1575]\n",
      "Starting data preprocessing...\n",
      "Features: 21, Targets: 10\n",
      "Theory columns: 10, TC columns: 10\n",
      "Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10']\n",
      "After removing missing values: 48233 samples\n",
      "Training samples: 33763\n",
      "Validation samples: 7235\n",
      "Test samples: 7235\n",
      "Using device: cpu\n",
      "Model created with 244874 parameters for 10 sensors\n",
      "Starting training with 10 sensors for h=0.1575...\n",
      "Epoch [  10/1000] Train: 50.009731 Val: 49.569119 Violations: 0.2216 (ðŸš¨ CONCERNING) LR: 0.00100000\n",
      "Epoch [  20/1000] Train: 50.008546 Val: 49.568889 Violations: 0.2216 (ðŸš¨ CONCERNING) LR: 0.00100000\n",
      "Epoch [  30/1000] Train: 50.007955 Val: 49.568593 Violations: 0.2216 (ðŸš¨ CONCERNING) LR: 0.00050000\n",
      "Epoch [  40/1000] Train: 50.007796 Val: 49.568675 Violations: 0.2216 (ðŸš¨ CONCERNING) LR: 0.00050000\n",
      "Epoch [  50/1000] Train: 50.007583 Val: 49.568301 Violations: 0.2216 (ðŸš¨ CONCERNING) LR: 0.00025000\n",
      "Epoch [ 100/1000] Train: 50.007452 Val: 49.568276 Violations: 0.2216 (ðŸš¨ CONCERNING) LR: 0.00006250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 408\u001b[39m\n\u001b[32m    405\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     model, results, X_scaler, y_scaler, tc_cols, violation_rates = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 364\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    355\u001b[39m model = EnhancedThermalNet(\n\u001b[32m    356\u001b[39m     input_size=train_loader.dataset.tensors[\u001b[32m0\u001b[39m].shape[\u001b[32m1\u001b[39m],\n\u001b[32m    357\u001b[39m     output_size=num_sensors,\n\u001b[32m    358\u001b[39m     hidden_dims=[\u001b[32m512\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m128\u001b[39m],\n\u001b[32m    359\u001b[39m     dropout_rate=\u001b[32m0.2\u001b[39m\n\u001b[32m    360\u001b[39m ).to(device)\n\u001b[32m    362\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel created with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel.parameters())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m parameters for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_sensors\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sensors\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m train_losses, val_losses, violation_rates = \u001b[43mtrain_model_with_conservation_monitoring\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_sensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\n\u001b[32m    366\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m sensor_depths = np.array([\u001b[32m0.0\u001b[39m, \u001b[32m0.018\u001b[39m, \u001b[32m0.035\u001b[39m, \u001b[32m0.053\u001b[39m, \u001b[32m0.070\u001b[39m, \u001b[32m0.099\u001b[39m, \u001b[32m0.105\u001b[39m, \u001b[32m0.123\u001b[39m, \u001b[32m0.140\u001b[39m, \u001b[32m0.158\u001b[39m])[:num_sensors]\n\u001b[32m    369\u001b[39m sensor_names = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTC\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_sensors)]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 273\u001b[39m, in \u001b[36mtrain_model_with_conservation_monitoring\u001b[39m\u001b[34m(model, train_loader, val_loader, device, num_sensors, h_value, epochs, patience)\u001b[39m\n\u001b[32m    271\u001b[39m optimizer.zero_grad()\n\u001b[32m    272\u001b[39m predictions = model(X_batch)\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_sensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.isnan(loss) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(loss):\n\u001b[32m    275\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: Invalid loss detected at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 235\u001b[39m, in \u001b[36mPhysicsInformedLoss.forward\u001b[39m\u001b[34m(self, predictions, targets, inputs, num_sensors)\u001b[39m\n\u001b[32m    232\u001b[39m     gradient_loss = torch.tensor(\u001b[32m0.0\u001b[39m, device=predictions.device)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m num_sensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     physics_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_physics_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_sensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    237\u001b[39m     physics_loss = torch.tensor(\u001b[32m50.0\u001b[39m, device=predictions.device, requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 185\u001b[39m, in \u001b[36mPhysicsInformedLoss.compute_physics_loss\u001b[39m\u001b[34m(self, predictions, targets, inputs, num_sensors)\u001b[39m\n\u001b[32m    183\u001b[39m     relative_violation = excess / (incoming_energy[violation_mask] + \u001b[32m1e-6\u001b[39m)\n\u001b[32m    184\u001b[39m     conservation_loss = conservation_loss + \u001b[38;5;28mself\u001b[39m.conservation_violation_penalty * torch.mean(relative_violation ** \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28mself\u001b[39m.violation_count += \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviolation_mask\u001b[49m\u001b[43m)\u001b[49m.item()\n\u001b[32m    187\u001b[39m margin = \u001b[32m0.05\u001b[39m * incoming_energy\n\u001b[32m    188\u001b[39m soft_violation_mask = (total_energy_stored > incoming_energy - margin) & (~violation_mask)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# # Block 5: Data Preprocessing (Modified)\n",
    "# def preprocess_data(data, test_size=0.3, val_size=0.5, use_enhanced_features=True, h_value=None):\n",
    "#     \"\"\"Enhanced data preprocessing with support for variable TC sensors\"\"\"\n",
    "#     print(\"Starting data preprocessing...\")\n",
    "    \n",
    "#     # Time normalization\n",
    "#     time_min = data[\"Time\"].min()\n",
    "#     time_max = data[\"Time\"].max()\n",
    "#     data[\"Time_norm\"] = (data[\"Time\"] - time_min) / (time_max - time_min)\n",
    "    \n",
    "#     # Enhanced time features\n",
    "#     if use_enhanced_features:\n",
    "#         data[\"TimeÂ²\"] = data[\"Time_norm\"] ** 2\n",
    "#         data[\"TimeÂ³\"] = data[\"Time_norm\"] ** 3\n",
    "#         data[\"Time_sin\"] = np.sin(2 * np.pi * data[\"Time_norm\"])\n",
    "#         data[\"Time_cos\"] = np.cos(2 * np.pi * data[\"Time_norm\"])\n",
    "#         data[\"flux_abs_interaction\"] = data[\"flux\"] * data[\"abs\"]\n",
    "#         data[\"h_flux_interaction\"] = data[\"h\"] * data[\"flux\"]\n",
    "#         base_features = [\"Time_norm\", \"TimeÂ²\", \"TimeÂ³\", \"Time_sin\", \"Time_cos\", \n",
    "#                          \"h\", \"flux\", \"abs\", \"surf\", \"flux_abs_interaction\", \"h_flux_interaction\"]\n",
    "#     else:\n",
    "#         data[\"TimeÂ²\"] = data[\"Time_norm\"] ** 2\n",
    "#         base_features = [\"Time_norm\", \"TimeÂ²\", \"h\", \"flux\", \"abs\", \"surf\"]\n",
    "    \n",
    "#     # Identify TC and theoretical temperature columns\n",
    "#     theory_cols = [c for c in data.columns if c.startswith(\"Theoretical_Temps_\")]\n",
    "#     tc_cols = [col for col in data.columns if (col.startswith(\"TC\") and not col.endswith(\"_rate\")) or col.startswith(\"TC_\")]\n",
    "    \n",
    "#     # Determine number of sensors based on h_value\n",
    "#     h_category = get_h_category(h_value) if h_value is not None else 'h6'\n",
    "#     if h_category == 'h2':\n",
    "#         num_sensors = 3\n",
    "#     elif h_category == 'h3':\n",
    "#         num_sensors = 5\n",
    "#     else:  # h6 or default\n",
    "#         num_sensors = 10\n",
    "    \n",
    "#     # Filter valid TC and theoretical columns\n",
    "#     tc_cols = tc_cols[:num_sensors]\n",
    "#     theory_cols = theory_cols[:num_sensors] if len(theory_cols) >= num_sensors else theory_cols\n",
    "    \n",
    "#     # Handle missing TC columns\n",
    "#     if not tc_cols:\n",
    "#         print(\"Warning: No TC columns found. Creating dummy TC columns.\")\n",
    "#         tc_cols = [f\"TC_{i+1}\" for i in range(num_sensors)]\n",
    "#         for col in tc_cols:\n",
    "#             if col not in data.columns:\n",
    "#                 data[col] = 20 + np.random.normal(0, 5, len(data))\n",
    "    \n",
    "#     # Prepare features and targets\n",
    "#     feature_cols = base_features + theory_cols\n",
    "#     feature_cols = [col for col in feature_cols if col in data.columns]\n",
    "    \n",
    "#     X = data[feature_cols].copy()\n",
    "#     y = data[tc_cols].copy()\n",
    "#     filenames = data[\"filename\"] if \"filename\" in data.columns else pd.Series([\"unknown\"] * len(data))\n",
    "    \n",
    "#     print(f\"Features: {len(feature_cols)}, Targets: {len(tc_cols)}\")\n",
    "#     print(f\"Theory columns: {len(theory_cols)}, TC columns: {len(tc_cols)}\")\n",
    "#     print(f\"Feature columns: {feature_cols}\")\n",
    "#     print(f\"Target columns: {tc_cols}\")\n",
    "    \n",
    "#     # Remove missing values\n",
    "#     mask = ~(X.isnull().any(axis=1) | y.isnull().any(axis=1))\n",
    "#     X = X[mask].reset_index(drop=True)\n",
    "#     y = y[mask].reset_index(drop=True)\n",
    "#     filenames = filenames[mask].reset_index(drop=True)\n",
    "    \n",
    "#     print(f\"After removing missing values: {len(X)} samples\")\n",
    "    \n",
    "#     if len(X) < 10:\n",
    "#         raise ValueError(\"Not enough valid samples for training!\")\n",
    "    \n",
    "#     # Train-test split\n",
    "#     X_train, X_temp, y_train, y_temp, _, _ = train_test_split(\n",
    "#         X, y, filenames, test_size=test_size, random_state=SEED\n",
    "#     )\n",
    "#     X_val, X_test, y_val, y_test, _, _ = train_test_split(\n",
    "#         X_temp, y_temp, _, test_size=val_size, random_state=SEED\n",
    "#     )\n",
    "    \n",
    "#     # Scaling\n",
    "#     X_scaler = StandardScaler()\n",
    "#     y_scaler = MinMaxScaler()\n",
    "    \n",
    "#     X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "#     X_val_scaled = X_scaler.transform(X_val)\n",
    "#     X_test_scaled = X_scaler.transform(X_test)\n",
    "    \n",
    "#     y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "#     y_val_scaled = y_scaler.transform(y_val)\n",
    "#     y_test_scaled = y_scaler.transform(y_test)\n",
    "    \n",
    "#     # Save scalers and metadata\n",
    "#     joblib.dump(X_scaler, f\"X_scaler_{h_category}.pkl\")\n",
    "#     joblib.dump(y_scaler, f\"y_scaler_{h_category}.pkl\")\n",
    "#     joblib.dump({\"time_min\": time_min, \"time_max\": time_max}, f\"time_range_{h_category}.pkl\")\n",
    "#     joblib.dump({\"feature_cols\": feature_cols, \"tc_cols\": tc_cols, \"num_sensors\": num_sensors}, f\"column_info_{h_category}.pkl\")\n",
    "    \n",
    "#     # Create DataLoaders\n",
    "#     train_dataset = TensorDataset(\n",
    "#         torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "#         torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "#     )\n",
    "#     val_dataset = TensorDataset(\n",
    "#         torch.tensor(X_val_scaled, dtype=torch.float32),\n",
    "#         torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "#     )\n",
    "#     test_dataset = TensorDataset(\n",
    "#         torch.tensor(X_test_scaled, dtype=torch.float32),\n",
    "#         torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "#     )\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=min(64, len(X_train)//4), shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=min(64, len(X_val)//4), shuffle=False)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=min(64, len(X_test)//4), shuffle=False)\n",
    "    \n",
    "#     print(f\"Training samples: {len(X_train)}\")\n",
    "#     print(f\"Validation samples: {len(X_val)}\")\n",
    "#     print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "#     return train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols, num_sensors\n",
    "\n",
    "# # Block 6: Physics-Informed Loss (Modified)\n",
    "# class PhysicsInformedLoss(nn.Module):\n",
    "#     \"\"\"Physics-informed loss function with support for variable TC sensors\"\"\"\n",
    "#     def __init__(self, smoothness_weight=0.005, gradient_weight=0.0001, physics_weight=0.5, \n",
    "#                  conservation_violation_penalty=100.0):\n",
    "#         super().__init__()\n",
    "#         self.mse_loss = nn.MSELoss()\n",
    "#         self.smoothness_weight = smoothness_weight\n",
    "#         self.gradient_weight = gradient_weight\n",
    "#         self.physics_weight = physics_weight\n",
    "#         self.conservation_violation_penalty = conservation_violation_penalty\n",
    "        \n",
    "#         # Physical constants\n",
    "#         self.r = 2.0375 * 0.0254\n",
    "#         self.A_rec = np.pi * (self.r ** 2)\n",
    "#         self.rho = 1836.31\n",
    "#         self.cp = 1512\n",
    "        \n",
    "#         self.A_rec_tensor = None\n",
    "#         self.rho_tensor = None\n",
    "#         self.cp_tensor = None\n",
    "        \n",
    "#         self.violation_count = 0\n",
    "#         self.total_batches = 0\n",
    "    \n",
    "#     def _init_tensors(self, device):\n",
    "#         if self.A_rec_tensor is None:\n",
    "#             self.A_rec_tensor = torch.tensor(self.A_rec, device=device, dtype=torch.float32)\n",
    "#             self.rho_tensor = torch.tensor(self.rho, device=device, dtype=torch.float32)\n",
    "#             self.cp_tensor = torch.tensor(self.cp, device=device, dtype=torch.float32)\n",
    "    \n",
    "#     def compute_physics_loss(self, predictions, targets, inputs, num_sensors):\n",
    "#         device = predictions.device\n",
    "#         self._init_tensors(device)\n",
    "        \n",
    "#         batch_size = predictions.shape[0]\n",
    "#         self.total_batches += 1\n",
    "        \n",
    "#         try:\n",
    "#             flux = inputs[:, 6]\n",
    "#             h = inputs[:, 5]\n",
    "#             incoming_energy = flux * self.A_rec_tensor\n",
    "#             mass = self.rho_tensor * h * self.A_rec_tensor\n",
    "            \n",
    "#             theoretical_start_idx = 11\n",
    "#             if inputs.shape[1] >= theoretical_start_idx + num_sensors:\n",
    "#                 theoretical_temps = inputs[:, theoretical_start_idx:theoretical_start_idx + num_sensors]\n",
    "#                 temp_change = torch.mean(predictions - theoretical_temps, dim=1)\n",
    "#             else:\n",
    "#                 temp_change = torch.mean(predictions - targets, dim=1)\n",
    "            \n",
    "#             dt = torch.ones_like(temp_change)\n",
    "#             total_energy_stored = mass * self.cp_tensor * temp_change / dt\n",
    "            \n",
    "#             conservation_loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "            \n",
    "#             violation_mask = total_energy_stored > incoming_energy\n",
    "#             if torch.any(violation_mask):\n",
    "#                 excess = total_energy_stored[violation_mask] - incoming_energy[violation_mask]\n",
    "#                 relative_violation = excess / (incoming_energy[violation_mask] + 1e-6)\n",
    "#                 conservation_loss = conservation_loss + self.conservation_violation_penalty * torch.mean(relative_violation ** 2)\n",
    "#                 self.violation_count += torch.sum(violation_mask).item()\n",
    "            \n",
    "#             margin = 0.05 * incoming_energy\n",
    "#             soft_violation_mask = (total_energy_stored > incoming_energy - margin) & (~violation_mask)\n",
    "#             if torch.any(soft_violation_mask):\n",
    "#                 soft_excess = total_energy_stored[soft_violation_mask] - incoming_energy[soft_violation_mask]\n",
    "#                 soft_relative_violation = soft_excess / (incoming_energy[soft_violation_mask] + 1e-6)\n",
    "#                 soft_penalty = torch.mean(soft_relative_violation ** 2)\n",
    "#                 conservation_loss = conservation_loss + 0.1 * self.conservation_violation_penalty * soft_penalty\n",
    "            \n",
    "#             acceptable_mask = total_energy_stored <= incoming_energy\n",
    "#             if torch.any(acceptable_mask):\n",
    "#                 energy_difference = incoming_energy[acceptable_mask] - total_energy_stored[acceptable_mask]\n",
    "#                 efficiency_penalty = torch.mean(energy_difference) * 0.01\n",
    "#                 conservation_loss = conservation_loss + efficiency_penalty\n",
    "            \n",
    "#             balance_tolerance = 0.01 * torch.abs(incoming_energy)\n",
    "#             energy_difference = torch.abs(incoming_energy - total_energy_stored)\n",
    "#             balance_mask = energy_difference <= balance_tolerance\n",
    "#             if torch.any(balance_mask):\n",
    "#                 balance_reward = torch.mean(energy_difference[balance_mask])\n",
    "#                 conservation_loss = conservation_loss - 0.05 * balance_reward\n",
    "            \n",
    "#             return conservation_loss\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Warning: Physics loss computation failed: {e}\")\n",
    "#             return torch.tensor(100.0, device=device, requires_grad=True)\n",
    "    \n",
    "#     def get_violation_rate(self):\n",
    "#         if self.total_batches == 0:\n",
    "#             return 0.0\n",
    "#         total_samples = self.total_batches * 64\n",
    "#         return self.violation_count / total_samples if total_samples > 0 else 0.0\n",
    "    \n",
    "#     def reset_violation_tracking(self):\n",
    "#         self.violation_count = 0\n",
    "#         self.total_batches = 0\n",
    "    \n",
    "#     def forward(self, predictions, targets, inputs=None, num_sensors=None):\n",
    "#         mse_loss = self.mse_loss(predictions, targets)\n",
    "        \n",
    "#         if predictions.shape[1] > 1:\n",
    "#             smoothness_loss = torch.mean(torch.abs(predictions[:, 1:] - predictions[:, :-1]))\n",
    "#             gradient_loss = torch.mean(torch.abs(torch.diff(predictions, dim=1)))\n",
    "#         else:\n",
    "#             smoothness_loss = torch.tensor(0.0, device=predictions.device)\n",
    "#             gradient_loss = torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "#         if inputs is not None and num_sensors is not None:\n",
    "#             physics_loss = self.compute_physics_loss(predictions, targets, inputs, num_sensors)\n",
    "#         else:\n",
    "#             physics_loss = torch.tensor(50.0, device=predictions.device, requires_grad=True)\n",
    "        \n",
    "#         total_loss = (mse_loss + \n",
    "#                      self.smoothness_weight * smoothness_loss + \n",
    "#                      self.gradient_weight * gradient_loss +\n",
    "#                      self.physics_weight * physics_loss)\n",
    "        \n",
    "#         return total_loss\n",
    "\n",
    "# # Block 7: Training Function (Modified)\n",
    "# def train_model_with_conservation_monitoring(model, train_loader, val_loader, device, num_sensors, h_value, epochs=1000, patience=50):\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, min_lr=1e-6)\n",
    "    \n",
    "#     criterion = PhysicsInformedLoss(\n",
    "#         smoothness_weight=0.005,\n",
    "#         gradient_weight=0.0001,\n",
    "#         physics_weight=0.5,\n",
    "#         conservation_violation_penalty=100.0\n",
    "#     )\n",
    "    \n",
    "#     best_val_loss = float('inf')\n",
    "#     patience_counter = 0\n",
    "#     train_losses = []\n",
    "#     val_losses = []\n",
    "#     violation_rates = []\n",
    "    \n",
    "#     print(f\"Starting training with {num_sensors} sensors for h={h_value}...\")\n",
    "#     for epoch in range(epochs):\n",
    "#         criterion.reset_violation_tracking()\n",
    "#         model.train()\n",
    "#         train_loss_epoch = 0\n",
    "#         for X_batch, y_batch in train_loader:\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             predictions = model(X_batch)\n",
    "#             loss = criterion(predictions, y_batch, X_batch, num_sensors)\n",
    "#             if torch.isnan(loss) or torch.isinf(loss):\n",
    "#                 print(f\"Warning: Invalid loss detected at epoch {epoch}\")\n",
    "#                 continue\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             optimizer.step()\n",
    "#             train_loss_epoch += loss.item()\n",
    "#         train_loss_epoch /= len(train_loader)\n",
    "        \n",
    "#         model.eval()\n",
    "#         val_loss_epoch = 0\n",
    "#         with torch.no_grad():\n",
    "#             for X_batch, y_batch in val_loader:\n",
    "#                 X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#                 predictions = model(X_batch)\n",
    "#                 loss = criterion(predictions, y_batch, X_batch, num_sensors)\n",
    "#                 val_loss_epoch += loss.item()\n",
    "#         val_loss_epoch /= len(val_loader)\n",
    "        \n",
    "#         violation_rate = criterion.get_violation_rate()\n",
    "#         violation_rates.append(violation_rate)\n",
    "#         scheduler.step(val_loss_epoch)\n",
    "#         train_losses.append(train_loss_epoch)\n",
    "#         val_losses.append(val_loss_epoch)\n",
    "        \n",
    "#         adjusted_val_loss = val_loss_epoch + (violation_rate * 100)\n",
    "#         if adjusted_val_loss < best_val_loss:\n",
    "#             best_val_loss = adjusted_val_loss\n",
    "#             patience_counter = 0\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'val_loss': val_loss_epoch,\n",
    "#                 'train_loss': train_loss_epoch,\n",
    "#                 'violation_rate': violation_rate,\n",
    "#             }, f'best_thermal_model_{get_h_category(h_value)}.pth')\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "#             if patience_counter >= patience:\n",
    "#                 print(f\"Early stopping at epoch {epoch}\")\n",
    "#                 break\n",
    "        \n",
    "#         if (epoch + 1) % 100 == 0 or (epoch < 50 and (epoch + 1) % 10 == 0):\n",
    "#             current_lr = optimizer.param_groups[0]['lr']\n",
    "#             status = \"âœ… EXCELLENT\" if violation_rate < 0.01 else \"âš ï¸ ACCEPTABLE\" if violation_rate < 0.05 else \"ðŸš¨ CONCERNING\"\n",
    "#             print(f\"Epoch [{epoch+1:4d}/{epochs}] \"\n",
    "#                   f\"Train: {train_loss_epoch:.6f} \"\n",
    "#                   f\"Val: {val_loss_epoch:.6f} \"\n",
    "#                   f\"Violations: {violation_rate:.4f} ({status}) \"\n",
    "#                   f\"LR: {current_lr:.8f}\")\n",
    "    \n",
    "#     checkpoint = torch.load(f'best_thermal_model_{get_h_category(h_value)}.pth')\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(\"TRAINING COMPLETED\")\n",
    "#     print(f\"Best validation loss: {checkpoint['val_loss']:.6f}\")\n",
    "#     print(f\"Final violation rate: {checkpoint['violation_rate']:.4f}\")\n",
    "    \n",
    "#     return train_losses, val_losses, violation_rates\n",
    "\n",
    "# # Block 8: Main Execution (Modified)\n",
    "# def main():\n",
    "#     try:\n",
    "#         DATA_DIR = get_data_directory()\n",
    "#         h_values = [0.0375, 0.084, 0.1575]  # h2, h3, h6\n",
    "#         all_results = {}\n",
    "        \n",
    "#         for h_val in h_values:\n",
    "#             h_category = get_h_category(h_val)\n",
    "#             print(f\"\\nProcessing dataset for h={h_val} ({h_category})\")\n",
    "#             data = load_data(DATA_DIR, h_filter=h_val)\n",
    "            \n",
    "#             train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols, num_sensors = preprocess_data(\n",
    "#                 data, use_enhanced_features=True, h_value=h_val\n",
    "#             )\n",
    "            \n",
    "#             device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#             print(f\"Using device: {device}\")\n",
    "            \n",
    "#             model = EnhancedThermalNet(\n",
    "#                 input_size=train_loader.dataset.tensors[0].shape[1],\n",
    "#                 output_size=num_sensors,\n",
    "#                 hidden_dims=[512, 256, 256, 128],\n",
    "#                 dropout_rate=0.2\n",
    "#             ).to(device)\n",
    "            \n",
    "#             print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters for {num_sensors} sensors\")\n",
    "            \n",
    "#             train_losses, val_losses, violation_rates = train_model_with_conservation_monitoring(\n",
    "#                 model, train_loader, val_loader, device, num_sensors, h_val, epochs=1000, patience=50\n",
    "#             )\n",
    "            \n",
    "#             sensor_depths = np.array([0.0, 0.018, 0.035, 0.053, 0.070, 0.099, 0.105, 0.123, 0.140, 0.158])[:num_sensors]\n",
    "#             sensor_names = [f\"TC{i+1}\" for i in range(num_sensors)]\n",
    "#             results = evaluate_model(model, test_loader, y_scaler, device, sensor_depths, sensor_names)\n",
    "            \n",
    "#             all_results[h_val] = {\n",
    "#                 'model': model,\n",
    "#                 'results': results,\n",
    "#                 'X_scaler': X_scaler,\n",
    "#                 'y_scaler': y_scaler,\n",
    "#                 'tc_cols': tc_cols,\n",
    "#                 'violation_rates': violation_rates\n",
    "#             }\n",
    "            \n",
    "#             print(\"\\n\" + \"=\"*50)\n",
    "#             print(f\"EVALUATION RESULTS FOR {h_category} (Temperature Â°C)\")\n",
    "#             print(\"=\"*50)\n",
    "#             print(f\"{'Sensor':<15} {'RMSE (Â°C)':<12} {'MAE (Â°C)':<12} {'MAPE (%)':<12} {'RÂ² Score':<12}\")\n",
    "#             print(\"-\"*65)\n",
    "#             for i, name in enumerate(results['sensor_names_rev']):\n",
    "#                 print(f\"{name:<15} {results['rmse_temp'][i]:<12.3f} {results['mae_temp'][i]:<12.3f} \"\n",
    "#                       f\"{results['mape_temp'][i]:<12.2f} {results['r2_temp'][i]:<12.3f}\")\n",
    "#             print(\"-\"*65)\n",
    "#             print(f\"{'Overall':<15} {results['overall_rmse_temp']:<12.3f} {results['overall_mae_temp']:<12.3f} \"\n",
    "#                   f\"{'':<12} {results['overall_r2_temp']:<12.3f}\")\n",
    "            \n",
    "#             print(\"\\n\" + \"=\"*50)\n",
    "#             print(f\"PHYSICS CONSERVATION MONITORING FOR {h_category}\")\n",
    "#             print(\"=\"*50)\n",
    "#             final_violation_rate = violation_rates[-1] if violation_rates else 0.0\n",
    "#             print(f\"Final energy conservation violation rate: {final_violation_rate:.4f}\")\n",
    "        \n",
    "#         return all_results\n",
    "                \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in main execution: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     model, results, X_scaler, y_scaler, tc_cols, violation_rates = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55e86761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data directory: data/new_processed_fix_new\n",
      "\n",
      "Processing dataset for h=0.0375 (h2)\n",
      "Loading data from: data/new_processed_fix_new\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_699s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv (622 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv (576 rows)\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_surf0_630s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf1_675s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv (576 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs0_surf0_585s - Sheet2_processed.csv (564 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_surf0_598s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs25_surf0_439s-Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs0_surf0_719s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs0_surf0_644s-Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_749s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf0_429s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv (501 rows)\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs25_surf0_660s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_692s-Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs0_surf0_431s - Sheet1_processed.csv (560 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf1_830s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs25_surf0_493s - Sheet1_processed.csv (572 rows)\n",
      "Loaded: cleaned_h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv (548 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv (703 rows)\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf1_491s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs92_surf0_115s - Sheet1_processed.csv (500 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv (540 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_651s - Sheet4_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv (508 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_surf0_628s - Sheet4_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_389s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_389s - Sheet4_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs0_surf0_825s-Sheer3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf1_613s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_747s-Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv\n",
      "\n",
      "==================================================\n",
      "FILE PROCESSING SUMMARY\n",
      "==================================================\n",
      "Total files scanned     : 77\n",
      "Files successfully loaded: 12\n",
      "Files skipped (filtered): 65\n",
      "Files skipped (unmatched): 0\n",
      "==================================================\n",
      "\n",
      "Total data loaded: 6770 rows, 19 columns\n",
      "Starting data preprocessing...\n",
      "Features: 14, Targets: 3\n",
      "Theory columns: 3, TC columns: 3\n",
      "Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3']\n",
      "After removing missing values: 6770 samples\n",
      "Training samples: 4739\n",
      "Validation samples: 1015\n",
      "Test samples: 1016\n",
      "Using device: cpu\n",
      "Model created with 404828 parameters for 3 sensors\n",
      "Starting training with 3 sensors for h=0.0375...\n",
      "Epoch [  10/1000] Train: 0.016810 Val: 0.011621 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  20/1000] Train: 0.013099 Val: 0.010406 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  30/1000] Train: 0.010565 Val: 0.008185 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  40/1000] Train: 0.006322 Val: 0.004419 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  50/1000] Train: 0.004858 Val: 0.003582 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [ 100/1000] Train: 0.002771 Val: 0.001866 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [ 200/1000] Train: 0.001697 Val: 0.001163 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00050000\n",
      "Epoch [ 300/1000] Train: 0.001369 Val: 0.001028 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00006250\n",
      "Epoch [ 400/1000] Train: 0.001352 Val: 0.001004 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00000781\n",
      "Early stopping at epoch 476\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "Best validation loss: 0.001000\n",
      "Final violation rate: 0.0000\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS FOR h2 (Temperature Â°C)\n",
      "==================================================\n",
      "Sensor          RMSE (Â°C)    MAE (Â°C)     MAPE (%)     RÂ² Score    \n",
      "-----------------------------------------------------------------\n",
      "TC3             0.824        0.531        0.16         0.998       \n",
      "TC2             0.361        0.268        0.07         0.997       \n",
      "TC1             0.254        0.175        0.05         0.999       \n",
      "-----------------------------------------------------------------\n",
      "Overall         0.540        0.325                     0.998       \n",
      "\n",
      "==================================================\n",
      "PHYSICS CONSERVATION MONITORING FOR h2\n",
      "==================================================\n",
      "Final energy conservation violation rate: 0.0000\n",
      "\n",
      "Processing dataset for h=0.084 (h3)\n",
      "Loading data from: data/new_processed_fix_new\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs20_surf0_699s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv (510 rows)\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs92_surf0_630s - Sheet2_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs20_surf1_675s - Sheet3_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h2_flux88_abs0_surf0_585s - Sheet2_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs92_surf0_598s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h3_flux88_abs25_surf0_439s-Sheet2_processed.csv (586 rows)\n",
      "Skipping (not h=0.084): cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv\n",
      "Loaded: cleaned_h3_flux88_abs0_surf0_719s-Sheet1_processed.csv (530 rows)\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h3_flux88_abs0_surf0_644s-Sheet2_processed.csv (553 rows)\n",
      "Skipping (not h=0.084): cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv (541 rows)\n",
      "Loaded: cleaned_h3_flux88_abs90_surf0_749s-Sheet1_processed.csv (540 rows)\n",
      "Skipping (not h=0.084): cleaned_h6_flux73_abs0_surf0_429s - Sheet2_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h3_flux88_abs25_surf0_660s-Sheet1_processed.csv (551 rows)\n",
      "Loaded: cleaned_h3_flux88_abs90_surf0_692s-Sheet2_processed.csv (622 rows)\n",
      "Loaded: cleaned_h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv (567 rows)\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h2_flux88_abs0_surf0_431s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs20_surf1_830s - Sheet2_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h2_flux88_abs25_surf0_493s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv (511 rows)\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs0_surf1_491s - Sheet3_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h2_flux88_abs92_surf0_115s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv (574 rows)\n",
      "Skipping (not h=0.084): cleaned_h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs20_surf0_651s - Sheet4_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv\n",
      "Loaded: cleaned_h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv (816 rows)\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs92_surf0_628s - Sheet4_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs20_surf0_389s - Sheet3_processed.csv\n",
      "Loaded: cleaned_h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv (612 rows)\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs0_surf0_389s - Sheet4_processed.csv\n",
      "Loaded: cleaned_h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv (555 rows)\n",
      "Skipping (not h=0.084): cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h3_flux88_abs0_surf0_825s-Sheer3_processed.csv (852 rows)\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv (555 rows)\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs20_surf1_613s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h3_flux88_abs90_surf0_747s-Sheet3_processed.csv (510 rows)\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv (505 rows)\n",
      "Loaded: cleaned_h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv (622 rows)\n",
      "Skipping (not h=0.084): cleaned_h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv\n",
      "\n",
      "==================================================\n",
      "FILE PROCESSING SUMMARY\n",
      "==================================================\n",
      "Total files scanned     : 77\n",
      "Files successfully loaded: 19\n",
      "Files skipped (filtered): 58\n",
      "Files skipped (unmatched): 0\n",
      "==================================================\n",
      "\n",
      "Total data loaded: 11112 rows, 21 columns\n",
      "Starting data preprocessing...\n",
      "Features: 16, Targets: 5\n",
      "Theory columns: 5, TC columns: 5\n",
      "Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5']\n",
      "After removing missing values: 11112 samples\n",
      "Training samples: 7778\n",
      "Validation samples: 1667\n",
      "Test samples: 1667\n",
      "Using device: cpu\n",
      "Model created with 406173 parameters for 5 sensors\n",
      "Starting training with 5 sensors for h=0.084...\n",
      "Epoch [  10/1000] Train: 0.005508 Val: 0.002589 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  20/1000] Train: 0.003704 Val: 0.002067 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  30/1000] Train: 0.002962 Val: 0.002291 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  40/1000] Train: 0.002533 Val: 0.001542 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  50/1000] Train: 0.002285 Val: 0.001609 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [ 100/1000] Train: 0.001662 Val: 0.001057 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [ 200/1000] Train: 0.001173 Val: 0.000838 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00025000\n",
      "Epoch [ 300/1000] Train: 0.001121 Val: 0.000805 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00003125\n",
      "Epoch [ 400/1000] Train: 0.001097 Val: 0.000799 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00000391\n",
      "Early stopping at epoch 434\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "Best validation loss: 0.000797\n",
      "Final violation rate: 0.0000\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS FOR h3 (Temperature Â°C)\n",
      "==================================================\n",
      "Sensor          RMSE (Â°C)    MAE (Â°C)     MAPE (%)     RÂ² Score    \n",
      "-----------------------------------------------------------------\n",
      "TC5             0.531        0.347        0.09         0.998       \n",
      "TC4             0.617        0.326        0.09         0.989       \n",
      "TC3             0.357        0.204        0.06         0.993       \n",
      "TC2             0.328        0.213        0.06         0.996       \n",
      "TC1             0.225        0.166        0.05         0.999       \n",
      "-----------------------------------------------------------------\n",
      "Overall         0.436        0.251                     0.995       \n",
      "\n",
      "==================================================\n",
      "PHYSICS CONSERVATION MONITORING FOR h3\n",
      "==================================================\n",
      "Final energy conservation violation rate: 0.0000\n",
      "\n",
      "Processing dataset for h=0.1575 (h6)\n",
      "Loading data from: data/new_processed_fix_new\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_699s - Sheet1_processed.csv (717 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv (532 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv (4621 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv (566 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv (621 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_630s - Sheet2_processed.csv (618 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv (516 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_675s - Sheet3_processed.csv (856 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv (826 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv (795 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv (932 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_585s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_598s - Sheet1_processed.csv (528 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_surf0_439s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv (912 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_719s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv (1087 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_644s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv (790 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_749s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_429s - Sheet2_processed.csv (880 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv (585 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv (4247 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv (611 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv (554 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_surf0_660s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_692s-Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv (589 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_431s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv (970 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv (803 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv (1411 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_830s - Sheet2_processed.csv (528 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv (787 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_surf0_493s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv (531 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_491s - Sheet3_processed.csv (702 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv (547 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_surf0_115s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv (3952 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_651s - Sheet4_processed.csv (554 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv (865 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv (870 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv (550 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_628s - Sheet4_processed.csv (539 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_389s - Sheet3_processed.csv (537 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_389s - Sheet4_processed.csv (2139 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv (887 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_825s-Sheer3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv (790 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_613s - Sheet1_processed.csv (550 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv (794 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_747s-Sheet3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv (4087 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv (849 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv (534 rows)\n",
      "\n",
      "==================================================\n",
      "FILE PROCESSING SUMMARY\n",
      "==================================================\n",
      "Total files scanned     : 77\n",
      "Files successfully loaded: 46\n",
      "Files skipped (filtered): 31\n",
      "Files skipped (unmatched): 0\n",
      "==================================================\n",
      "\n",
      "Total data loaded: 48187 rows, 26 columns\n",
      "Starting data preprocessing...\n",
      "Features: 21, Targets: 10\n",
      "Theory columns: 10, TC columns: 10\n",
      "Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10']\n",
      "After removing missing values: 48187 samples\n",
      "Training samples: 33730\n",
      "Validation samples: 7228\n",
      "Test samples: 7229\n",
      "Using device: cpu\n",
      "Model created with 409549 parameters for 10 sensors\n",
      "Starting training with 10 sensors for h=0.1575...\n",
      "Epoch [  10/1000] Train: 49.913624 Val: 50.007855 Violations: 0.2209 (ðŸš¨ CONCERNING) LR: 0.00100000\n",
      "Epoch [  20/1000] Train: 50.007666 Val: 50.007361 Violations: 0.2209 (ðŸš¨ CONCERNING) LR: 0.00100000\n",
      "Epoch [  30/1000] Train: 50.007271 Val: 50.007261 Violations: 0.2209 (ðŸš¨ CONCERNING) LR: 0.00050000\n",
      "Epoch [  40/1000] Train: 49.912441 Val: 50.006946 Violations: 0.2209 (ðŸš¨ CONCERNING) LR: 0.00050000\n",
      "Epoch [  50/1000] Train: 50.007036 Val: 50.006988 Violations: 0.2209 (ðŸš¨ CONCERNING) LR: 0.00025000\n",
      "Epoch [ 100/1000] Train: 49.912242 Val: 50.006830 Violations: 0.2209 (ðŸš¨ CONCERNING) LR: 0.00006250\n",
      "Epoch [ 200/1000] Train: 49.912217 Val: 50.006823 Violations: 0.2209 (ðŸš¨ CONCERNING) LR: 0.00000195\n",
      "Early stopping at epoch 251\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "Best validation loss: 50.006822\n",
      "Final violation rate: 0.2209\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS FOR h6 (Temperature Â°C)\n",
      "==================================================\n",
      "Sensor          RMSE (Â°C)    MAE (Â°C)     MAPE (%)     RÂ² Score    \n",
      "-----------------------------------------------------------------\n",
      "TC10            4.775        3.057        0.97         0.989       \n",
      "TC9             2.441        1.591        0.51         0.997       \n",
      "TC8             2.210        1.088        0.43         0.997       \n",
      "TC7             1.998        1.031        0.39         0.998       \n",
      "TC6             1.878        0.961        0.36         0.998       \n",
      "TC5             2.122        0.966        0.33         0.997       \n",
      "TC4             1.941        1.308        0.42         0.998       \n",
      "TC3             1.784        0.986        0.34         0.998       \n",
      "TC2             1.841        1.112        0.38         0.998       \n",
      "TC1             1.972        1.240        0.42         0.998       \n",
      "-----------------------------------------------------------------\n",
      "Overall         2.447        1.334                     0.997       \n",
      "\n",
      "==================================================\n",
      "PHYSICS CONSERVATION MONITORING FOR h6\n",
      "==================================================\n",
      "Final energy conservation violation rate: 0.2209\n",
      "\n",
      "Training completed for all models.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def get_h_category(h_value):\n",
    "    \"\"\"Map h value to category\"\"\"\n",
    "    if abs(h_value - 0.0375) < 0.001:  # h2\n",
    "        return 'h2'\n",
    "    elif abs(h_value - 0.084) < 0.001:  # h3\n",
    "        return 'h3'\n",
    "    elif abs(h_value - 0.1575) < 0.001:  # h6\n",
    "        return 'h6'\n",
    "    else:\n",
    "        # Default fallback - try to determine based on available TC columns\n",
    "        return 'h6'\n",
    "\n",
    "# Your provided functions (unchanged)\n",
    "def preprocess_data(data, test_size=0.3, val_size=0.5, use_enhanced_features=True, h_value=None):\n",
    "    \"\"\"Enhanced data preprocessing with support for variable TC sensors\"\"\"\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    \n",
    "    time_min = data[\"Time\"].min()\n",
    "    time_max = data[\"Time\"].max()\n",
    "    data[\"Time_norm\"] = (data[\"Time\"] - time_min) / (time_max - time_min)\n",
    "    \n",
    "    if use_enhanced_features:\n",
    "        data[\"TimeÂ²\"] = data[\"Time_norm\"] ** 2\n",
    "        data[\"TimeÂ³\"] = data[\"Time_norm\"] ** 3\n",
    "        data[\"Time_sin\"] = np.sin(2 * np.pi * data[\"Time_norm\"])\n",
    "        data[\"Time_cos\"] = np.cos(2 * np.pi * data[\"Time_norm\"])\n",
    "        data[\"flux_abs_interaction\"] = data[\"flux\"] * data[\"abs\"]\n",
    "        data[\"h_flux_interaction\"] = data[\"h\"] * data[\"flux\"]\n",
    "        base_features = [\"Time_norm\", \"TimeÂ²\", \"TimeÂ³\", \"Time_sin\", \"Time_cos\", \n",
    "                         \"h\", \"flux\", \"abs\", \"surf\", \"flux_abs_interaction\", \"h_flux_interaction\"]\n",
    "    else:\n",
    "        data[\"TimeÂ²\"] = data[\"Time_norm\"] ** 2\n",
    "        base_features = [\"Time_norm\", \"TimeÂ²\", \"h\", \"flux\", \"abs\", \"surf\"]\n",
    "    \n",
    "    theory_cols = [c for c in data.columns if c.startswith(\"Theoretical_Temps_\")]\n",
    "    tc_cols = [col for col in data.columns if (col.startswith(\"TC\") and not col.endswith(\"_rate\")) or col.startswith(\"TC_\")]\n",
    "    \n",
    "    h_category = get_h_category(h_value) if h_value is not None else 'h6'\n",
    "    if h_category == 'h2':\n",
    "        num_sensors = 3\n",
    "    elif h_category == 'h3':\n",
    "        num_sensors = 5\n",
    "    else:\n",
    "        num_sensors = 10\n",
    "    \n",
    "    tc_cols = tc_cols[:num_sensors]\n",
    "    theory_cols = theory_cols[:num_sensors] if len(theory_cols) >= num_sensors else theory_cols\n",
    "    \n",
    "    if not tc_cols:\n",
    "        print(\"Warning: No TC columns found. Creating dummy TC columns.\")\n",
    "        tc_cols = [f\"TC_{i+1}\" for i in range(num_sensors)]\n",
    "        for col in tc_cols:\n",
    "            if col not in data.columns:\n",
    "                data[col] = 20 + np.random.normal(0, 5, len(data))\n",
    "    \n",
    "    feature_cols = base_features + theory_cols\n",
    "    feature_cols = [col for col in feature_cols if col in data.columns]\n",
    "    \n",
    "    X = data[feature_cols].copy()\n",
    "    y = data[tc_cols].copy()\n",
    "    filenames = data[\"filename\"] if \"filename\" in data.columns else pd.Series([\"unknown\"] * len(data))\n",
    "    \n",
    "    print(f\"Features: {len(feature_cols)}, Targets: {len(tc_cols)}\")\n",
    "    print(f\"Theory columns: {len(theory_cols)}, TC columns: {len(tc_cols)}\")\n",
    "    print(f\"Feature columns: {feature_cols}\")\n",
    "    print(f\"Target columns: {tc_cols}\")\n",
    "    \n",
    "    mask = ~(X.isnull().any(axis=1) | y.isnull().any(axis=1))\n",
    "    X = X[mask].reset_index(drop=True)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "    filenames = filenames[mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"After removing missing values: {len(X)} samples\")\n",
    "    \n",
    "    if len(X) < 10:\n",
    "        raise ValueError(\"Not enough valid samples for training!\")\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp, _, _ = train_test_split(\n",
    "        X, y, filenames, test_size=test_size, random_state=SEED\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test, _, _ = train_test_split(\n",
    "        X_temp, y_temp, _, test_size=val_size, random_state=SEED\n",
    "    )\n",
    "    \n",
    "    X_scaler = StandardScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "    \n",
    "    X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "    X_val_scaled = X_scaler.transform(X_val)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "    \n",
    "    y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "    y_val_scaled = y_scaler.transform(y_val)\n",
    "    y_test_scaled = y_scaler.transform(y_test)\n",
    "    \n",
    "    model_dir = os.path.join(\"models\", h_category)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    joblib.dump(X_scaler, os.path.join(model_dir, \"X_scaler.pkl\"))\n",
    "    joblib.dump(y_scaler, os.path.join(model_dir, \"y_scaler.pkl\"))\n",
    "    joblib.dump({\"time_min\": time_min, \"time_max\": time_max}, os.path.join(model_dir, \"time_range.pkl\"))\n",
    "    joblib.dump({\"feature_cols\": feature_cols, \"tc_cols\": tc_cols, \"num_sensors\": num_sensors}, os.path.join(model_dir, \"column_info.pkl\"))\n",
    "    \n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "    )\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(X_test_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=min(64, len(X_train)//4), shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=min(64, len(X_val)//4), shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=min(64, len(X_test)//4), shuffle=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols, num_sensors\n",
    "\n",
    "class PhysicsInformedLoss(nn.Module):\n",
    "    \"\"\"Physics-informed loss function with support for variable TC sensors\"\"\"\n",
    "    def __init__(self, smoothness_weight=0.005, gradient_weight=0.0001, physics_weight=0.5, \n",
    "                 conservation_violation_penalty=100.0):\n",
    "        super().__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.smoothness_weight = smoothness_weight\n",
    "        self.gradient_weight = gradient_weight\n",
    "        self.physics_weight = physics_weight\n",
    "        self.conservation_violation_penalty = conservation_violation_penalty\n",
    "        \n",
    "        self.r = 2.0375 * 0.0254\n",
    "        self.A_rec = np.pi * (self.r ** 2)\n",
    "        self.rho = 1836.31\n",
    "        self.cp = 1512\n",
    "        \n",
    "        self.A_rec_tensor = None\n",
    "        self.rho_tensor = None\n",
    "        self.cp_tensor = None\n",
    "        \n",
    "        self.violation_count = 0\n",
    "        self.total_batches = 0\n",
    "    \n",
    "    def _init_tensors(self, device):\n",
    "        if self.A_rec_tensor is None:\n",
    "            self.A_rec_tensor = torch.tensor(self.A_rec, device=device, dtype=torch.float32)\n",
    "            self.rho_tensor = torch.tensor(self.rho, device=device, dtype=torch.float32)\n",
    "            self.cp_tensor = torch.tensor(self.cp, device=device, dtype=torch.float32)\n",
    "    \n",
    "    def compute_physics_loss(self, predictions, targets, inputs, num_sensors):\n",
    "        device = predictions.device\n",
    "        self._init_tensors(device)\n",
    "        \n",
    "        batch_size = predictions.shape[0]\n",
    "        self.total_batches += 1\n",
    "        \n",
    "        try:\n",
    "            flux = inputs[:, 6]\n",
    "            h = inputs[:, 5]\n",
    "            incoming_energy = flux * self.A_rec_tensor\n",
    "            mass = self.rho_tensor * h * self.A_rec_tensor\n",
    "            \n",
    "            theoretical_start_idx = 11\n",
    "            if inputs.shape[1] >= theoretical_start_idx + num_sensors:\n",
    "                theoretical_temps = inputs[:, theoretical_start_idx:theoretical_start_idx + num_sensors]\n",
    "                temp_change = torch.mean(predictions - theoretical_temps, dim=1)\n",
    "            else:\n",
    "                temp_change = torch.mean(predictions - targets, dim=1)\n",
    "            \n",
    "            dt = torch.ones_like(temp_change)\n",
    "            total_energy_stored = mass * self.cp_tensor * temp_change / dt\n",
    "            \n",
    "            conservation_loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "            \n",
    "            violation_mask = total_energy_stored > incoming_energy\n",
    "            if torch.any(violation_mask):\n",
    "                excess = total_energy_stored[violation_mask] - incoming_energy[violation_mask]\n",
    "                relative_violation = excess / (incoming_energy[violation_mask] + 1e-6)\n",
    "                conservation_loss = conservation_loss + self.conservation_violation_penalty * torch.mean(relative_violation ** 2)\n",
    "                self.violation_count += torch.sum(violation_mask).item()\n",
    "            \n",
    "            margin = 0.05 * incoming_energy\n",
    "            soft_violation_mask = (total_energy_stored > incoming_energy - margin) & (~violation_mask)\n",
    "            if torch.any(soft_violation_mask):\n",
    "                soft_excess = total_energy_stored[soft_violation_mask] - incoming_energy[soft_violation_mask]\n",
    "                soft_relative_violation = soft_excess / (incoming_energy[soft_violation_mask] + 1e-6)\n",
    "                soft_penalty = torch.mean(soft_relative_violation ** 2)\n",
    "                conservation_loss = conservation_loss + 0.1 * self.conservation_violation_penalty * soft_penalty\n",
    "            \n",
    "            acceptable_mask = total_energy_stored <= incoming_energy\n",
    "            if torch.any(acceptable_mask):\n",
    "                energy_difference = incoming_energy[acceptable_mask] - total_energy_stored[acceptable_mask]\n",
    "                efficiency_penalty = torch.mean(energy_difference) * 0.01\n",
    "                conservation_loss = conservation_loss + efficiency_penalty\n",
    "            \n",
    "            balance_tolerance = 0.01 * torch.abs(incoming_energy)\n",
    "            energy_difference = torch.abs(incoming_energy - total_energy_stored)\n",
    "            balance_mask = energy_difference <= balance_tolerance\n",
    "            if torch.any(balance_mask):\n",
    "                balance_reward = torch.mean(energy_difference[balance_mask])\n",
    "                conservation_loss = conservation_loss - 0.05 * balance_reward\n",
    "            \n",
    "            return conservation_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Physics loss computation failed: {e}\")\n",
    "            return torch.tensor(100.0, device=device, requires_grad=True)\n",
    "    \n",
    "    def get_violation_rate(self):\n",
    "        if self.total_batches == 0:\n",
    "            return 0.0\n",
    "        total_samples = self.total_batches * 64\n",
    "        return self.violation_count / total_samples if total_samples > 0 else 0.0\n",
    "    \n",
    "    def reset_violation_tracking(self):\n",
    "        self.violation_count = 0\n",
    "        self.total_batches = 0\n",
    "    \n",
    "    def forward(self, predictions, targets, inputs=None, num_sensors=None):\n",
    "        mse_loss = self.mse_loss(predictions, targets)\n",
    "        \n",
    "        if predictions.shape[1] > 1:\n",
    "            smoothness_loss = torch.mean(torch.abs(predictions[:, 1:] - predictions[:, :-1]))\n",
    "            gradient_loss = torch.mean(torch.abs(torch.diff(predictions, dim=1)))\n",
    "        else:\n",
    "            smoothness_loss = torch.tensor(0.0, device=predictions.device)\n",
    "            gradient_loss = torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "        if inputs is not None and num_sensors is not None:\n",
    "            physics_loss = self.compute_physics_loss(predictions, targets, inputs, num_sensors)\n",
    "        else:\n",
    "            physics_loss = torch.tensor(50.0, device=predictions.device, requires_grad=True)\n",
    "        \n",
    "        total_loss = (mse_loss + \n",
    "                     self.smoothness_weight * smoothness_loss + \n",
    "                     self.gradient_weight * gradient_loss +\n",
    "                     self.physics_weight * physics_loss)\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "def train_model_with_conservation_monitoring(model, train_loader, val_loader, device, num_sensors, h_value, epochs=1000, patience=50):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, min_lr=1e-6)\n",
    "    \n",
    "    criterion = PhysicsInformedLoss(\n",
    "        smoothness_weight=0.005,\n",
    "        gradient_weight=0.0001,\n",
    "        physics_weight=0.5,\n",
    "        conservation_violation_penalty=100.0\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    violation_rates = []\n",
    "    \n",
    "    print(f\"Starting training with {num_sensors} sensors for h={h_value}...\")\n",
    "    for epoch in range(epochs):\n",
    "        criterion.reset_violation_tracking()\n",
    "        model.train()\n",
    "        train_loss_epoch = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch, X_batch, num_sensors)\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"Warning: Invalid loss detected at epoch {epoch}\")\n",
    "                continue\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss_epoch += loss.item()\n",
    "        train_loss_epoch /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss_epoch = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                predictions = model(X_batch)\n",
    "                loss = criterion(predictions, y_batch, X_batch, num_sensors)\n",
    "                val_loss_epoch += loss.item()\n",
    "        val_loss_epoch /= len(val_loader)\n",
    "        \n",
    "        violation_rate = criterion.get_violation_rate()\n",
    "        violation_rates.append(violation_rate)\n",
    "        scheduler.step(val_loss_epoch)\n",
    "        train_losses.append(train_loss_epoch)\n",
    "        val_losses.append(val_loss_epoch)\n",
    "        \n",
    "        adjusted_val_loss = val_loss_epoch + (violation_rate * 100)\n",
    "        if adjusted_val_loss < best_val_loss:\n",
    "            best_val_loss = adjusted_val_loss\n",
    "            patience_counter = 0\n",
    "            model_dir = os.path.join(\"models\", get_h_category(h_value))\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss_epoch,\n",
    "                'train_loss': train_loss_epoch,\n",
    "                'violation_rate': violation_rate,\n",
    "            }, os.path.join(model_dir, \"best_thermal_model.pth\"))\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0 or (epoch < 50 and (epoch + 1) % 10 == 0):\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            status = \"âœ… EXCELLENT\" if violation_rate < 0.01 else \"âš ï¸ ACCEPTABLE\" if violation_rate < 0.05 else \"ðŸš¨ CONCERNING\"\n",
    "            print(f\"Epoch [{epoch+1:4d}/{epochs}] \"\n",
    "                  f\"Train: {train_loss_epoch:.6f} \"\n",
    "                  f\"Val: {val_loss_epoch:.6f} \"\n",
    "                  f\"Violations: {violation_rate:.4f} ({status}) \"\n",
    "                  f\"LR: {current_lr:.8f}\")\n",
    "    \n",
    "    model_dir = os.path.join(\"models\", get_h_category(h_value))\n",
    "    checkpoint = torch.load(os.path.join(model_dir, \"best_thermal_model.pth\"))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING COMPLETED\")\n",
    "    print(f\"Best validation loss: {checkpoint['val_loss']:.6f}\")\n",
    "    print(f\"Final violation rate: {checkpoint['violation_rate']:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses, violation_rates\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "# Utility to serialize scaler to bytes\n",
    "def scaler_to_bytes(scaler):\n",
    "    buf = BytesIO()\n",
    "    joblib.dump(scaler, buf)\n",
    "    buf.seek(0)\n",
    "    return buf.read()\n",
    "\n",
    "# Utility to deserialize scaler from bytes\n",
    "def bytes_to_scaler(bytes_obj):\n",
    "    buf = BytesIO(bytes_obj)\n",
    "    scaler = joblib.load(buf)\n",
    "    return scaler\n",
    "\n",
    "# --- Your existing preprocess_data, PhysicsInformedLoss, train_model_with_conservation_monitoring, etc. unchanged ---\n",
    "\n",
    "# Updated main() with single bundle save:\n",
    "def main():\n",
    "    try:\n",
    "        DATA_DIR = get_data_directory()\n",
    "        h_values = [0.0375, 0.084, 0.1575]  # h2, h3, h6\n",
    "        all_results = {}\n",
    "        model_metadata = {}\n",
    "        \n",
    "        all_models_bundle = {}\n",
    "        \n",
    "        for h_val in h_values:\n",
    "            h_category = get_h_category(h_val)\n",
    "            print(f\"\\nProcessing dataset for h={h_val} ({h_category})\")\n",
    "            data = load_data(DATA_DIR, h_filter=h_val)\n",
    "            \n",
    "            train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols, num_sensors = preprocess_data(\n",
    "                data, use_enhanced_features=True, h_value=h_val\n",
    "            )\n",
    "            \n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            print(f\"Using device: {device}\")\n",
    "            \n",
    "            model = EnhancedThermalNet(\n",
    "                input_size=train_loader.dataset.tensors[0].shape[1],\n",
    "                output_size=num_sensors,\n",
    "                hidden_dims=[512, 256, 256, 128],\n",
    "                dropout_rate=0.2\n",
    "            ).to(device)\n",
    "            \n",
    "            print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters for {num_sensors} sensors\")\n",
    "            \n",
    "            train_losses, val_losses, violation_rates = train_model_with_conservation_monitoring(\n",
    "                model, train_loader, val_loader, device, num_sensors, h_val, epochs=1000, patience=50\n",
    "            )\n",
    "            \n",
    "            sensor_depths = np.array([0.0, 0.018, 0.035, 0.053, 0.070, 0.099, 0.105, 0.123, 0.140, 0.158])[:num_sensors]\n",
    "            sensor_names = [f\"TC{i+1}\" for i in range(num_sensors)]\n",
    "            results = evaluate_model(model, test_loader, y_scaler, device, sensor_depths, sensor_names)\n",
    "            \n",
    "            all_results[h_val] = {\n",
    "                'model': model,\n",
    "                'results': results,\n",
    "                'X_scaler': X_scaler,\n",
    "                'y_scaler': y_scaler,\n",
    "                'tc_cols': tc_cols,\n",
    "                'violation_rates': violation_rates\n",
    "            }\n",
    "            \n",
    "            # Save metadata info\n",
    "            model_metadata[h_category] = {\n",
    "                \"h_value\": h_val,\n",
    "                \"num_sensors\": num_sensors,\n",
    "                \"tc_cols\": tc_cols,\n",
    "            }\n",
    "            \n",
    "            # Save model state dict and scalers as bytes inside the bundle\n",
    "            all_models_bundle[h_category] = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'X_scaler_bytes': scaler_to_bytes(X_scaler),\n",
    "                'y_scaler_bytes': scaler_to_bytes(y_scaler),\n",
    "                'tc_cols': tc_cols,\n",
    "                'num_sensors': num_sensors,\n",
    "                'feature_dim': train_loader.dataset.tensors[0].shape[1],\n",
    "                'violation_rates': violation_rates,\n",
    "            }\n",
    "            \n",
    "            # Print evaluation summary\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"EVALUATION RESULTS FOR {h_category} (Temperature Â°C)\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"{'Sensor':<15} {'RMSE (Â°C)':<12} {'MAE (Â°C)':<12} {'MAPE (%)':<12} {'RÂ² Score':<12}\")\n",
    "            print(\"-\"*65)\n",
    "            for i, name in enumerate(results['sensor_names_rev']):\n",
    "                print(f\"{name:<15} {results['rmse_temp'][i]:<12.3f} {results['mae_temp'][i]:<12.3f} \"\n",
    "                      f\"{results['mape_temp'][i]:<12.2f} {results['r2_temp'][i]:<12.3f}\")\n",
    "            print(\"-\"*65)\n",
    "            print(f\"{'Overall':<15} {results['overall_rmse_temp']:<12.3f} {results['overall_mae_temp']:<12.3f} \"\n",
    "                  f\"{'':<12} {results['overall_r2_temp']:<12.3f}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"PHYSICS CONSERVATION MONITORING FOR {h_category}\")\n",
    "            print(\"=\"*50)\n",
    "            final_violation_rate = violation_rates[-1] if violation_rates else 0.0\n",
    "            print(f\"Final energy conservation violation rate: {final_violation_rate:.4f}\")\n",
    "        \n",
    "        # Save entire model+scaler bundle in one .pth file\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        torch.save(all_models_bundle, \"models/all_h_models_bundle.pth\")\n",
    "        \n",
    "        # Save JSON metadata separately\n",
    "        with open(os.path.join(\"models\", \"model_metadata.json\"), \"w\") as f:\n",
    "            json.dump(model_metadata, f, indent=4)\n",
    "        \n",
    "        return all_results\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# Loading example (for inference or prediction later):\n",
    "\n",
    "# def load_bundle_model_and_scalers(h_category):\n",
    "#     bundle = torch.load(\"models/all_h_models_bundle.pth\")\n",
    "#     entry = bundle[h_category]\n",
    "    \n",
    "#     model = EnhancedThermalNet(\n",
    "#         input_size=entry['feature_dim'],\n",
    "#         output_size=entry['num_sensors'],\n",
    "#         hidden_dims=[512, 256, 256, 128],\n",
    "#         dropout_rate=0.2\n",
    "#     )\n",
    "#     model.load_state_dict(entry['model_state_dict'])\n",
    "#     model.eval()\n",
    "    \n",
    "#     X_scaler = bytes_to_scaler(entry['X_scaler_bytes'])\n",
    "#     y_scaler = bytes_to_scaler(entry['y_scaler_bytes'])\n",
    "#     tc_cols = entry['tc_cols']\n",
    "    \n",
    "#     return model, X_scaler, y_scaler, tc_cols\n",
    "\n",
    "# --- Your other code remains unchanged ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()\n",
    "    if results is not None:\n",
    "        print(\"\\nTraining completed for all models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f53b66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "H3 INTERPOLATION EXAMPLE\n",
      "============================================================\n",
      "Loading h-specific models...\n",
      "Found bundled model, loading...\n",
      "âœ“ Successfully loaded h2 model\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 failed: module 'joblib' has no attribute 'loads'\n",
      "Deserialization method 3 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 failed: module 'joblib' has no attribute 'loads'\n",
      "Deserialization method 3 failed: STACK_GLOBAL requires str\n",
      "âœ“ Loaded scalers for h2\n",
      "âœ“ Successfully loaded h3 model\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 failed: module 'joblib' has no attribute 'loads'\n",
      "Deserialization method 3 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 failed: module 'joblib' has no attribute 'loads'\n",
      "Deserialization method 3 failed: STACK_GLOBAL requires str\n",
      "âœ“ Loaded scalers for h3\n",
      "âœ“ Successfully loaded h6 model\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 failed: module 'joblib' has no attribute 'loads'\n",
      "Deserialization method 3 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 failed: module 'joblib' has no attribute 'loads'\n",
      "Deserialization method 3 failed: STACK_GLOBAL requires str\n",
      "âœ“ Loaded scalers for h6\n",
      "âœ“ Interpolation models loaded successfully!\n",
      "Available models: ['h2', 'h3', 'h6']\n",
      "\n",
      "ðŸ” PREDICTION COMPARISON\n",
      "==================================================\n",
      "ðŸ“Š Direct h3 prediction:\n",
      "  TC1_tip: 358.215 Â°C\n",
      "  TC2: 359.257 Â°C\n",
      "  TC3: 364.551 Â°C\n",
      "\n",
      "ðŸ“Š Interpolated h3 prediction (from h2 & h6):\n",
      "\n",
      "ðŸ”„ Interpolating h=0.084 using models: ['h2', 'h6']\n",
      "âœ“ Got prediction from h2 (h=0.0375)\n",
      "âœ“ Got prediction from h6 (h=0.1575)\n",
      "ðŸ“Š Linear interpolation: h2(0.0375) --[0.388]-> h6(0.1575)\n",
      "  TC1_tip: 357.376 Â°C\n",
      "  TC2: 360.231 Â°C\n",
      "  TC3: 351.557 Â°C\n",
      "\n",
      "ðŸ“Š Differences (Direct - Interpolated):\n",
      "  TC1_tip: +0.839 Â°C\n",
      "  TC2: -0.974 Â°C\n",
      "  TC3: +12.995 Â°C\n",
      "\n",
      "ðŸ“Š RMSE between direct and interpolated: 7.539 Â°C\n",
      "\n",
      "==================================================\n",
      "DIFFERENT INTERPOLATION METHODS\n",
      "==================================================\n",
      "\n",
      "ðŸ“Š Method: linear\n",
      "\n",
      "ðŸ”„ Interpolating h=0.084 using models: ['h2', 'h6']\n",
      "âœ“ Got prediction from h2 (h=0.0375)\n",
      "âœ“ Got prediction from h6 (h=0.1575)\n",
      "ðŸ“Š Linear interpolation: h2(0.0375) --[0.388]-> h6(0.1575)\n",
      "  TC1_tip: 357.376 Â°C\n",
      "  TC2: 360.231 Â°C\n",
      "  TC3: 351.557 Â°C\n",
      "\n",
      "ðŸ“Š Method: weighted_average\n",
      "\n",
      "ðŸ”„ Interpolating h=0.084 using models: ['h2', 'h6']\n",
      "âœ“ Got prediction from h2 (h=0.0375)\n",
      "âœ“ Got prediction from h6 (h=0.1575)\n",
      "ðŸ“Š Weighted interpolation weights: {'h2': 0.714158239143367, 'h6': 0.28584176085663304}\n",
      "  TC1_tip: 358.456 Â°C\n",
      "  TC2: 362.062 Â°C\n",
      "  TC3: 351.672 Â°C\n",
      "\n",
      "============================================================\n",
      "BATCH INTERPOLATION TEST\n",
      "============================================================\n",
      "Loading h-specific models...\n",
      "Found bundled model, loading...\n",
      "âœ“ Successfully loaded h2 model\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 failed: module 'joblib' has no attribute 'loads'\n",
      "Deserialization method 3 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 failed: module 'joblib' has no attribute 'loads'\n",
      "Deserialization method 3 failed: STACK_GLOBAL requires str\n",
      "âœ“ Loaded scalers for h2\n",
      "âœ“ Successfully loaded h3 model\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 failed: module 'joblib' has no attribute 'loads'\n",
      "Deserialization method 3 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 failed: module 'joblib' has no attribute 'loads'\n",
      "Deserialization method 3 failed: STACK_GLOBAL requires str\n",
      "âœ“ Loaded scalers for h3\n",
      "âœ“ Successfully loaded h6 model\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 failed: module 'joblib' has no attribute 'loads'\n",
      "Deserialization method 3 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 failed: module 'joblib' has no attribute 'loads'\n",
      "Deserialization method 3 failed: STACK_GLOBAL requires str\n",
      "âœ“ Loaded scalers for h6\n",
      "âœ“ Interpolation models loaded successfully!\n",
      "Available models: ['h2', 'h3', 'h6']\n",
      "Time (s) | TC1_tip | TC2    | TC3    | Method\n",
      "--------------------------------------------------\n",
      "\n",
      "ðŸ”„ Interpolating h=0.084 using models: ['h2', 'h6']\n",
      "âœ“ Got prediction from h2 (h=0.0375)\n",
      "âœ“ Got prediction from h6 (h=0.1575)\n",
      "ðŸ“Š Linear interpolation: h2(0.0375) --[0.388]-> h6(0.1575)\n",
      "     600 | 358.395  | 361.272 | 352.690 | Linear\n",
      "\n",
      "ðŸ”„ Interpolating h=0.084 using models: ['h2', 'h6']\n",
      "âœ“ Got prediction from h2 (h=0.0375)\n",
      "âœ“ Got prediction from h6 (h=0.1575)\n",
      "ðŸ“Š Linear interpolation: h2(0.0375) --[0.388]-> h6(0.1575)\n",
      "    1200 | 357.920  | 360.792 | 352.195 | Linear\n",
      "\n",
      "ðŸ”„ Interpolating h=0.084 using models: ['h2', 'h6']\n",
      "âœ“ Got prediction from h2 (h=0.0375)\n",
      "âœ“ Got prediction from h6 (h=0.1575)\n",
      "ðŸ“Š Linear interpolation: h2(0.0375) --[0.388]-> h6(0.1575)\n",
      "    1800 | 357.376  | 360.231 | 351.557 | Linear\n",
      "\n",
      "ðŸ”„ Interpolating h=0.084 using models: ['h2', 'h6']\n",
      "âœ“ Got prediction from h2 (h=0.0375)\n",
      "âœ“ Got prediction from h6 (h=0.1575)\n",
      "ðŸ“Š Linear interpolation: h2(0.0375) --[0.388]-> h6(0.1575)\n",
      "    2400 | 356.738  | 359.599 | 350.837 | Linear\n",
      "\n",
      "ðŸ”„ Interpolating h=0.084 using models: ['h2', 'h6']\n",
      "âœ“ Got prediction from h2 (h=0.0375)\n",
      "âœ“ Got prediction from h6 (h=0.1575)\n",
      "ðŸ“Š Linear interpolation: h2(0.0375) --[0.388]-> h6(0.1575)\n",
      "    3600 | 355.102  | 358.038 | 349.158 | Linear\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Prediction System with H-Value Interpolation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from typing import Dict, List, Union, Optional, Tuple\n",
    "from io import BytesIO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your actual EnhancedThermalNet architecture (keeping the same)\n",
    "class EnhancedThermalNet(nn.Module):\n",
    "    \"\"\"Enhanced thermal neural network with attention and residual connections\"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_dims=[512, 256, 256, 128], dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Input processing\n",
    "        self.input_layer = nn.Linear(input_size, hidden_dims[0])\n",
    "        self.input_norm = nn.LayerNorm(hidden_dims[0])\n",
    "        \n",
    "        # Feature attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_size, max(1, input_size // 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(max(1, input_size // 2), input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Hidden layers with residual connections\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.residual_projections = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            layer = nn.Linear(hidden_dims[i], hidden_dims[i + 1])\n",
    "            norm = nn.LayerNorm(hidden_dims[i + 1])\n",
    "            \n",
    "            if hidden_dims[i] != hidden_dims[i + 1]:\n",
    "                residual_proj = nn.Linear(hidden_dims[i], hidden_dims[i + 1])\n",
    "            else:\n",
    "                residual_proj = nn.Identity()\n",
    "            \n",
    "            self.layers.append(layer)\n",
    "            self.norms.append(norm)\n",
    "            self.residual_projections.append(residual_proj)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_size)\n",
    "        \n",
    "        # Activation and regularization\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using He initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply attention to input features\n",
    "        attention_weights = self.attention(x)\n",
    "        x_attended = x * attention_weights\n",
    "        \n",
    "        # Input processing\n",
    "        x = self.activation(self.input_norm(self.input_layer(x_attended)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Forward through hidden layers with residual connections\n",
    "        for i, (layer, norm, residual_proj) in enumerate(zip(self.layers, self.norms, self.residual_projections)):\n",
    "            residual = residual_proj(x)\n",
    "            x = layer(x)\n",
    "            x = norm(x)\n",
    "            x = self.activation(x)\n",
    "            x = x + residual\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "\n",
    "class HInterpolationPredictor:\n",
    "    \"\"\"Enhanced prediction system with h-value interpolation capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, models_dir=\"models\"):\n",
    "        self.models_dir = models_dir\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.metadata = {}\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # H-value to configuration mapping\n",
    "        self.h_configs = {\n",
    "            0.0375: 'h2',\n",
    "            0.084: 'h3',\n",
    "            0.1575: 'h6'\n",
    "        }\n",
    "        \n",
    "        # Expected TC counts and sensor mapping\n",
    "        self.tc_counts = {\n",
    "            'h2': 3,\n",
    "            'h3': 5,\n",
    "            'h6': 10\n",
    "        }\n",
    "        \n",
    "        self.tc_sensors = {\n",
    "            'h2': ['TC1_tip', 'TC2', 'TC3'],\n",
    "            'h3': ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5'],\n",
    "            'h6': ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10']\n",
    "        }\n",
    "        \n",
    "        # Common sensor mapping for interpolation\n",
    "        self.common_sensors = ['TC1_tip', 'TC2', 'TC3']  # Sensors available in all configurations\n",
    "        \n",
    "        # Default feature columns\n",
    "        self.default_feature_cols = [\n",
    "            'Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos',\n",
    "            'h', 'flux', 'abs', 'surf',\n",
    "            'flux_abs_interaction', 'h_flux_interaction'\n",
    "        ]\n",
    "    \n",
    "    def load_models(self):\n",
    "        \"\"\"Load all h-specific models and their components\"\"\"\n",
    "        print(\"Loading h-specific models...\")\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(os.path.join(self.models_dir, \"all_h_models_bundle.pth\")):\n",
    "                print(\"Found bundled model, loading...\")\n",
    "                self._load_bundled_models()\n",
    "                return len(self.models) > 0\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load bundled model: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        # Fall back to loading individual models\n",
    "        for h_val, h_name in self.h_configs.items():\n",
    "            try:\n",
    "                self._load_individual_model(h_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {h_name} model: {e}\")\n",
    "        \n",
    "        loaded_models = list(self.models.keys())\n",
    "        print(f\"Successfully loaded models: {loaded_models}\")\n",
    "        return len(loaded_models) > 0\n",
    "    \n",
    "    def _deserialize_scaler(self, scaler_bytes):\n",
    "        \"\"\"Deserialize scaler from bytes - Multiple methods\"\"\"\n",
    "        methods = [\n",
    "            lambda: pickle.loads(scaler_bytes),\n",
    "            lambda: joblib.loads(scaler_bytes),\n",
    "            lambda: pickle.load(BytesIO(scaler_bytes)),\n",
    "            lambda: joblib.load(BytesIO(scaler_bytes))\n",
    "        ]\n",
    "        \n",
    "        for i, method in enumerate(methods, 1):\n",
    "            try:\n",
    "                return method()\n",
    "            except Exception as e:\n",
    "                print(f\"Deserialization method {i} failed: {e}\")\n",
    "        \n",
    "        # Check if it's already a scaler object\n",
    "        if hasattr(scaler_bytes, 'transform'):\n",
    "            print(\"Scaler is already an object, not bytes\")\n",
    "            return scaler_bytes\n",
    "        \n",
    "        print(f\"All deserialization methods failed. Data type: {type(scaler_bytes)}\")\n",
    "        return None\n",
    "    \n",
    "    def _load_bundled_models(self):\n",
    "        \"\"\"Load from bundled model file\"\"\"\n",
    "        bundle_path = os.path.join(self.models_dir, \"all_h_models_bundle.pth\")\n",
    "        bundle = torch.load(bundle_path, map_location=self.device)\n",
    "        \n",
    "        # Load metadata if available\n",
    "        metadata_path = os.path.join(self.models_dir, \"model_metadata.json\")\n",
    "        if os.path.exists(metadata_path):\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                self.metadata = json.load(f)\n",
    "        \n",
    "        # Extract models and components from bundle\n",
    "        for h_name in ['h2', 'h3', 'h6']:\n",
    "            if h_name in bundle:\n",
    "                h_data = bundle[h_name]\n",
    "                \n",
    "                try:\n",
    "                    # Load model\n",
    "                    if 'model_state_dict' in h_data:\n",
    "                        input_size = h_data.get('feature_dim')\n",
    "                        output_size = h_data.get('num_sensors')\n",
    "                        \n",
    "                        if input_size is None or output_size is None:\n",
    "                            state_dict = h_data['model_state_dict']\n",
    "                            if 'input_layer.weight' in state_dict:\n",
    "                                input_size = state_dict['input_layer.weight'].shape[1]\n",
    "                            if 'output_layer.weight' in state_dict:\n",
    "                                output_size = state_dict['output_layer.weight'].shape[0]\n",
    "                        \n",
    "                        if input_size is not None and output_size is not None:\n",
    "                            model = EnhancedThermalNet(\n",
    "                                input_size=input_size,\n",
    "                                output_size=output_size,\n",
    "                                hidden_dims=[512, 256, 256, 128],\n",
    "                                dropout_rate=0.3\n",
    "                            ).to(self.device)\n",
    "                            \n",
    "                            model.load_state_dict(h_data['model_state_dict'])\n",
    "                            model.eval()\n",
    "                            self.models[h_name] = model\n",
    "                            print(f\"âœ“ Successfully loaded {h_name} model\")\n",
    "                        else:\n",
    "                            print(f\"âœ— Could not determine model dimensions for {h_name}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Load scalers\n",
    "                    if 'X_scaler_bytes' in h_data and 'y_scaler_bytes' in h_data:\n",
    "                        X_scaler = self._deserialize_scaler(h_data['X_scaler_bytes'])\n",
    "                        y_scaler = self._deserialize_scaler(h_data['y_scaler_bytes'])\n",
    "                        \n",
    "                        if X_scaler is not None and y_scaler is not None:\n",
    "                            # Create feature columns list\n",
    "                            feature_cols = self.default_feature_cols.copy()\n",
    "                            tc_count = self.tc_counts[h_name]\n",
    "                            for i in range(tc_count):\n",
    "                                feature_cols.append(f'Theoretical_Temps_{i+1}')\n",
    "                            \n",
    "                            # Validate feature count\n",
    "                            if hasattr(X_scaler, 'n_features_in_'):\n",
    "                                expected_features = X_scaler.n_features_in_\n",
    "                                if len(feature_cols) != expected_features:\n",
    "                                    print(f\"Warning: Feature count mismatch for {h_name}. Expected: {expected_features}, Got: {len(feature_cols)}\")\n",
    "                                    if len(feature_cols) > expected_features:\n",
    "                                        feature_cols = feature_cols[:expected_features]\n",
    "                                    else:\n",
    "                                        while len(feature_cols) < expected_features:\n",
    "                                            feature_cols.append(f'feature_{len(feature_cols)}')\n",
    "                            \n",
    "                            self.scalers[h_name] = {\n",
    "                                'X_scaler': X_scaler,\n",
    "                                'y_scaler': y_scaler,\n",
    "                                'time_range': {'time_min': 0, 'time_max': 7200},\n",
    "                                'feature_cols': feature_cols,\n",
    "                                'tc_cols': h_data.get('tc_cols', self.tc_sensors[h_name])\n",
    "                            }\n",
    "                            print(f\"âœ“ Loaded scalers for {h_name}\")\n",
    "                        else:\n",
    "                            print(f\"âœ— Failed to deserialize scalers for {h_name}\")\n",
    "                            self._create_dummy_scalers(h_name, input_size, output_size)\n",
    "                    else:\n",
    "                        print(f\"âœ— Missing scaler bytes for {h_name}\")\n",
    "                        self._create_dummy_scalers(h_name, input_size, output_size)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âœ— Error loading {h_name}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "    \n",
    "    def _create_dummy_scalers(self, h_name, input_size, output_size):\n",
    "        \"\"\"Create dummy scalers when deserialization fails\"\"\"\n",
    "        try:\n",
    "            X_scaler = StandardScaler()\n",
    "            y_scaler = MinMaxScaler()  # Match training scaler\n",
    "            \n",
    "            # Fit with dummy data\n",
    "            X_dummy = np.random.randn(10, input_size)\n",
    "            y_dummy = np.random.randn(10, output_size)  # Shape (10, output_size)\n",
    "            \n",
    "            X_scaler.fit(X_dummy)\n",
    "            y_scaler.fit(y_dummy)  # Fit on multi-output shape\n",
    "            \n",
    "            feature_cols = self.default_feature_cols.copy()\n",
    "            tc_count = self.tc_counts[h_name]\n",
    "            for i in range(tc_count):\n",
    "                feature_cols.append(f'Theoretical_Temps_{i+1}')\n",
    "                \n",
    "            if len(feature_cols) > input_size:\n",
    "                feature_cols = feature_cols[:input_size]\n",
    "            else:\n",
    "                while len(feature_cols) < input_size:\n",
    "                    feature_cols.append(f'dummy_feature_{len(feature_cols)}')\n",
    "            \n",
    "            self.scalers[h_name] = {\n",
    "                'X_scaler': X_scaler,\n",
    "                'y_scaler': y_scaler,\n",
    "                'time_range': {'time_min': 0, 'time_max': 7200},\n",
    "                'feature_cols': feature_cols,\n",
    "                'tc_cols': self.tc_sensors[h_name]\n",
    "            }\n",
    "            print(f\"âœ“ Created dummy scalers for {h_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to create dummy scalers for {h_name}: {e}\")\n",
    "    \n",
    "    def get_h_category(self, h_value):\n",
    "        \"\"\"Get h category from h value\"\"\"\n",
    "        for h_key, category in self.h_configs.items():\n",
    "            if abs(h_value - h_key) < 0.001:\n",
    "                return category\n",
    "        \n",
    "        closest_h = min(self.h_configs.keys(), key=lambda x: abs(x - h_value))\n",
    "        category = self.h_configs[closest_h]\n",
    "        print(f\"Warning: h={h_value} not exactly matched. Using closest: h={closest_h} ({category})\")\n",
    "        return category\n",
    "    \n",
    "    def _prepare_features(self, time, h, flux, abs_val, surf, theoretical_temps, h_category):\n",
    "        \"\"\"Prepare feature vector for a specific h-category\"\"\"\n",
    "        scalers = self.scalers[h_category]\n",
    "        \n",
    "        # Validate theoretical temperatures\n",
    "        expected_count = self.tc_counts[h_category]\n",
    "        if len(theoretical_temps) != expected_count:\n",
    "            if len(theoretical_temps) < expected_count:\n",
    "                theoretical_temps = list(theoretical_temps) + [theoretical_temps[-1]] * (expected_count - len(theoretical_temps))\n",
    "            else:\n",
    "                theoretical_temps = theoretical_temps[:expected_count]\n",
    "        \n",
    "        # Normalize time\n",
    "        time_range = scalers['time_range']\n",
    "        time_min = time_range['time_min']\n",
    "        time_max = time_range['time_max']\n",
    "        time_norm = (time - time_min) / (time_max - time_min)\n",
    "        \n",
    "        # Create feature vector\n",
    "        features = {\n",
    "            'Time_norm': time_norm,\n",
    "            'TimeÂ²': time_norm ** 2,\n",
    "            'TimeÂ³': time_norm ** 3,\n",
    "            'Time_sin': np.sin(2 * np.pi * time_norm),\n",
    "            'Time_cos': np.cos(2 * np.pi * time_norm),\n",
    "            'h': h,\n",
    "            'flux': flux,\n",
    "            'abs': abs_val,\n",
    "            'surf': surf,\n",
    "            'flux_abs_interaction': flux * abs_val,\n",
    "            'h_flux_interaction': h * flux\n",
    "        }\n",
    "        \n",
    "        # Add theoretical temperatures\n",
    "        for i, temp in enumerate(theoretical_temps):\n",
    "            features[f'Theoretical_Temps_{i+1}'] = temp\n",
    "        \n",
    "        # Create DataFrame and select features\n",
    "        input_data = pd.DataFrame([features])\n",
    "        feature_cols = scalers['feature_cols']\n",
    "        \n",
    "        # Ensure all required features exist\n",
    "        for col in feature_cols:\n",
    "            if col not in input_data.columns:\n",
    "                input_data[col] = 0.0\n",
    "        \n",
    "        return input_data[feature_cols].values\n",
    "    \n",
    "    def predict_single_model(self, time, h, flux, abs_val, surf, theoretical_temps, h_category):\n",
    "        \"\"\"Make prediction using a single h-specific model\"\"\"\n",
    "        try:\n",
    "            if h_category not in self.models:\n",
    "                raise ValueError(f\"Model for {h_category} not loaded. Available: {list(self.models.keys())}\")\n",
    "            \n",
    "            model = self.models[h_category]\n",
    "            scalers = self.scalers[h_category]\n",
    "            \n",
    "            # Prepare features\n",
    "            X_input = self._prepare_features(time, h, flux, abs_val, surf, theoretical_temps, h_category)\n",
    "            \n",
    "            # Scale and predict\n",
    "            X_scaled = scalers['X_scaler'].transform(X_input)\n",
    "            X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                predictions_scaled = model(X_tensor)  # Shape (1, output_size)\n",
    "                predictions_scaled = predictions_scaled.cpu().numpy()  # Shape (1, output_size)\n",
    "            \n",
    "            # Corrected inverse transform\n",
    "            predictions_temp = scalers['y_scaler'].inverse_transform(\n",
    "                predictions_scaled  # Already (1, output_size)\n",
    "            ).flatten()\n",
    "            \n",
    "            # Map to sensor names\n",
    "            tc_cols = scalers['tc_cols']\n",
    "            predicted_temperatures = {}\n",
    "            for i, sensor_name in enumerate(tc_cols):\n",
    "                if i < len(predictions_temp):\n",
    "                    predicted_temperatures[sensor_name] = predictions_temp[i]\n",
    "            \n",
    "            return predicted_temperatures\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Single model prediction failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def interpolate_h_predictions(self, time, target_h, flux, abs_val, surf, theoretical_temps, \n",
    "                                source_models=['h2', 'h6'], method='linear'):\n",
    "        \"\"\"\n",
    "        Predict for target h-value by interpolating between source models\n",
    "        \n",
    "        Args:\n",
    "            time: Time value\n",
    "            target_h: Target h-value to predict for\n",
    "            flux, abs_val, surf: Physical parameters\n",
    "            theoretical_temps: Theoretical temperatures (will be adjusted per model)\n",
    "            source_models: List of source models to use for interpolation\n",
    "            method: Interpolation method ('linear', 'weighted_average')\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with interpolated predictions\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ”„ Interpolating h={target_h} using models: {source_models}\")\n",
    "        \n",
    "        # Get predictions from source models\n",
    "        source_predictions = {}\n",
    "        source_h_values = {}\n",
    "        \n",
    "        for model_name in source_models:\n",
    "            if model_name not in self.models:\n",
    "                print(f\"âŒ Model {model_name} not available, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Get the actual h-value for this model\n",
    "            model_h = next(h for h, name in self.h_configs.items() if name == model_name)\n",
    "            source_h_values[model_name] = model_h\n",
    "            \n",
    "            # Adjust theoretical temps for this model\n",
    "            model_tc_count = self.tc_counts[model_name]\n",
    "            adjusted_temps = theoretical_temps[:model_tc_count] if len(theoretical_temps) >= model_tc_count else \\\n",
    "                            theoretical_temps + [theoretical_temps[-1]] * (model_tc_count - len(theoretical_temps))\n",
    "            \n",
    "            # Get prediction\n",
    "            pred = self.predict_single_model(time, model_h, flux, abs_val, surf, adjusted_temps, model_name)\n",
    "            if pred is not None:\n",
    "                source_predictions[model_name] = pred\n",
    "                print(f\"âœ“ Got prediction from {model_name} (h={model_h})\")\n",
    "            else:\n",
    "                print(f\"âŒ Failed to get prediction from {model_name}\")\n",
    "        \n",
    "        if len(source_predictions) < 2:\n",
    "            print(f\"âŒ Need at least 2 source predictions, got {len(source_predictions)}\")\n",
    "            return None\n",
    "        \n",
    "        # Perform interpolation\n",
    "        if method == 'linear':\n",
    "            return self._linear_interpolation(target_h, source_predictions, source_h_values)\n",
    "        elif method == 'weighted_average':\n",
    "            return self._weighted_average_interpolation(target_h, source_predictions, source_h_values)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown interpolation method: {method}\")\n",
    "    \n",
    "    def _linear_interpolation(self, target_h, source_predictions, source_h_values):\n",
    "        \"\"\"Linear interpolation between two models\"\"\"\n",
    "        model_names = list(source_predictions.keys())\n",
    "        \n",
    "        if len(model_names) == 2:\n",
    "            model1, model2 = model_names\n",
    "            h1, h2 = source_h_values[model1], source_h_values[model2]\n",
    "            \n",
    "            # Ensure h1 < h2\n",
    "            if h1 > h2:\n",
    "                model1, model2 = model2, model1\n",
    "                h1, h2 = h2, h1\n",
    "            \n",
    "            # Linear interpolation weight\n",
    "            if h2 != h1:  # Avoid division by zero\n",
    "                weight = (target_h - h1) / (h2 - h1)\n",
    "                weight = max(0, min(1, weight))  # Clamp to [0, 1]\n",
    "            else:\n",
    "                weight = 0.5\n",
    "            \n",
    "            print(f\"ðŸ“Š Linear interpolation: {model1}({h1}) --[{weight:.3f}]-> {model2}({h2})\")\n",
    "            \n",
    "            # Interpolate for common sensors\n",
    "            interpolated_temps = {}\n",
    "            pred1 = source_predictions[model1]\n",
    "            pred2 = source_predictions[model2]\n",
    "            \n",
    "            for sensor in self.common_sensors:\n",
    "                if sensor in pred1 and sensor in pred2:\n",
    "                    interpolated_temps[sensor] = (1 - weight) * pred1[sensor] + weight * pred2[sensor]\n",
    "            \n",
    "            return {\n",
    "                'predicted_temperatures': interpolated_temps,\n",
    "                'interpolation_info': {\n",
    "                    'method': 'linear',\n",
    "                    'source_models': model_names,\n",
    "                    'source_h_values': [h1, h2],\n",
    "                    'target_h': target_h,\n",
    "                    'weight': weight,\n",
    "                    'common_sensors': list(interpolated_temps.keys())\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            # For more than 2 models, use weighted average\n",
    "            return self._weighted_average_interpolation(target_h, source_predictions, source_h_values)\n",
    "    \n",
    "    def _weighted_average_interpolation(self, target_h, source_predictions, source_h_values):\n",
    "        \"\"\"Weighted average interpolation using inverse distance weighting\"\"\"\n",
    "        weights = {}\n",
    "        total_weight = 0\n",
    "        \n",
    "        # Calculate weights based on inverse distance\n",
    "        for model_name, h_val in source_h_values.items():\n",
    "            distance = abs(target_h - h_val)\n",
    "            if distance == 0:\n",
    "                # Exact match, give full weight\n",
    "                weights = {model_name: 1.0}\n",
    "                total_weight = 1.0\n",
    "                break\n",
    "            else:\n",
    "                weight = 1.0 / (distance ** 2)  # Inverse square distance\n",
    "                weights[model_name] = weight\n",
    "                total_weight += weight\n",
    "        \n",
    "        # Normalize weights\n",
    "        if total_weight > 0:\n",
    "            for model_name in weights:\n",
    "                weights[model_name] /= total_weight\n",
    "        \n",
    "        print(f\"ðŸ“Š Weighted interpolation weights: {weights}\")\n",
    "        \n",
    "        # Weighted interpolation for common sensors\n",
    "        interpolated_temps = {}\n",
    "        \n",
    "        for sensor in self.common_sensors:\n",
    "            weighted_temp = 0\n",
    "            valid_predictions = 0\n",
    "            \n",
    "            for model_name, weight in weights.items():\n",
    "                if model_name in source_predictions and sensor in source_predictions[model_name]:\n",
    "                    weighted_temp += weight * source_predictions[model_name][sensor]\n",
    "                    valid_predictions += 1\n",
    "            \n",
    "            if valid_predictions > 0:\n",
    "                interpolated_temps[sensor] = weighted_temp\n",
    "        \n",
    "        return {\n",
    "            'predicted_temperatures': interpolated_temps,\n",
    "            'interpolation_info': {\n",
    "                'method': 'weighted_average',\n",
    "                'source_models': list(weights.keys()),\n",
    "                'source_h_values': list(source_h_values.values()),\n",
    "                'target_h': target_h,\n",
    "                'weights': weights,\n",
    "                'common_sensors': list(interpolated_temps.keys())\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def predict_h3_from_h2_h6(self, time, flux, abs_val, surf, theoretical_temps, method='linear'):\n",
    "        \"\"\"\n",
    "        Convenience function to predict h3 using h2 and h6 models\n",
    "        \n",
    "        Args:\n",
    "            time: Time in seconds\n",
    "            flux: Heat flux\n",
    "            abs_val: Absorption coefficient\n",
    "            surf: Surface emissivity  \n",
    "            theoretical_temps: List of theoretical temperatures (will be adjusted per model)\n",
    "            method: Interpolation method\n",
    "        \n",
    "        Returns:\n",
    "            Interpolated predictions for h3\n",
    "        \"\"\"\n",
    "        target_h = 0.084  # h3 value\n",
    "        return self.interpolate_h_predictions(\n",
    "            time, target_h, flux, abs_val, surf, theoretical_temps,\n",
    "            source_models=['h2', 'h6'], method=method\n",
    "        )\n",
    "    \n",
    "    def compare_predictions(self, time, flux, abs_val, surf, theoretical_temps):\n",
    "        \"\"\"\n",
    "        Compare direct h3 prediction vs interpolated h3 prediction\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with comparison results\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ” PREDICTION COMPARISON\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Direct h3 prediction\n",
    "        if 'h3' in self.models:\n",
    "            print(\"ðŸ“Š Direct h3 prediction:\")\n",
    "            h3_direct = self.predict_single_model(\n",
    "                time, 0.084, flux, abs_val, surf, theoretical_temps[:5], 'h3'\n",
    "            )\n",
    "            results['direct_h3'] = h3_direct\n",
    "            \n",
    "            if h3_direct:\n",
    "                for sensor, temp in h3_direct.items():\n",
    "                    if sensor in self.common_sensors:\n",
    "                        print(f\"  {sensor}: {temp:.3f} Â°C\")\n",
    "        else:\n",
    "            print(\"âŒ No direct h3 model available\")\n",
    "            results['direct_h3'] = None\n",
    "        \n",
    "        # Interpolated h3 prediction\n",
    "        print(\"\\nðŸ“Š Interpolated h3 prediction (from h2 & h6):\")\n",
    "        h3_interpolated = self.predict_h3_from_h2_h6(time, flux, abs_val, surf, theoretical_temps)\n",
    "        results['interpolated_h3'] = h3_interpolated\n",
    "        \n",
    "        if h3_interpolated and 'predicted_temperatures' in h3_interpolated:\n",
    "            for sensor, temp in h3_interpolated['predicted_temperatures'].items():\n",
    "                print(f\"  {sensor}: {temp:.3f} Â°C\")\n",
    "        \n",
    "        # Calculate differences if both exist\n",
    "        if results['direct_h3'] and results['interpolated_h3']:\n",
    "            print(\"\\nðŸ“Š Differences (Direct - Interpolated):\")\n",
    "            differences = {}\n",
    "            \n",
    "            direct_temps = results['direct_h3']\n",
    "            interp_temps = results['interpolated_h3']['predicted_temperatures']\n",
    "            \n",
    "            for sensor in self.common_sensors:\n",
    "                if sensor in direct_temps and sensor in interp_temps:\n",
    "                    diff = direct_temps[sensor] - interp_temps[sensor]\n",
    "                    differences[sensor] = diff\n",
    "                    print(f\"  {sensor}: {diff:+.3f} Â°C\")\n",
    "            \n",
    "            results['differences'] = differences\n",
    "            \n",
    "            # Calculate RMSE\n",
    "            if differences:\n",
    "                rmse = np.sqrt(np.mean([diff**2 for diff in differences.values()]))\n",
    "                print(f\"\\nðŸ“Š RMSE between direct and interpolated: {rmse:.3f} Â°C\")\n",
    "                results['rmse'] = rmse\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Global predictor instance\n",
    "global_predictor = None\n",
    "\n",
    "def load_interpolation_models():\n",
    "    \"\"\"Load models for interpolation\"\"\"\n",
    "    global global_predictor\n",
    "    global_predictor = HInterpolationPredictor()\n",
    "    success = global_predictor.load_models()\n",
    "    \n",
    "    if success:\n",
    "        print(\"âœ“ Interpolation models loaded successfully!\")\n",
    "        print(f\"Available models: {list(global_predictor.models.keys())}\")\n",
    "        return global_predictor\n",
    "    else:\n",
    "        print(\"âœ— Failed to load interpolation models\")\n",
    "        return None\n",
    "\n",
    "def predict_h3_interpolated(time, flux, abs_val, surf, theoretical_temps, method='linear'):\n",
    "    \"\"\"\n",
    "    Main function to predict h3 using h2 and h6 models\n",
    "    \n",
    "    Args:\n",
    "        time: Time value in seconds\n",
    "        flux: Heat flux\n",
    "        abs_val: Absorption coefficient\n",
    "        surf: Surface emissivity\n",
    "        theoretical_temps: List of theoretical temperatures\n",
    "        method: 'linear' or 'weighted_average'\n",
    "    \n",
    "    Returns:\n",
    "        Interpolated h3 predictions\n",
    "    \"\"\"\n",
    "    global global_predictor\n",
    "    \n",
    "    if global_predictor is None:\n",
    "        print(\"Loading interpolation models...\")\n",
    "        global_predictor = load_interpolation_models()\n",
    "        if global_predictor is None:\n",
    "            return None\n",
    "    \n",
    "    return global_predictor.predict_h3_from_h2_h6(time, flux, abs_val, surf, theoretical_temps, method)\n",
    "\n",
    "def run_interpolation_example():\n",
    "    \"\"\"Run a complete example of h3 interpolation\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"H3 INTERPOLATION EXAMPLE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load models\n",
    "    predictor = load_interpolation_models()\n",
    "    if not predictor:\n",
    "        return\n",
    "    \n",
    "    # Example parameters\n",
    "    time = 1800  # 30 minutes\n",
    "    flux = 21250\n",
    "    abs_val = 3\n",
    "    surf = 0.98\n",
    "    theoretical_temps = [25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0]\n",
    "    \n",
    "    # Compare predictions\n",
    "    comparison = predictor.compare_predictions(time, flux, abs_val, surf, theoretical_temps)\n",
    "    \n",
    "    # Try different interpolation methods\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"DIFFERENT INTERPOLATION METHODS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    methods = ['linear', 'weighted_average']\n",
    "    for method in methods:\n",
    "        print(f\"\\nðŸ“Š Method: {method}\")\n",
    "        result = predictor.predict_h3_from_h2_h6(time, flux, abs_val, surf, theoretical_temps, method)\n",
    "        if result and 'predicted_temperatures' in result:\n",
    "            for sensor, temp in result['predicted_temperatures'].items():\n",
    "                print(f\"  {sensor}: {temp:.3f} Â°C\")\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# Example usage and testing functions\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the interpolation example\n",
    "    run_interpolation_example()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BATCH INTERPOLATION TEST\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test with multiple time points\n",
    "    predictor = load_interpolation_models()\n",
    "    if predictor:\n",
    "        time_points = [600, 1200, 1800, 2400, 3600]  # Different time points\n",
    "        theoretical_temps = [25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0]\n",
    "        \n",
    "        print(\"Time (s) | TC1_tip | TC2    | TC3    | Method\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for time in time_points:\n",
    "            result = predict_h3_interpolated(\n",
    "                time=time,\n",
    "                flux=21250,\n",
    "                abs_val=3,\n",
    "                surf=0.98,\n",
    "                theoretical_temps=theoretical_temps,\n",
    "                method='linear'\n",
    "            )\n",
    "            \n",
    "            if result and 'predicted_temperatures' in result:\n",
    "                temps = result['predicted_temperatures']\n",
    "                print(f\"{time:8d} | {temps.get('TC1_tip', 0):.3f}  | {temps.get('TC2', 0):.3f} | {temps.get('TC3', 0):.3f} | Linear\")\n",
    "\n",
    "def validate_interpolation_accuracy(predictor, test_points=10):\n",
    "    \"\"\"\n",
    "    Validate interpolation accuracy by testing on known h3 data\n",
    "    (if h3 model is available)\n",
    "    \"\"\"\n",
    "    if 'h3' not in predictor.models:\n",
    "        print(\"âŒ No h3 model available for validation\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"INTERPOLATION ACCURACY VALIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Generate test scenarios\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    \n",
    "    validation_results = []\n",
    "    total_error = 0\n",
    "    valid_comparisons = 0\n",
    "    \n",
    "    for i in range(test_points):\n",
    "        # Random test parameters\n",
    "        time = np.random.uniform(300, 6000)  # 5min to 100min\n",
    "        flux = np.random.uniform(15000, 25000)\n",
    "        abs_val = np.random.uniform(2, 4)\n",
    "        surf = np.random.uniform(0.95, 0.99)\n",
    "        \n",
    "        # Random theoretical temperatures (increasing with depth)\n",
    "        base_temp = np.random.uniform(20, 30)\n",
    "        theoretical_temps = [base_temp + i*5 + np.random.normal(0, 1) for i in range(10)]\n",
    "        \n",
    "        # Get direct h3 prediction\n",
    "        direct_pred = predictor.predict_single_model(\n",
    "            time, 0.084, flux, abs_val, surf, theoretical_temps[:5], 'h3'\n",
    "        )\n",
    "        \n",
    "        # Get interpolated prediction\n",
    "        interp_result = predictor.predict_h3_from_h2_h6(\n",
    "            time, flux, abs_val, surf, theoretical_temps, method='linear'\n",
    "        )\n",
    "        \n",
    "        if direct_pred and interp_result and 'predicted_temperatures' in interp_result:\n",
    "            interp_pred = interp_result['predicted_temperatures']\n",
    "            \n",
    "            # Calculate errors for common sensors\n",
    "            test_result = {\n",
    "                'test_id': i+1,\n",
    "                'time': time,\n",
    "                'direct': direct_pred,\n",
    "                'interpolated': interp_pred,\n",
    "                'errors': {}\n",
    "            }\n",
    "            \n",
    "            for sensor in predictor.common_sensors:\n",
    "                if sensor in direct_pred and sensor in interp_pred:\n",
    "                    error = abs(direct_pred[sensor] - interp_pred[sensor])\n",
    "                    test_result['errors'][sensor] = error\n",
    "                    total_error += error\n",
    "                    valid_comparisons += 1\n",
    "            \n",
    "            validation_results.append(test_result)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    if valid_comparisons > 0:\n",
    "        mean_absolute_error = total_error / valid_comparisons\n",
    "        \n",
    "        print(f\"ðŸ“Š Validation Results (n={test_points}):\")\n",
    "        print(f\"  Mean Absolute Error: {mean_absolute_error:.3f} Â°C\")\n",
    "        print(f\"  Valid Comparisons: {valid_comparisons}\")\n",
    "        \n",
    "        # Show detailed results for first few tests\n",
    "        print(f\"\\nðŸ“‹ Detailed Results (first 5 tests):\")\n",
    "        print(\"Test | Sensor  | Direct | Interp | Error\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        for i, result in enumerate(validation_results[:5]):\n",
    "            for sensor, error in result['errors'].items():\n",
    "                direct_temp = result['direct'][sensor]\n",
    "                interp_temp = result['interpolated'][sensor]\n",
    "                print(f\"{i+1:4d} | {sensor:7s} | {direct_temp:6.2f} | {interp_temp:6.2f} | {error:5.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'mean_absolute_error': mean_absolute_error,\n",
    "            'validation_results': validation_results,\n",
    "            'valid_comparisons': valid_comparisons\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        print(\"âŒ No valid comparisons could be made\")\n",
    "        return None\n",
    "\n",
    "def plot_interpolation_comparison(predictor, save_plot=False):\n",
    "    \"\"\"\n",
    "    Create a plot comparing direct vs interpolated predictions over time\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'h3' not in predictor.models:\n",
    "            print(\"âŒ No h3 model available for plotting\")\n",
    "            return\n",
    "        \n",
    "        # Time series\n",
    "        time_points = np.linspace(300, 6000, 20)  # 5min to 100min\n",
    "        \n",
    "        # Fixed parameters for comparison\n",
    "        flux = 21250\n",
    "        abs_val = 3\n",
    "        surf = 0.98\n",
    "        theoretical_temps = [25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0]\n",
    "        \n",
    "        # Collect predictions\n",
    "        results = {'time': [], 'direct': {}, 'interpolated': {}}\n",
    "        \n",
    "        for sensor in predictor.common_sensors:\n",
    "            results['direct'][sensor] = []\n",
    "            results['interpolated'][sensor] = []\n",
    "        \n",
    "        for time in time_points:\n",
    "            results['time'].append(time/60)  # Convert to minutes\n",
    "            \n",
    "            # Direct prediction\n",
    "            direct_pred = predictor.predict_single_model(\n",
    "                time, 0.084, flux, abs_val, surf, theoretical_temps[:5], 'h3'\n",
    "            )\n",
    "            \n",
    "            # Interpolated prediction\n",
    "            interp_result = predictor.predict_h3_from_h2_h6(\n",
    "                time, flux, abs_val, surf, theoretical_temps, method='linear'\n",
    "            )\n",
    "            \n",
    "            for sensor in predictor.common_sensors:\n",
    "                if direct_pred and sensor in direct_pred:\n",
    "                    results['direct'][sensor].append(direct_pred[sensor])\n",
    "                else:\n",
    "                    results['direct'][sensor].append(np.nan)\n",
    "                \n",
    "                if (interp_result and 'predicted_temperatures' in interp_result and \n",
    "                    sensor in interp_result['predicted_temperatures']):\n",
    "                    results['interpolated'][sensor].append(interp_result['predicted_temperatures'][sensor])\n",
    "                else:\n",
    "                    results['interpolated'][sensor].append(np.nan)\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        colors = ['blue', 'red', 'green']\n",
    "        \n",
    "        for i, sensor in enumerate(predictor.common_sensors):\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            \n",
    "            # Plot direct predictions\n",
    "            plt.plot(results['time'], results['direct'][sensor], \n",
    "                    'o-', color=colors[i], label='Direct H3', linewidth=2, markersize=6)\n",
    "            \n",
    "            # Plot interpolated predictions\n",
    "            plt.plot(results['time'], results['interpolated'][sensor], \n",
    "                    's--', color=colors[i], alpha=0.7, label='Interpolated (H2+H6)', linewidth=2, markersize=4)\n",
    "            \n",
    "            plt.title(f'{sensor} Temperature Prediction')\n",
    "            plt.xlabel('Time (minutes)')\n",
    "            plt.ylabel('Temperature (Â°C)')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Overall comparison plot\n",
    "        plt.subplot(2, 2, 4)\n",
    "        for i, sensor in enumerate(predictor.common_sensors):\n",
    "            direct_vals = np.array(results['direct'][sensor])\n",
    "            interp_vals = np.array(results['interpolated'][sensor])\n",
    "            \n",
    "            # Remove NaN values for correlation\n",
    "            mask = ~(np.isnan(direct_vals) | np.isnan(interp_vals))\n",
    "            if np.sum(mask) > 0:\n",
    "                plt.scatter(direct_vals[mask], interp_vals[mask], \n",
    "                           color=colors[i], alpha=0.7, label=sensor, s=50)\n",
    "        \n",
    "        # Perfect prediction line\n",
    "        all_temps = []\n",
    "        for sensor in predictor.common_sensors:\n",
    "            all_temps.extend([t for t in results['direct'][sensor] if not np.isnan(t)])\n",
    "            all_temps.extend([t for t in results['interpolated'][sensor] if not np.isnan(t)])\n",
    "        \n",
    "        if all_temps:\n",
    "            temp_range = [min(all_temps), max(all_temps)]\n",
    "            plt.plot(temp_range, temp_range, 'k--', alpha=0.5, label='Perfect Prediction')\n",
    "        \n",
    "        plt.xlabel('Direct H3 Prediction (Â°C)')\n",
    "        plt.ylabel('Interpolated Prediction (Â°C)')\n",
    "        plt.title('Direct vs Interpolated Correlation')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_plot:\n",
    "            plt.savefig('h3_interpolation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "            print(\"ðŸ“Š Plot saved as 'h3_interpolation_comparison.png'\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"âŒ Matplotlib not available for plotting\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating plot: {e}\")\n",
    "\n",
    "# Advanced interpolation methods\n",
    "class AdvancedInterpolator:\n",
    "    \"\"\"Advanced interpolation techniques for neural network predictions\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def polynomial_interpolation(h_values, predictions, target_h, degree=2):\n",
    "        \"\"\"\n",
    "        Polynomial interpolation of predictions\n",
    "        \n",
    "        Args:\n",
    "            h_values: List of h-values\n",
    "            predictions: Dict of predictions for each h-value  \n",
    "            target_h: Target h-value\n",
    "            degree: Polynomial degree\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from scipy.interpolate import lagrange\n",
    "            \n",
    "            interpolated = {}\n",
    "            \n",
    "            # Get common sensors\n",
    "            common_sensors = set(predictions[list(predictions.keys())[0]].keys())\n",
    "            for pred in predictions.values():\n",
    "                common_sensors = common_sensors.intersection(set(pred.keys()))\n",
    "            \n",
    "            for sensor in common_sensors:\n",
    "                # Extract temperature values for this sensor\n",
    "                temps = [predictions[h][sensor] for h in h_values]\n",
    "                \n",
    "                # Create polynomial interpolator\n",
    "                poly = lagrange(h_values, temps)\n",
    "                interpolated[sensor] = float(poly(target_h))\n",
    "            \n",
    "            return interpolated\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"âŒ SciPy not available for polynomial interpolation\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Polynomial interpolation failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def spline_interpolation(h_values, predictions, target_h):\n",
    "        \"\"\"\n",
    "        Spline interpolation of predictions\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from scipy.interpolate import CubicSpline\n",
    "            \n",
    "            interpolated = {}\n",
    "            \n",
    "            # Get common sensors\n",
    "            common_sensors = set(predictions[list(predictions.keys())[0]].keys())\n",
    "            for pred in predictions.values():\n",
    "                common_sensors = common_sensors.intersection(set(pred.keys()))\n",
    "            \n",
    "            for sensor in common_sensors:\n",
    "                # Extract temperature values for this sensor\n",
    "                temps = [predictions[h][sensor] for h in h_values]\n",
    "                \n",
    "                # Create spline interpolator\n",
    "                if len(h_values) >= 3:  # Need at least 3 points for cubic spline\n",
    "                    spline = CubicSpline(h_values, temps)\n",
    "                    interpolated[sensor] = float(spline(target_h))\n",
    "                else:\n",
    "                    # Fall back to linear interpolation\n",
    "                    from scipy.interpolate import interp1d\n",
    "                    linear_interp = interp1d(h_values, temps, kind='linear')\n",
    "                    interpolated[sensor] = float(linear_interp(target_h))\n",
    "            \n",
    "            return interpolated\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"âŒ SciPy not available for spline interpolation\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Spline interpolation failed: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bb0083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction components loaded successfully!\n",
      "\n",
      "Predicted Temperatures:\n",
      "TC1_tip: 361.058 Â°C\n",
      "TC2: 359.502 Â°C\n",
      "TC3: 360.845 Â°C\n",
      "TC4: 360.734 Â°C\n",
      "TC5: 360.469 Â°C\n",
      "TC6: 362.078 Â°C\n",
      "TC7: 360.542 Â°C\n",
      "TC8: 361.206 Â°C\n",
      "TC9: 358.724 Â°C\n",
      "TC10: 338.315 Â°C\n",
      "[361.057861328125, 359.50201416015625, 360.8453063964844, 360.733642578125, 360.468505859375, 362.0776062011719, 360.5418395996094, 361.2062683105469, 358.7239990234375, 338.3153076171875]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Block 10: Standalone Prediction Functionality\n",
    "def load_for_prediction():\n",
    "    \"\"\"Load all components needed for prediction\"\"\"\n",
    "    try:\n",
    "        # Load scalers and metadata\n",
    "        X_scaler = joblib.load(\"X_scaler_enhanced.pkl\")\n",
    "        y_scaler = joblib.load(\"y_scaler_enhanced.pkl\")\n",
    "        time_range_data = joblib.load(\"time_range_enhanced.pkl\")\n",
    "        column_info = joblib.load(\"column_info.pkl\")\n",
    "        \n",
    "        # Set device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load model\n",
    "        model = EnhancedThermalNet(\n",
    "            input_size=len(column_info['feature_cols']),\n",
    "            output_size=len(column_info['tc_cols']),\n",
    "            hidden_dims=[512, 256, 256, 128],\n",
    "            dropout_rate=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        checkpoint = torch.load('best_thermal_model_enhanced.pth', map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"Prediction components loaded successfully!\")\n",
    "        return model, X_scaler, y_scaler, time_range_data, column_info, device\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prediction components: {e}\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "def make_prediction(time, h, flux, abs_val, surf, theoretical_temps):\n",
    "    \"\"\"\n",
    "    Make a temperature prediction with given parameters\n",
    "    \n",
    "    Args:\n",
    "        time: Time value in seconds\n",
    "        h: Heat transfer coefficient (0.0375, 0.084, or 0.1575)\n",
    "        flux: Heat flux (19400, 21250, or 25900)\n",
    "        abs_val: Absorption coefficient (3 or 100)\n",
    "        surf: Surface emissivity (0.76 or 0.98)\n",
    "        theoretical_temps: List of 10 theoretical temperatures\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with predicted temperatures for each TC sensor\n",
    "    \"\"\"\n",
    "    # Load components if not already loaded\n",
    "    if not all(var in globals() for var in ['pred_model', 'pred_X_scaler', 'pred_y_scaler', \n",
    "                                           'pred_time_range', 'pred_column_info', 'pred_device']):\n",
    "        global pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device\n",
    "        pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device = load_for_prediction()\n",
    "        if pred_model is None:\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        # Validate inputs\n",
    "        if len(theoretical_temps) != 10:\n",
    "            raise ValueError(\"Exactly 10 theoretical temperatures required\")\n",
    "        \n",
    "        if h not in [0.0375, 0.084, 0.1575]:\n",
    "            print(f\"Warning: h value {h} not in expected values [0.0375, 0.084, 0.1575]\")\n",
    "            \n",
    "        if flux not in [19400, 21250, 25900]:\n",
    "            print(f\"Warning: flux value {flux} not in expected values [19400, 21250, 25900]\")\n",
    "            \n",
    "        if abs_val not in [3, 100]:\n",
    "            print(f\"Warning: abs value {abs_val} not in expected values [3, 100]\")\n",
    "            \n",
    "        if surf not in [0.76, 0.98]:\n",
    "            print(f\"Warning: surf value {surf} not in expected values [0.76, 0.98]\")\n",
    "        \n",
    "        # Make prediction\n",
    "        result = predict_temperature(\n",
    "            pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info,\n",
    "            time, h, flux, abs_val, surf, theoretical_temps, pred_device\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Prediction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (run this block after training once to load components)\n",
    "pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device = load_for_prediction()\n",
    "\n",
    "# Now you can make predictions anytime using:\n",
    "prediction = make_prediction(\n",
    "    time=44,\n",
    "    h=0.1575,\n",
    "    flux=21250,\n",
    "    abs_val=3,\n",
    "    surf=0.98,\n",
    "    theoretical_temps=[303.991791613348,324.34732303606,324.34732303606,324.34732303606,324.34732303606,324.34732303606,324.34732303606,324.34732303606,324.34732303606,324.34732303606]\n",
    "    \n",
    ")\n",
    "temp_arr=[]\n",
    "if prediction:\n",
    "    print(\"\\nPredicted Temperatures:\")\n",
    "    for tc_name, temp in prediction['predicted_temperatures'].items():\n",
    "        print(f\"{tc_name}: {temp:.3f} Â°C\")\n",
    "        temp_arr.append(temp)\n",
    "\n",
    "print(temp_arr)\n",
    "\n",
    "# Test batch inference\n",
    "        # print(\"\\n3. Testing batch prediction...\")\n",
    "        # try:\n",
    "        #     if model_inf is not None:\n",
    "        #         # Create batch of test data\n",
    "        #         batch_data = [\n",
    "        #             {\n",
    "        #                 'time': 1800,  # 30 minutes\n",
    "        #                 'h': 0.1575,\n",
    "        #                 'flux': 25900,\n",
    "        #                 'abs': 100,\n",
    "        #                 'surf': 0.98,\n",
    "        #                 'theoretical_temps': [25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0]\n",
    "        #             },\n",
    "        #             {\n",
    "        #                 'time': 3600,  # 60 minutes\n",
    "        #                 'h': 0.1575,\n",
    "        #                 'flux': 19400,\n",
    "        #                 'abs': 3,\n",
    "        #                 'surf': 0.76,\n",
    "        #                 'theoretical_temps': [28.0, 32.5, 37.2, 41.8, 46.5, 51.1, 55.8, 60.4, 65.1, 69.7]\n",
    "        #             }\n",
    "        #         ]\n",
    "                \n",
    "        #         batch_results = batch_predict_temperature(\n",
    "        #             model_inf, X_scaler_inf, y_scaler_inf, time_range_inf, column_info_inf,\n",
    "        #             batch_data, device_inf\n",
    "        #         )\n",
    "                \n",
    "        #         print(f\"Batch prediction completed for {len(batch_results)} samples\")\n",
    "                \n",
    "        #         for i, result in enumerate(batch_results):\n",
    "        #             if result:\n",
    "        #                 print(f\"\\nBatch Sample {i+1}:\")\n",
    "        #                 print(f\"  Time: {result['input_parameters']['time']/60:.1f} min\")\n",
    "        #                 print(f\"  Flux: {result['input_parameters']['flux']}\")\n",
    "        #                 print(f\"  Average predicted temp: {np.mean(list(result['predicted_temperatures'].values())):.2f} Â°C\")\n",
    "                \n",
    "        #         print(\"âœ“ Batch prediction test successful!\")\n",
    "                \n",
    "        #     else:\n",
    "        #         print(\"âœ— Batch prediction test failed - model not loaded\")\n",
    "                \n",
    "        # except Exception as e:\n",
    "        #     print(f\"âœ— Batch prediction test failed: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc8430",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def plot_vertical_profile(predicted, actual, filename=\"Sample Profile\"):\n",
    "    # Reverse order so TC10 (surface) is at the top\n",
    "    predicted = predicted[::-1]\n",
    "    actual = actual[::-1]\n",
    "    sensor_labels = [f\"TC{i}\" for i in range(10, 0, -1)]  # TC10 to TC1\n",
    "\n",
    "    total_height = 0.1575  # Total receiver height in meters\n",
    "    spacing = total_height / 9\n",
    "    depths = [0 - i * spacing for i in range(10)]  # TC10 at 0.0, TC1 at -total_height\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.plot(actual, depths, label=\"Actual\", color='blue', marker='o', linewidth=2)\n",
    "    plt.plot(predicted, depths, label=\"Predicted\", color='red', marker='x', linestyle='--', linewidth=2)\n",
    "\n",
    "    plt.gca().invert_yaxis()  # So 0 (surface) is at top\n",
    "\n",
    "    # Set clean numeric y-ticks\n",
    "    plt.yticks(depths, [f\"{d:.3f}\" for d in depths])\n",
    "    plt.ylim(min(depths) - spacing * 0.5, max(depths) + spacing * 0.5)\n",
    "\n",
    "    plt.xlabel(\"Temperature (Â°C)\")\n",
    "    plt.ylabel(\"Depth (m)\")\n",
    "    plt.title(f\"Vertical Profile: {filename}\")\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Add sensor labels next to each point\n",
    "    for i, label in enumerate(sensor_labels):\n",
    "        plt.text(\n",
    "            actual[i], depths[i], label,\n",
    "            ha='right', va='center', fontsize=8,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#cce6ff\", ec=\"navy\", lw=1, alpha=0.7)\n",
    "        )\n",
    "        plt.text(\n",
    "            predicted[i], depths[i], label,\n",
    "            ha='left', va='center', fontsize=8,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#ffcccc\", ec=\"darkred\", lw=1, alpha=0.7)\n",
    "        )\n",
    "\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{filename}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# actual = [361.36,360.52,360.93,362.1,361.66,363.36,361.29,361.44,360.32,337.33]\n",
    "# pred = [361.057861328125, 359.50201416015625, 360.8453063964844, 360.733642578125, 360.468505859375, 362.0776062011719, 360.5418395996094, 361.2062683105469, 358.7239990234375, 338.3153076171875]\n",
    "\n",
    "# plot_vertical_profile(pred, actual, filename=\"h6_flux78_abs0_surf0_newSalt_641s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164e4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data directory: data/new_processed_reset\n",
      "\n",
      "=== Running Standalone Test Cross-Check ===\n",
      "Loading data from: data/new_processed_reset\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_699s - Sheet1_processed.csv (717 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv (532 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv (4621 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv (566 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv (621 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_630s - Sheet2_processed.csv (618 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv (516 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_675s - Sheet3_processed.csv (856 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv (826 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv (795 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv (932 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_585s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_598s - Sheet1_processed.csv (528 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_surf0_439s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv (912 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_719s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv (1087 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_644s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv (790 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_749s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_429s - Sheet2_processed.csv (880 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv (585 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv (4247 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv (611 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv (554 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_surf0_660s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_692s-Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv (589 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_431s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv (970 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv (803 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv (1411 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_830s - Sheet2_processed.csv (528 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv (787 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_surf0_493s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv (531 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_491s - Sheet3_processed.csv (702 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv (547 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_surf0_115s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv (3952 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_651s - Sheet4_processed.csv (554 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv (865 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv (870 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv (550 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_628s - Sheet4_processed.csv (539 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_389s - Sheet3_processed.csv (537 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_389s - Sheet4_processed.csv (2139 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv (887 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_825s-Sheer3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv (790 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_613s - Sheet1_processed.csv (550 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv (794 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_747s-Sheet3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv (4087 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv (849 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv (534 rows)\n",
      "\n",
      "Total data loaded: 48187 rows, 26 columns\n",
      "Starting data preprocessing...\n",
      "Features: 21, Targets: 10\n",
      "Theory columns: 10, TC columns: 10\n",
      "Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10']\n",
      "After removing missing values: 48187 samples\n",
      "Training samples: 33730\n",
      "Validation samples: 7228\n",
      "Test samples: 7229\n",
      "\n",
      "=== Cross-Checking 20 Test Samples ===\n",
      "\n",
      "Test Sample 5329:\n",
      "Inputs: Time=1631.00s, h=0.1575, flux=25900, abs=20, surf=0.98\n",
      "Theoretical Temps: ['400.89', '433.32', '433.32', '433.30', '420.60', '398.60', '380.57', '374.79', '374.79', '374.79']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         354.513         354.420         0.093          \n",
      "TC2             356.382         356.340         0.042          \n",
      "TC3             364.416         364.160         0.256          \n",
      "TC4             372.895         372.510         0.385          \n",
      "TC5             377.492         377.270         0.222          \n",
      "TC6             384.512         384.030         0.482          \n",
      "TC7             385.562         385.110         0.452          \n",
      "TC8             387.721         386.960         0.761          \n",
      "TC9             389.416         388.650         0.766          \n",
      "TC10            377.838         377.090         0.748          \n",
      "\n",
      "Test Sample 6382:\n",
      "Inputs: Time=315.00s, h=0.1575, flux=25900, abs=20, surf=0.98\n",
      "Theoretical Temps: ['348.50', '373.94', '373.94', '371.97', '358.60', '344.10', '333.61', '330.17', '330.17', '330.17']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         354.536         353.820         0.716          \n",
      "TC2             354.537         354.700         0.163          \n",
      "TC3             358.763         359.400         0.637          \n",
      "TC4             361.627         362.290         0.663          \n",
      "TC5             363.146         363.620         0.474          \n",
      "TC6             366.982         367.320         0.338          \n",
      "TC7             367.030         366.920         0.110          \n",
      "TC8             368.754         368.520         0.234          \n",
      "TC9             369.947         370.220         0.273          \n",
      "TC10            357.335         358.750         1.415          \n",
      "\n",
      "Test Sample 3900:\n",
      "Inputs: Time=734.00s, h=0.1575, flux=25900, abs=3, surf=0.76\n",
      "Theoretical Temps: ['350.56', '371.79', '371.79', '371.79', '371.79', '371.79', '371.79', '371.79', '371.79', '371.79']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         365.407         365.500         0.093          \n",
      "TC2             363.651         363.720         0.069          \n",
      "TC3             366.195         366.050         0.145          \n",
      "TC4             366.379         366.340         0.039          \n",
      "TC5             366.563         366.170         0.393          \n",
      "TC6             368.545         368.400         0.145          \n",
      "TC7             368.067         367.670         0.397          \n",
      "TC8             369.058         368.670         0.388          \n",
      "TC9             368.679         368.810         0.131          \n",
      "TC10            357.300         355.240         2.060          \n",
      "\n",
      "Test Sample 1027:\n",
      "Inputs: Time=294.00s, h=0.1575, flux=25900, abs=100, surf=0.98\n",
      "Theoretical Temps: ['432.94', '447.33', '360.64', '316.88', '303.87', '300.82', '300.17', '300.04', '300.01', '300.00']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         349.085         349.380         0.295          \n",
      "TC2             349.955         350.640         0.685          \n",
      "TC3             353.788         354.150         0.362          \n",
      "TC4             354.871         355.890         1.019          \n",
      "TC5             355.761         355.770         0.009          \n",
      "TC6             358.365         358.410         0.045          \n",
      "TC7             357.724         356.950         0.774          \n",
      "TC8             360.795         359.970         0.825          \n",
      "TC9             381.318         380.420         0.898          \n",
      "TC10            383.946         381.930         2.016          \n",
      "\n",
      "Test Sample 3516:\n",
      "Inputs: Time=355.00s, h=0.1575, flux=25900, abs=20, surf=0.76\n",
      "Theoretical Temps: ['366.53', '389.44', '389.44', '382.16', '364.39', '348.04', '336.66', '332.99', '332.99', '332.99']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         352.549         352.400         0.149          \n",
      "TC2             352.681         353.350         0.669          \n",
      "TC3             357.444         357.760         0.316          \n",
      "TC4             360.709         361.070         0.361          \n",
      "TC5             362.597         362.370         0.227          \n",
      "TC6             367.412         367.390         0.022          \n",
      "TC7             368.306         368.400         0.094          \n",
      "TC8             371.268         371.690         0.422          \n",
      "TC9             375.453         376.290         0.837          \n",
      "TC10            365.775         364.950         0.825          \n",
      "\n",
      "Test Sample 6744:\n",
      "Inputs: Time=228.00s, h=0.1575, flux=25900, abs=100, surf=0.98\n",
      "Theoretical Temps: ['431.75', '445.66', '359.43', '316.40', '303.74', '300.79', '300.16', '300.03', '300.01', '300.00']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         351.278         350.100         1.178          \n",
      "TC2             351.800         351.420         0.380          \n",
      "TC3             355.175         355.480         0.305          \n",
      "TC4             355.938         356.890         0.952          \n",
      "TC5             356.580         356.770         0.190          \n",
      "TC6             358.815         359.080         0.265          \n",
      "TC7             357.814         358.060         0.246          \n",
      "TC8             360.074         360.020         0.054          \n",
      "TC9             377.538         377.690         0.152          \n",
      "TC10            378.142         379.980         1.838          \n",
      "\n",
      "Test Sample 6423:\n",
      "Inputs: Time=1240.00s, h=0.1575, flux=25900, abs=3, surf=0.98\n",
      "Theoretical Temps: ['359.58', '386.40', '386.40', '386.40', '386.40', '386.40', '386.40', '386.40', '386.40', '386.40']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         368.607         367.930         0.677          \n",
      "TC2             366.479         365.330         1.149          \n",
      "TC3             369.489         368.580         0.909          \n",
      "TC4             369.630         368.900         0.730          \n",
      "TC5             369.751         368.640         1.111          \n",
      "TC6             370.929         370.150         0.779          \n",
      "TC7             370.931         370.150         0.781          \n",
      "TC8             371.826         369.670         2.156          \n",
      "TC9             370.003         368.340         1.663          \n",
      "TC10            363.076         361.510         1.566          \n",
      "\n",
      "Test Sample 6321:\n",
      "Inputs: Time=425.00s, h=0.1575, flux=25900, abs=100, surf=0.98\n",
      "Theoretical Temps: ['440.67', '458.25', '369.00', '320.40', '304.88', '301.05', '300.22', '300.05', '300.01', '300.00']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         345.605         345.630         0.025          \n",
      "TC2             347.053         347.210         0.157          \n",
      "TC3             351.585         352.320         0.735          \n",
      "TC4             353.299         354.670         1.371          \n",
      "TC5             354.540         355.090         0.550          \n",
      "TC6             358.064         358.140         0.076          \n",
      "TC7             358.300         357.920         0.380          \n",
      "TC8             363.278         363.140         0.138          \n",
      "TC9             388.127         388.430         0.303          \n",
      "TC10            391.869         392.650         0.781          \n",
      "\n",
      "Test Sample 1987:\n",
      "Inputs: Time=638.00s, h=0.1575, flux=25900, abs=3, surf=0.76\n",
      "Theoretical Temps: ['334.36', '353.98', '353.98', '353.98', '353.98', '353.98', '353.98', '353.98', '353.98', '353.98']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         363.311         363.970         0.659          \n",
      "TC2             362.173         363.310         1.137          \n",
      "TC3             364.158         365.230         1.072          \n",
      "TC4             364.517         365.300         0.783          \n",
      "TC5             364.303         364.680         0.377          \n",
      "TC6             365.818         366.660         0.842          \n",
      "TC7             364.030         364.510         0.480          \n",
      "TC8             364.440         365.070         0.630          \n",
      "TC9             359.918         360.700         0.782          \n",
      "TC10            336.963         334.970         1.993          \n",
      "\n",
      "Test Sample 2543:\n",
      "Inputs: Time=275.00s, h=0.1575, flux=25900, abs=100, surf=0.98\n",
      "Theoretical Temps: ['446.89', '467.28', '376.47', '323.82', '305.94', '301.31', '300.28', '300.06', '300.01', '300.00']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         351.982         352.350         0.368          \n",
      "TC2             352.480         353.480         1.000          \n",
      "TC3             355.629         355.900         0.271          \n",
      "TC4             356.401         357.640         1.239          \n",
      "TC5             357.083         357.420         0.337          \n",
      "TC6             359.810         359.810         0.000          \n",
      "TC7             358.905         358.040         0.865          \n",
      "TC8             361.906         360.980         0.926          \n",
      "TC9             379.928         381.360         1.432          \n",
      "TC10            377.303         375.530         1.773          \n",
      "\n",
      "Test Sample 4309:\n",
      "Inputs: Time=268.00s, h=0.1575, flux=25900, abs=3, surf=0.76\n",
      "Theoretical Temps: ['318.64', '336.78', '336.78', '336.78', '336.78', '336.78', '336.78', '336.78', '336.78', '336.78']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         363.200         363.780         0.580          \n",
      "TC2             361.452         361.830         0.378          \n",
      "TC3             363.620         364.020         0.400          \n",
      "TC4             363.760         364.470         0.710          \n",
      "TC5             363.783         364.340         0.557          \n",
      "TC6             365.515         366.540         1.025          \n",
      "TC7             364.814         365.190         0.376          \n",
      "TC8             365.622         366.250         0.628          \n",
      "TC9             364.951         366.580         1.629          \n",
      "TC10            352.519         352.380         0.139          \n",
      "\n",
      "Test Sample 5859:\n",
      "Inputs: Time=1021.00s, h=0.1575, flux=25900, abs=20, surf=0.98\n",
      "Theoretical Temps: ['379.51', '408.95', '408.95', '408.24', '393.61', '373.61', '358.22', '353.31', '353.31', '353.31']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         352.159         352.230         0.071          \n",
      "TC2             353.780         353.940         0.160          \n",
      "TC3             361.107         361.210         0.103          \n",
      "TC4             368.398         368.160         0.238          \n",
      "TC5             372.198         371.950         0.248          \n",
      "TC6             378.321         377.380         0.941          \n",
      "TC7             378.917         377.820         1.097          \n",
      "TC8             380.608         379.320         1.288          \n",
      "TC9             382.007         380.550         1.457          \n",
      "TC10            370.375         368.570         1.805          \n",
      "\n",
      "Test Sample 1644:\n",
      "Inputs: Time=230.00s, h=0.1575, flux=25900, abs=3, surf=0.98\n",
      "Theoretical Temps: ['323.99', '346.54', '346.54', '346.54', '346.54', '346.54', '346.54', '346.54', '346.54', '346.54']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         361.289         360.570         0.719          \n",
      "TC2             359.635         360.430         0.794          \n",
      "TC3             361.582         360.670         0.912          \n",
      "TC4             361.736         362.120         0.385          \n",
      "TC5             361.640         361.370         0.270          \n",
      "TC6             363.099         362.370         0.729          \n",
      "TC7             362.327         362.090         0.237          \n",
      "TC8             363.208         363.830         0.622          \n",
      "TC9             360.701         359.240         1.461          \n",
      "TC10            345.552         346.830         1.278          \n",
      "\n",
      "Test Sample 6837:\n",
      "Inputs: Time=423.00s, h=0.1575, flux=25900, abs=100, surf=0.98\n",
      "Theoretical Temps: ['443.50', '462.34', '372.31', '321.88', '305.33', '301.16', '300.24', '300.05', '300.01', '300.00']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         346.016         344.870         1.146          \n",
      "TC2             347.452         346.660         0.792          \n",
      "TC3             351.887         352.290         0.403          \n",
      "TC4             353.559         354.630         1.071          \n",
      "TC5             354.787         354.970         0.183          \n",
      "TC6             358.314         358.220         0.094          \n",
      "TC7             358.444         358.410         0.034          \n",
      "TC8             363.381         363.500         0.119          \n",
      "TC9             388.089         388.410         0.321          \n",
      "TC10            391.041         394.240         3.199          \n",
      "\n",
      "Test Sample 4060:\n",
      "Inputs: Time=1433.00s, h=0.1575, flux=25900, abs=3, surf=0.98\n",
      "Theoretical Temps: ['359.20', '385.97', '385.97', '385.97', '385.97', '385.97', '385.97', '385.97', '385.97', '385.97']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         369.236         368.650         0.586          \n",
      "TC2             367.551         367.040         0.511          \n",
      "TC3             370.325         369.720         0.605          \n",
      "TC4             370.508         370.190         0.318          \n",
      "TC5             370.629         369.840         0.789          \n",
      "TC6             372.180         371.710         0.470          \n",
      "TC7             371.873         371.700         0.173          \n",
      "TC8             372.289         371.750         0.539          \n",
      "TC9             371.091         369.710         1.381          \n",
      "TC10            362.646         361.070         1.576          \n",
      "\n",
      "Test Sample 4460:\n",
      "Inputs: Time=1130.00s, h=0.1575, flux=25900, abs=3, surf=0.98\n",
      "Theoretical Temps: ['353.19', '379.21', '379.21', '379.21', '379.21', '379.21', '379.21', '379.21', '379.21', '379.21']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         344.491         345.640         1.149          \n",
      "TC2             344.185         347.180         2.995          \n",
      "TC3             345.952         346.300         0.348          \n",
      "TC4             347.301         348.550         1.249          \n",
      "TC5             347.849         348.620         0.771          \n",
      "TC6             349.345         350.120         0.775          \n",
      "TC7             349.771         351.010         1.239          \n",
      "TC8             351.816         353.440         1.624          \n",
      "TC9             350.143         351.690         1.547          \n",
      "TC10            337.642         340.260         2.618          \n",
      "\n",
      "Test Sample 1435:\n",
      "Inputs: Time=766.00s, h=0.1575, flux=21250, abs=3, surf=0.98\n",
      "Theoretical Temps: ['325.72', '348.46', '348.46', '348.46', '348.46', '348.46', '348.46', '348.46', '348.46', '348.46']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         353.522         350.820         2.702          \n",
      "TC2             352.396         350.380         2.016          \n",
      "TC3             353.772         351.480         2.292          \n",
      "TC4             353.812         351.650         2.162          \n",
      "TC5             353.424         351.140         2.284          \n",
      "TC6             355.064         353.130         1.934          \n",
      "TC7             353.203         351.180         2.023          \n",
      "TC8             353.642         351.120         2.522          \n",
      "TC9             349.003         346.100         2.903          \n",
      "TC10            324.686         318.670         6.016          \n",
      "\n",
      "Test Sample 1454:\n",
      "Inputs: Time=144.00s, h=0.1575, flux=25900, abs=3, surf=0.76\n",
      "Theoretical Temps: ['316.82', '334.79', '334.79', '334.79', '334.79', '334.79', '334.79', '334.79', '334.79', '334.79']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         362.406         363.590         1.184          \n",
      "TC2             360.794         362.050         1.256          \n",
      "TC3             362.543         362.340         0.203          \n",
      "TC4             362.531         362.910         0.379          \n",
      "TC5             362.455         362.590         0.135          \n",
      "TC6             364.223         364.380         0.157          \n",
      "TC7             363.023         362.000         1.023          \n",
      "TC8             363.825         363.630         0.195          \n",
      "TC9             362.781         363.300         0.519          \n",
      "TC10            346.108         347.220         1.112          \n",
      "\n",
      "Test Sample 2776:\n",
      "Inputs: Time=709.00s, h=0.1575, flux=19400, abs=3, surf=0.76\n",
      "Theoretical Temps: ['332.60', '352.05', '352.05', '352.05', '352.05', '352.05', '352.05', '352.05', '352.05', '352.05']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         355.090         355.000         0.090          \n",
      "TC2             354.165         354.860         0.695          \n",
      "TC3             355.964         356.150         0.186          \n",
      "TC4             356.192         356.630         0.438          \n",
      "TC5             355.874         356.250         0.376          \n",
      "TC6             357.702         358.180         0.478          \n",
      "TC7             355.751         356.000         0.249          \n",
      "TC8             355.926         356.540         0.614          \n",
      "TC9             351.804         352.960         1.156          \n",
      "TC10            328.412         329.030         0.618          \n",
      "\n",
      "Test Sample 5969:\n",
      "Inputs: Time=365.00s, h=0.1575, flux=19400, abs=3, surf=0.76\n",
      "Theoretical Temps: ['316.56', '334.51', '334.51', '334.51', '334.51', '334.51', '334.51', '334.51', '334.51', '334.51']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         359.808         360.380         0.572          \n",
      "TC2             358.249         358.230         0.019          \n",
      "TC3             360.492         360.720         0.228          \n",
      "TC4             360.672         361.090         0.418          \n",
      "TC5             360.650         360.780         0.130          \n",
      "TC6             362.485         362.570         0.085          \n",
      "TC7             361.520         360.910         0.610          \n",
      "TC8             362.138         362.010         0.128          \n",
      "TC9             361.111         361.310         0.199          \n",
      "TC10            347.328         346.840         0.488          \n",
      "\n",
      "=== Average Errors (Test Set) ===\n",
      "Sensor          Avg Error (Â°C) \n",
      "------------------------------\n",
      "TC1_tip         0.653          \n",
      "TC2             0.753          \n",
      "TC3             0.534          \n",
      "TC4             0.746          \n",
      "TC5             0.482          \n",
      "TC6             0.484          \n",
      "TC7             0.582          \n",
      "TC8             0.741          \n",
      "TC9             0.990          \n",
      "TC10            1.693          \n",
      "Plot saved: h0.1575_flux25900.0_abs20.0_surf0.9800000190734863_time1631s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs20.0_surf0.9800000190734863_time315s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.7599999904632568_time734s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs100.0_surf0.9800000190734863_time294s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs20.0_surf0.7599999904632568_time355s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs100.0_surf0.9800000190734863_time228s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.9800000190734863_time1240s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs100.0_surf0.9800000190734863_time425s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.7599999904632568_time638s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs100.0_surf0.9800000190734863_time275s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.7599999904632568_time268s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs20.0_surf0.9800000190734863_time1021s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.9800000190734863_time230s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs100.0_surf0.9800000190734863_time423s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.9800000190734863_time1433s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.9800000190734863_time1130s.png\n",
      "Plot saved: h0.1575_flux21250.0_abs3.0_surf0.9800000190734863_time766s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.7599999904632568_time144s.png\n",
      "Plot saved: h0.1575_flux19400.0_abs3.0_surf0.7599999904632568_time709s.png\n",
      "Plot saved: h0.1575_flux19400.0_abs3.0_surf0.7599999904632568_time365s.png\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Block 11: Standalone Testing\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')  # Use non-interactive Agg backend to avoid tkinter dependency\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# def cross_check_test_predictions(data_dir, h_filter=0.1575, num_samples=5, filter_condition=None):\n",
    "#     \"\"\"Cross-check model predictions with rows from the test dataset.\"\"\"\n",
    "#     # Load inference components\n",
    "#     global pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device\n",
    "#     if not all(var in globals() for var in ['pred_model', 'pred_X_scaler', 'pred_y_scaler', \n",
    "#                                            'pred_time_range', 'pred_column_info', 'pred_device']):\n",
    "#         pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device = load_for_prediction()\n",
    "#         if pred_model is None:\n",
    "#             raise ValueError(\"Failed to load inference components. Ensure model and scalers are saved.\")\n",
    "\n",
    "#     # Load and preprocess data to get test_loader\n",
    "#     data = load_data(data_dir, h_filter=h_filter)\n",
    "#     train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols = preprocess_data(data, use_enhanced_features=True)\n",
    "    \n",
    "#     # Extract test dataset from test_loader\n",
    "#     X_test_scaled = np.concatenate([batch[0].numpy() for batch in test_loader], axis=0)\n",
    "#     y_test_scaled = np.concatenate([batch[1].numpy() for batch in test_loader], axis=0)\n",
    "    \n",
    "#     # Inverse transform to get original feature and target values\n",
    "#     X_test = pred_X_scaler.inverse_transform(X_test_scaled)\n",
    "#     y_test = pred_y_scaler.inverse_transform(y_test_scaled)\n",
    "    \n",
    "#     # Create DataFrame for test data\n",
    "#     feature_cols = pred_column_info['feature_cols']\n",
    "#     test_df = pd.DataFrame(X_test, columns=feature_cols)\n",
    "#     test_df[tc_cols] = y_test\n",
    "    \n",
    "#     # Add original 'Time' (before normalization) for filtering\n",
    "#     time_min = pred_time_range['time_min']\n",
    "#     time_max = pred_time_range['time_max']\n",
    "#     test_df['Time'] = test_df['Time_norm'] * (time_max - time_min) + time_min\n",
    "    \n",
    "#     # Apply filter condition if provided (e.g., specific time or flux)\n",
    "#     if filter_condition is not None:\n",
    "#         test_df = test_df.query(filter_condition)\n",
    "#         if test_df.empty:\n",
    "#             raise ValueError(f\"No test rows match the condition: {filter_condition}\")\n",
    "    \n",
    "#     # Sample rows for cross-checking (no random_state for true randomness)\n",
    "#     num_samples = min(num_samples, len(test_df))\n",
    "#     if num_samples == 0:\n",
    "#         raise ValueError(\"No test samples available after filtering!\")\n",
    "#     sample_rows = test_df.sample(n=num_samples,random_state=33) if filter_condition is None else test_df.head(num_samples)\n",
    "    \n",
    "#     print(f\"\\n=== Cross-Checking {len(sample_rows)} Test Samples ===\")\n",
    "#     results = []\n",
    "    \n",
    "#     for idx, row in sample_rows.iterrows():\n",
    "#         # Prepare input features\n",
    "#         time = row['Time']\n",
    "#         h = row['h']\n",
    "#         flux = row['flux']\n",
    "#         abs_val = row['abs']\n",
    "#         surf = row['surf']\n",
    "#         theoretical_temps = [row[col] for col in [c for c in feature_cols if c.startswith('Theoretical_Temps_')]]\n",
    "        \n",
    "#         # Make prediction\n",
    "#         try:\n",
    "#             pred_result = predict_temperature(\n",
    "#                 pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info,\n",
    "#                 time, h, flux, abs_val, surf, theoretical_temps, pred_device\n",
    "#             )\n",
    "            \n",
    "#             # Get actual temperatures\n",
    "#             actual_temps = {col: row[col] for col in tc_cols}\n",
    "            \n",
    "#             # Compare predictions with actuals\n",
    "#             comparison = {\n",
    "#                 'index': idx,\n",
    "#                 'inputs': {\n",
    "#                     'time': time,\n",
    "#                     'h': h,\n",
    "#                     'flux': flux,\n",
    "#                     'abs': abs_val,\n",
    "#                     'surf': surf,\n",
    "#                     'theoretical_temps': theoretical_temps\n",
    "#                 },\n",
    "#                 'predicted_temps': pred_result['predicted_temperatures'],\n",
    "#                 'actual_temps': actual_temps,\n",
    "#                 'errors': {col: abs(pred_result['predicted_temperatures'][col] - actual_temps[col]) \n",
    "#                           for col in tc_cols}\n",
    "#             }\n",
    "            \n",
    "#             results.append(comparison)\n",
    "            \n",
    "#             # Print comparison\n",
    "#             print(f\"\\nTest Sample {idx}:\")\n",
    "#             print(f\"Inputs: Time={time:.2f}s, h={h:.4f}, flux={flux:.0f}, abs={abs_val:.0f}, surf={surf:.2f}\")\n",
    "#             print(\"Theoretical Temps:\", [f\"{t:.2f}\" for t in theoretical_temps])\n",
    "#             print(\"Predicted vs Actual Temperatures:\")\n",
    "#             print(\"-\" * 50)\n",
    "#             print(f\"{'Sensor':<15} {'Predicted (Â°C)':<15} {'Actual (Â°C)':<15} {'Error (Â°C)':<15}\")\n",
    "#             print(\"-\" * 50)\n",
    "#             for col in tc_cols:\n",
    "#                 pred_temp = pred_result['predicted_temperatures'][col]\n",
    "#                 actual_temp = actual_temps[col]\n",
    "#                 error = comparison['errors'][col]\n",
    "#                 print(f\"{col:<15} {pred_temp:<15.3f} {actual_temp:<15.3f} {error:<15.3f}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing test sample {idx}: {e}\")\n",
    "    \n",
    "#     # Calculate average errors\n",
    "#     if results:\n",
    "#         avg_errors = {col: np.mean([r['errors'][col] for r in results]) for col in tc_cols}\n",
    "#         print(\"\\n=== Average Errors (Test Set) ===\")\n",
    "#         print(f\"{'Sensor':<15} {'Avg Error (Â°C)':<15}\")\n",
    "#         print(\"-\" * 30)\n",
    "#         for col, avg_error in avg_errors.items():\n",
    "#             print(f\"{col:<15} {avg_error:<15.3f}\")\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def run_test_cross_check(data_dir, h_filter=0.1575, num_samples=5, filter_condition=None):\n",
    "#     \"\"\"Run cross-checking on test data without requiring main execution.\"\"\"\n",
    "#     try:\n",
    "#         print(\"\\n=== Running Standalone Test Cross-Check ===\")\n",
    "#         results = cross_check_test_predictions(data_dir, h_filter, num_samples, filter_condition)\n",
    "        \n",
    "#         # Plot results using plot_vertical_profile\n",
    "#         for result in results:\n",
    "#             # Extract predicted and actual temperatures in order TC1_tip to TC10\n",
    "#             tc_cols = ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10']\n",
    "#             predicted = [result['predicted_temps'][col] for col in tc_cols]\n",
    "#             actual = [result['actual_temps'][col] for col in tc_cols]\n",
    "#             # Create filename based on input conditions\n",
    "#             filename = (\n",
    "#                 f\"h{h_filter}_flux{result['inputs']['flux']}_\"\n",
    "#                 f\"abs{result['inputs']['abs']}_surf{result['inputs']['surf']}_\"\n",
    "#                 f\"time{result['inputs']['time']:.0f}s\"\n",
    "#             )\n",
    "#             try:\n",
    "#                 plot_vertical_profile(predicted, actual, filename=f\"Sample {result['index']} - {filename}\")\n",
    "#                 print(f\"Plot saved: {filename}.png\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error plotting sample {result['index']}: {e}\")\n",
    "        \n",
    "#         # Save results to CSV\n",
    "#         # pd.DataFrame(results).to_csv('test_cross_check_results.csv')\n",
    "#         # print(\"Results saved to 'test_cross_check_results.csv'\")\n",
    "        \n",
    "#         return results\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in standalone test cross-check: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     DATA_DIR = get_data_directory()\n",
    "#     run_test_cross_check(DATA_DIR, h_filter=0.1575, num_samples=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
