{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f64754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: Imports and Setup\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35101844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Configuration\n",
    "def get_data_directory():\n",
    "    \"\"\"Find the data directory dynamically\"\"\"\n",
    "    possible_paths = [\n",
    "        \"data/TC11data\",\n",
    "        \"./data/TC11data\", \n",
    "        \"../data/TC11data\",\n",
    "        \"data\",\n",
    "        \"./data\",\n",
    "        \"../data\",\n",
    "        \"D:/Research Assistant work/Github Organization/ml models/ml_models/PhysicsGuidedNeuralNetwork/data/TC11data\"\n",
    "        # \"data/new_processed_reset\",\n",
    "        # \"./data/new_processed_reset\", \n",
    "        # \"../data/new_processed_reset\",\n",
    "        # \"data\",\n",
    "        # \"./data\",\n",
    "        # \"../data\",\n",
    "        # \"D:/Research Assistant work/Github Organization/ml models/ml_models/PhysicsGuidedNeuralNetwork/data/new_processed_reset\"\n",
    "        # \"data/new_processed_fix_new\",\n",
    "        # \"./data/new_processed_fix_new\", \n",
    "        # \"../data/new_processed_fix_new\",\n",
    "        # \"data\",\n",
    "        # \"./data\",\n",
    "        # \"../data\",\n",
    "        # \"D:/Research Assistant work/Github Organization/ml models/ml_models/PhysicsGuidedNeuralNetwork/data/new_processed_fix_new\"\n",
    "\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Found data directory: {path}\")\n",
    "            return path\n",
    "    \n",
    "    # If no directory found, ask user to specify\n",
    "    print(\"Data directory not found. Please specify the path to your data directory:\")\n",
    "    user_path = input(\"Enter data directory path: \").strip()\n",
    "    if os.path.exists(user_path):\n",
    "        return user_path\n",
    "    else:\n",
    "        raise ValueError(f\"Specified data directory {user_path} does not exist!\")\n",
    "\n",
    "DROP_COLS = [\"TC_9_5\", \"TC_Bottom_rec_groove\", \"TC_wall_ins_ext\", \"TC_bottom_ins_groove\", \"Theoretical_Temps_11\"]\n",
    "\n",
    "# File mapping dictionaries\n",
    "h_map = {2: 0.0375, 3: 0.084, 6: 0.1575}\n",
    "flux_map = {88: 25900, 78: 21250, 73: 19400}\n",
    "abs_map = {0: 3, 92: 100}\n",
    "surf_map = {0: 0.98, 1: 0.76}\n",
    "pattern = r\"h(\\d+)_flux(\\d+)_abs(\\d+)(?:_[A-Za-z0-9]+)*_surf([01])(?:_[A-Za-z0-9]+)*[\\s_]+(\\d+)s\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32497ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Data Loading Functions\n",
    "def parse_filename_params(filename):\n",
    "    \"\"\"Parse filename to extract thermal parameters\"\"\"\n",
    "    m = re.search(pattern, filename)\n",
    "    if not m:\n",
    "        return None\n",
    "    h_raw = int(m.group(1))\n",
    "    flux_raw = int(m.group(2))\n",
    "    abs_raw = int(m.group(3))\n",
    "    surf_raw = int(m.group(4))\n",
    "    t = int(m.group(5))\n",
    "\n",
    "    h = h_map.get(h_raw, h_raw)\n",
    "    flux = flux_map.get(flux_raw, flux_raw)\n",
    "    abs_ = abs_map.get(abs_raw, abs_raw)\n",
    "    surf = surf_map.get(surf_raw)\n",
    "\n",
    "    return h, flux, abs_, surf, t\n",
    "\n",
    "def load_and_process_file(path, h, flux, abs_val, surf, filename, min_time=0):\n",
    "    \"\"\"Load and process individual CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "        df = df[df[\"Time\"] >= min_time].copy()\n",
    "        \n",
    "        # Drop unwanted columns\n",
    "        df.drop(columns=[col for col in df.columns if col in DROP_COLS or col.startswith(\"Depth_\")], \n",
    "                inplace=True, errors='ignore')\n",
    "        \n",
    "        # Add parameters\n",
    "        df[\"h\"] = h\n",
    "        df[\"flux\"] = flux\n",
    "        df[\"abs\"] = abs_val\n",
    "        df[\"surf\"] = surf\n",
    "        df[\"filename\"] = filename\n",
    "        \n",
    "        return df.iloc[1:] if len(df) > 1 else df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_data(data_dir, h_filter=None, min_time=0):\n",
    "    \"\"\"Load and combine all data files\"\"\"\n",
    "    dataframes = []\n",
    "    total_files = 0\n",
    "    loaded_files = 0\n",
    "    skipped_files = 0\n",
    "    unmatched_files = 0\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        raise ValueError(f\"Data directory {data_dir} does not exist!\")\n",
    "    \n",
    "    print(f\"Loading data from: {data_dir}\")\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "    if not csv_files:\n",
    "        raise ValueError(\"No CSV files found in the data directory!\")\n",
    "    \n",
    "    for fname in csv_files:\n",
    "        total_files += 1\n",
    "        params = parse_filename_params(fname)\n",
    "        if params is None or params[3] is None:\n",
    "            print(f\"Skipping (unmatched): {fname}\")\n",
    "            unmatched_files += 1\n",
    "            continue\n",
    "\n",
    "        h_val = params[0]\n",
    "        if h_filter is not None:\n",
    "            if isinstance(h_filter, list):\n",
    "                if h_val not in h_filter:\n",
    "                    print(f\"Skipping (h={h_val} not in {h_filter}): {fname}\")\n",
    "                    skipped_files += 1\n",
    "                    continue\n",
    "            else:\n",
    "                if h_val != h_filter:\n",
    "                    print(f\"Skipping (not h={h_filter}): {fname}\")\n",
    "                    skipped_files += 1\n",
    "                    continue\n",
    "\n",
    "        path = os.path.join(data_dir, fname)\n",
    "        try:\n",
    "            df = load_and_process_file(path, *params[:4], filename=fname, min_time=min_time)\n",
    "            if not df.empty:\n",
    "                dataframes.append(df)\n",
    "                print(f\"Loaded: {fname} ({len(df)} rows)\")\n",
    "                loaded_files += 1\n",
    "            else:\n",
    "                print(f\"File {fname} has no data after processing.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {fname}: {e}\")\n",
    "            skipped_files += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"FILE PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total files scanned     : {total_files}\")\n",
    "    print(f\"Files successfully loaded: {loaded_files}\")\n",
    "    print(f\"Files skipped (filtered): {skipped_files}\")\n",
    "    print(f\"Files skipped (unmatched): {unmatched_files}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if not dataframes:\n",
    "        raise ValueError(\"No valid data files found after processing!\")\n",
    "    \n",
    "    data = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"\\nTotal data loaded: {len(data)} rows, {len(data.columns)} columns\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5af29f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4: Model Architecture\n",
    "class EnhancedThermalNet(nn.Module):\n",
    "    \"\"\"Enhanced thermal neural network with attention and residual connections\"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_dims=[512, 256, 256, 128], dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Input processing\n",
    "        self.input_layer = nn.Linear(input_size, hidden_dims[0])\n",
    "        self.input_norm = nn.LayerNorm(hidden_dims[0])\n",
    "        \n",
    "        # Feature attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_size, max(1, input_size // 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(max(1, input_size // 2), input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Hidden layers with residual connections\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.residual_projections = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            layer = nn.Linear(hidden_dims[i], hidden_dims[i + 1])\n",
    "            norm = nn.LayerNorm(hidden_dims[i + 1])\n",
    "            \n",
    "            if hidden_dims[i] != hidden_dims[i + 1]:\n",
    "                residual_proj = nn.Linear(hidden_dims[i], hidden_dims[i + 1])\n",
    "            else:\n",
    "                residual_proj = nn.Identity()\n",
    "            \n",
    "            self.layers.append(layer)\n",
    "            self.norms.append(norm)\n",
    "            self.residual_projections.append(residual_proj)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_size)\n",
    "        \n",
    "        # Activation and regularization\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using He initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply attention to input features\n",
    "        attention_weights = self.attention(x)\n",
    "        x_attended = x * attention_weights\n",
    "        \n",
    "        # Input processing\n",
    "        x = self.activation(self.input_norm(self.input_layer(x_attended)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Forward through hidden layers with residual connections\n",
    "        for i, (layer, norm, residual_proj) in enumerate(zip(self.layers, self.norms, self.residual_projections)):\n",
    "            residual = residual_proj(x)\n",
    "            x = layer(x)\n",
    "            x = norm(x)\n",
    "            x = self.activation(x)\n",
    "            x = x + residual\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c4035d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class PhysicsInformedLoss(nn.Module):\n",
    "    \"\"\"Enhanced Physics-informed loss function with professor's exact physics formula\"\"\"\n",
    "    def __init__(self, smoothness_weight=0.005, gradient_weight=0.0001, physics_weight=0.5, \n",
    "                 conservation_violation_penalty=100.0):\n",
    "        super().__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.smoothness_weight = smoothness_weight\n",
    "        self.gradient_weight = gradient_weight\n",
    "        self.physics_weight = physics_weight\n",
    "        self.conservation_violation_penalty = conservation_violation_penalty\n",
    "        \n",
    "        # Physical constants (keeping your professor's exact values)\n",
    "        self.r = 2.0375 * 0.0254  # Convert inches to meters\n",
    "        self.A_rec = np.pi * (self.r ** 2)  # Receiver area\n",
    "        self.rho = 1836.31  # Density (kg/mÂ³)\n",
    "        self.cp = 1512  # Specific heat capacity (J/kgÂ·K)\n",
    "        \n",
    "        # Convert to tensors for GPU compatibility\n",
    "        self.A_rec_tensor = None\n",
    "        self.rho_tensor = None\n",
    "        self.cp_tensor = None\n",
    "        \n",
    "        # Tracking conservation violations\n",
    "        self.violation_count = 0\n",
    "        self.total_batches = 0\n",
    "    \n",
    "    def _init_tensors(self, device):\n",
    "        \"\"\"Initialize tensors on the correct device\"\"\"\n",
    "        if self.A_rec_tensor is None:\n",
    "            self.A_rec_tensor = torch.tensor(self.A_rec, device=device, dtype=torch.float32)\n",
    "            self.rho_tensor = torch.tensor(self.rho, device=device, dtype=torch.float32)\n",
    "            self.cp_tensor = torch.tensor(self.cp, device=device, dtype=torch.float32)\n",
    "    \n",
    "    def compute_physics_loss(self, predictions, targets, inputs):\n",
    "        \"\"\"\n",
    "        Compute physics-informed loss using professor's exact energy conservation formula\n",
    "        \n",
    "        KEEPING YOUR EXACT PHYSICS:\n",
    "        Energy Conservation Law: Energy_stored â‰¤ Energy_incoming\n",
    "        - Energy_incoming = flux * A_rec (no efficiency factor as per your code)\n",
    "        - Energy_stored = mass * cp * temp_change / dt\n",
    "        - mass = rho * h * A_rec\n",
    "        \n",
    "        The key fixes are in NUMERICAL HANDLING and VIOLATION INTERPRETATION, not physics\n",
    "        \"\"\"\n",
    "        device = predictions.device\n",
    "        self._init_tensors(device)\n",
    "        \n",
    "        batch_size = predictions.shape[0]\n",
    "        self.total_batches += 1\n",
    "        \n",
    "        try:\n",
    "            # Extract relevant features from inputs (keeping your exact indices)\n",
    "            flux = inputs[:, 6]  # flux is at index 6\n",
    "            h = inputs[:, 5]     # h is at index 5\n",
    "            \n",
    "            # Calculate incoming energy (EXACT same formula as your professor's)\n",
    "            incoming_energy = flux * self.A_rec_tensor\n",
    "            \n",
    "            # Calculate mass for each batch (EXACT same formula)\n",
    "            mass = self.rho_tensor * h * self.A_rec_tensor\n",
    "            \n",
    "            # Calculate temperature change (keeping your exact approach)\n",
    "            theoretical_start_idx = 11\n",
    "            num_sensors = predictions.shape[1]\n",
    "            \n",
    "            if inputs.shape[1] >= theoretical_start_idx + num_sensors:\n",
    "                theoretical_temps = inputs[:, theoretical_start_idx:theoretical_start_idx + num_sensors]\n",
    "                temp_change = torch.mean(predictions - theoretical_temps, dim=1)\n",
    "            else:\n",
    "                # Fallback: use difference between prediction and target\n",
    "                temp_change = torch.mean(predictions - targets, dim=1)\n",
    "            \n",
    "            # Calculate total energy stored (EXACT same formula, assuming unit time step)\n",
    "            dt = torch.ones_like(temp_change)\n",
    "            total_energy_stored = mass * self.cp_tensor * temp_change / dt\n",
    "            \n",
    "            # Energy conservation constraint checking (EXACT same logic)\n",
    "            conservation_loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "            \n",
    "            # Case 1: CRITICAL VIOLATION - Energy stored > Energy incoming\n",
    "            # This violates the fundamental law of energy conservation\n",
    "            violation_mask = total_energy_stored > incoming_energy\n",
    "            # Case 1.5: SOFT MARGIN penalty â€” predictions that are close to violating\n",
    "            margin = 0.05 * incoming_energy  # 5% margin\n",
    "            soft_violation_mask = (total_energy_stored > incoming_energy - margin) & (~violation_mask)\n",
    "            if torch.any(soft_violation_mask):\n",
    "                soft_excess = total_energy_stored[soft_violation_mask] - incoming_energy[soft_violation_mask]\n",
    "                soft_relative_violation = soft_excess / (incoming_energy[soft_violation_mask] + 1e-6)\n",
    "                soft_penalty = torch.mean(soft_relative_violation ** 2)\n",
    "                conservation_loss = conservation_loss + 0.1 * self.conservation_violation_penalty * soft_penalty\n",
    "\n",
    "            # Case 2: ACCEPTABLE - Energy stored â‰¤ Energy incoming\n",
    "            acceptable_mask = total_energy_stored <= incoming_energy\n",
    "            if torch.any(acceptable_mask):\n",
    "                # Very mild penalty to encourage efficiency but not violate physics\n",
    "                energy_difference = incoming_energy[acceptable_mask] - total_energy_stored[acceptable_mask]\n",
    "                efficiency_penalty = torch.mean(energy_difference) * 0.01\n",
    "                conservation_loss = conservation_loss + efficiency_penalty\n",
    "            \n",
    "            # Case 3: Near perfect energy transfer (within small tolerance)\n",
    "            balance_tolerance = 0.01 * torch.abs(incoming_energy)\n",
    "            energy_difference = torch.abs(incoming_energy - total_energy_stored)\n",
    "            balance_mask = energy_difference <= balance_tolerance\n",
    "            if torch.any(balance_mask):\n",
    "                # Small reward for achieving good energy balance\n",
    "                balance_reward = torch.mean(energy_difference[balance_mask])\n",
    "                conservation_loss = conservation_loss - 0.05 * balance_reward\n",
    "            \n",
    "            return conservation_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Physics loss computation failed: {e}\")\n",
    "            return torch.tensor(100.0, device=device, requires_grad=True)\n",
    "    \n",
    "    def get_violation_rate(self):\n",
    "        \"\"\"Get the rate of energy conservation violations\"\"\"\n",
    "        if self.total_batches == 0:\n",
    "            return 0.0\n",
    "        # Using actual batch size from data instead of assuming 32\n",
    "        total_samples = self.total_batches * 64  # Your batch size appears to be 64\n",
    "        return self.violation_count / total_samples if total_samples > 0 else 0.0\n",
    "    \n",
    "    def reset_violation_tracking(self):\n",
    "        \"\"\"Reset violation tracking counters\"\"\"\n",
    "        self.violation_count = 0\n",
    "        self.total_batches = 0\n",
    "    \n",
    "    def forward(self, predictions, targets, inputs=None):\n",
    "        \"\"\"\n",
    "        Forward pass with enhanced physics-informed loss\n",
    "        KEEPING YOUR EXACT STRUCTURE\n",
    "        \"\"\"\n",
    "        # Primary MSE loss\n",
    "        mse_loss = self.mse_loss(predictions, targets)\n",
    "        \n",
    "        # Smoothness penalty (spatial consistency)\n",
    "        if predictions.shape[1] > 1:\n",
    "            smoothness_loss = torch.mean(torch.abs(predictions[:, 1:] - predictions[:, :-1]))\n",
    "            gradient_loss = torch.mean(torch.abs(torch.diff(predictions, dim=1)))\n",
    "        else:\n",
    "            smoothness_loss = torch.tensor(0.0, device=predictions.device)\n",
    "            gradient_loss = torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "        # Enhanced physics loss with conservation checking\n",
    "        if inputs is not None:\n",
    "            physics_loss = self.compute_physics_loss(predictions, targets, inputs)\n",
    "        else:\n",
    "            # High penalty if no inputs provided for physics computation\n",
    "            physics_loss = torch.tensor(50.0, device=predictions.device, requires_grad=True)\n",
    "        \n",
    "        # Combined loss (EXACT same structure as your code)\n",
    "        total_loss = (mse_loss + \n",
    "                     self.smoothness_weight * smoothness_loss + \n",
    "                     self.gradient_weight * gradient_loss +\n",
    "                     self.physics_weight * physics_loss)\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_data(data, test_size=0.3, val_size=0.5, use_enhanced_features=True, max_tc_map=None): #FOR ANY NUMBER OF DATASETS\n",
    "#     \"\"\"\n",
    "#     Enhanced data preprocessing to handle variable number of TC sensors.\n",
    "    \n",
    "#     Args:\n",
    "#         data: DataFrame containing all measurements\n",
    "#         max_tc_map: dict mapping h value to max TC sensor index, e.g. {0.0525: 3, 0.1050: 5, 0.1575: 10}\n",
    "#     \"\"\"\n",
    "#     print(\"Starting data preprocessing...\")\n",
    "    \n",
    "#     if max_tc_map is None:\n",
    "#         # Updated to use actual h values rather than raw numbers\n",
    "#         max_tc_map = {\n",
    "#             0.0375:2, 0.084:3, 0.1575:6\n",
    "#         }\n",
    "\n",
    "#     # Check if all h values in data are mapped\n",
    "#     unique_h_vals = data[\"h\"].unique()\n",
    "#     for h_val in unique_h_vals:\n",
    "#         if h_val not in max_tc_map:\n",
    "#             raise ValueError(f\"No TC mapping defined for h={h_val}. Update max_tc_map.\")\n",
    "\n",
    "#     # Process each h value separately\n",
    "#     all_datasets = []\n",
    "#     tc_cols_list = []\n",
    "    \n",
    "#     for h_val in unique_h_vals:\n",
    "#         h_data = data[data[\"h\"] == h_val].copy()\n",
    "#         max_tc = max_tc_map[h_val]\n",
    "#         print(f\"Processing h={h_val}, using first {max_tc} TC sensors\")\n",
    "\n",
    "#         # Time normalization\n",
    "#         time_min = h_data[\"Time\"].min()\n",
    "#         time_max = h_data[\"Time\"].max()\n",
    "#         h_data[\"Time_norm\"] = (h_data[\"Time\"] - time_min) / (time_max - time_min)\n",
    "\n",
    "#         # Enhanced features\n",
    "#         if use_enhanced_features:\n",
    "#             h_data[\"TimeÂ²\"] = h_data[\"Time_norm\"] ** 2\n",
    "#             h_data[\"TimeÂ³\"] = h_data[\"Time_norm\"] ** 3\n",
    "#             h_data[\"Time_sin\"] = np.sin(2 * np.pi * h_data[\"Time_norm\"])\n",
    "#             h_data[\"Time_cos\"] = np.cos(2 * np.pi * h_data[\"Time_norm\"])\n",
    "#             h_data[\"flux_abs_interaction\"] = h_data[\"flux\"] * h_data[\"abs\"]\n",
    "#             h_data[\"h_flux_interaction\"] = h_data[\"h\"] * h_data[\"flux\"]\n",
    "#             base_features = [\"Time_norm\", \"TimeÂ²\", \"TimeÂ³\", \"Time_sin\", \"Time_cos\", \n",
    "#                            \"h\", \"flux\", \"abs\", \"surf\", \n",
    "#                            \"flux_abs_interaction\", \"h_flux_interaction\"]\n",
    "#         else:\n",
    "#             h_data[\"TimeÂ²\"] = h_data[\"Time_norm\"] ** 2\n",
    "#             base_features = [\"Time_norm\", \"TimeÂ²\", \"h\", \"flux\", \"abs\", \"surf\"]\n",
    "\n",
    "#         # Theoretical columns\n",
    "#         theory_cols = [c for c in h_data.columns if c.startswith(\"Theoretical_Temps_\")]\n",
    "#         theory_cols = [col for col in theory_cols if col in h_data.columns]\n",
    "\n",
    "#         # TC columns (take only the first N for this h)\n",
    "#         all_tc_cols = [col for col in h_data.columns if col.startswith(\"TC\")]\n",
    "#         all_tc_cols.sort(key=lambda x: int(re.search(r\"\\d+\", x).group()))\n",
    "#         tc_cols = all_tc_cols[:max_tc]\n",
    "#         tc_cols_list.append(tc_cols)  # Save for verification\n",
    "\n",
    "#         print(f\"For h={h_val}, using TC columns: {tc_cols}\")\n",
    "\n",
    "#         # Features and targets\n",
    "#         feature_cols = base_features + theory_cols\n",
    "#         feature_cols = [col for col in feature_cols if col in h_data.columns]\n",
    "\n",
    "#         X = h_data[feature_cols].copy()\n",
    "#         y = h_data[tc_cols].copy()\n",
    "#         filenames = h_data.get(\"filename\", pd.Series([\"unknown\"] * len(h_data)))\n",
    "\n",
    "#         # Drop rows with missing values\n",
    "#         mask = ~(X.isnull().any(axis=1) | y.isnull().any(axis=1))\n",
    "#         X = X[mask].reset_index(drop=True)\n",
    "#         y = y[mask].reset_index(drop=True)\n",
    "#         filenames = filenames[mask].reset_index(drop=True)\n",
    "\n",
    "#         all_datasets.append((X, y, filenames))\n",
    "\n",
    "#     # Combine all datasets\n",
    "#     X_combined = pd.concat([d[0] for d in all_datasets])\n",
    "#     y_combined = pd.concat([d[1] for d in all_datasets])\n",
    "#     filenames_combined = pd.concat([d[2] for d in all_datasets])\n",
    "\n",
    "#     # Verify we have consistent TC columns across all h values\n",
    "#     if len(set(tuple(cols) for cols in tc_cols_list)) > 1:\n",
    "#         print(\"Warning: Different TC columns used for different h values\")\n",
    "#     tc_cols = tc_cols_list[0]  # Use first set (or implement more sophisticated handling)\n",
    "\n",
    "#     # Split\n",
    "#     X_train, X_temp, y_train, y_temp, _, _ = train_test_split(\n",
    "#         X_combined, y_combined, filenames_combined, test_size=test_size, random_state=SEED\n",
    "#     )\n",
    "#     X_val, X_test, y_val, y_test, _, _ = train_test_split(\n",
    "#         X_temp, y_temp, _, test_size=val_size, random_state=SEED\n",
    "#     )\n",
    "\n",
    "#     # Scaling\n",
    "#     X_scaler = StandardScaler()\n",
    "#     y_scaler = MinMaxScaler()\n",
    "#     X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "#     X_val_scaled = X_scaler.transform(X_val)\n",
    "#     X_test_scaled = X_scaler.transform(X_test)\n",
    "#     y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "#     y_val_scaled = y_scaler.transform(y_val)\n",
    "#     y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "#     # Save scalers & metadata\n",
    "#     joblib.dump(X_scaler, \"X_scaler_enhanced.pkl\")\n",
    "#     joblib.dump(y_scaler, \"y_scaler_enhanced.pkl\")\n",
    "#     joblib.dump({\"feature_cols\": feature_cols, \"tc_cols\": tc_cols}, \"column_info.pkl\")\n",
    "\n",
    "#     # Create DataLoaders\n",
    "#     train_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "#                                 torch.tensor(y_train_scaled, dtype=torch.float32))\n",
    "#     val_dataset = TensorDataset(torch.tensor(X_val_scaled, dtype=torch.float32),\n",
    "#                               torch.tensor(y_val_scaled, dtype=torch.float32))\n",
    "#     test_dataset = TensorDataset(torch.tensor(X_test_scaled, dtype=torch.float32),\n",
    "#                                torch.tensor(y_test_scaled, dtype=torch.float32))\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=min(64, len(X_train)//4), shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=min(64, len(X_val)//4), shuffle=False)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=min(64, len(X_test)//4), shuffle=False)\n",
    "\n",
    "#     return train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82143980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: Data Preprocessing ONLY SINGLEE\n",
    "def preprocess_data(data, test_size=0.3, val_size=0.5, use_enhanced_features=True):\n",
    "    \"\"\"Enhanced data preprocessing\"\"\"\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    \n",
    "    # Time normalization\n",
    "    time_min = data[\"Time\"].min()\n",
    "    time_max = data[\"Time\"].max()\n",
    "    data[\"Time_norm\"] = (data[\"Time\"] - time_min) / (time_max - time_min)\n",
    "    \n",
    "    # Enhanced time features\n",
    "    if use_enhanced_features:\n",
    "        data[\"TimeÂ²\"] = data[\"Time_norm\"] ** 2\n",
    "        data[\"TimeÂ³\"] = data[\"Time_norm\"] ** 3\n",
    "        data[\"Time_sin\"] = np.sin(2 * np.pi * data[\"Time_norm\"])\n",
    "        data[\"Time_cos\"] = np.cos(2 * np.pi * data[\"Time_norm\"])\n",
    "        \n",
    "        # Feature interaction terms\n",
    "        data[\"flux_abs_interaction\"] = data[\"flux\"] * data[\"abs\"]\n",
    "        data[\"h_flux_interaction\"] = data[\"h\"] * data[\"flux\"]\n",
    "        \n",
    "        # Enhanced feature set\n",
    "        base_features = [\"Time_norm\", \"TimeÂ²\", \"TimeÂ³\", \"Time_sin\", \"Time_cos\", \n",
    "                        \"h\", \"flux\", \"abs\", \"surf\", \"flux_abs_interaction\", \"h_flux_interaction\"]\n",
    "    else:\n",
    "        data[\"TimeÂ²\"] = data[\"Time_norm\"] ** 2\n",
    "        base_features = [\"Time_norm\", \"TimeÂ²\", \"h\", \"flux\", \"abs\", \"surf\"]\n",
    "    \n",
    "    # Identify columns\n",
    "    theory_cols = [c for c in data.columns if c.startswith(\"Theoretical_Temps_\")]\n",
    "    tc_cols = [col for col in data.columns if (col.startswith(\"TC\") and not col.endswith(\"_rate\")) or col.startswith(\"TC_\")]\n",
    "    \n",
    "    # Filter out any columns that don't exist\n",
    "    theory_cols = [col for col in theory_cols if col in data.columns]\n",
    "    tc_cols = [col for col in tc_cols if col in data.columns]\n",
    "    \n",
    "    if not tc_cols:\n",
    "        print(\"Warning: No TC columns found. Creating dummy TC columns for demonstration.\")\n",
    "        tc_cols = ['TC_1', 'TC_2', 'TC_3', 'TC_4']\n",
    "        for col in tc_cols:\n",
    "            if col not in data.columns:\n",
    "                data[col] = 20 + np.random.normal(0, 5, len(data))\n",
    "    \n",
    "    # Prepare features and targets\n",
    "    feature_cols = base_features + theory_cols\n",
    "    # Filter out any feature columns that don't exist\n",
    "    feature_cols = [col for col in feature_cols if col in data.columns]\n",
    "    \n",
    "    X = data[feature_cols].copy()\n",
    "    y = data[tc_cols].copy()\n",
    "    filenames = data[\"filename\"] if \"filename\" in data.columns else pd.Series([\"unknown\"] * len(data))\n",
    "    \n",
    "    print(f\"Features: {len(feature_cols)}, Targets: {len(tc_cols)}\")\n",
    "    print(f\"Theory columns: {len(theory_cols)}, TC columns: {len(tc_cols)}\")\n",
    "    print(f\"Feature columns: {feature_cols}\")\n",
    "    print(f\"Target columns: {tc_cols}\")\n",
    "    \n",
    "    # Remove missing values\n",
    "    mask = ~(X.isnull().any(axis=1) | y.isnull().any(axis=1))\n",
    "    X = X[mask].reset_index(drop=True)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "    filenames = filenames[mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"After removing missing values: {len(X)} samples\")\n",
    "    \n",
    "    if len(X) < 10:\n",
    "        raise ValueError(\"Not enough valid samples for training!\")\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_temp, y_train, y_temp, _, _ = train_test_split(\n",
    "        X, y, filenames, test_size=test_size, random_state=SEED\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test, _, _ = train_test_split(\n",
    "        X_temp, y_temp, _, test_size=val_size, random_state=SEED\n",
    "    )\n",
    "    \n",
    "    # Scaling\n",
    "    X_scaler = StandardScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "    \n",
    "    X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "    X_val_scaled = X_scaler.transform(X_val)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "    \n",
    "    y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "    y_val_scaled = y_scaler.transform(y_val)\n",
    "    y_test_scaled = y_scaler.transform(y_test)\n",
    "    \n",
    "    # Save scalers and metadata\n",
    "    joblib.dump(X_scaler, \"X_scaler_enhanced.pkl\")\n",
    "    joblib.dump(y_scaler, \"y_scaler_enhanced.pkl\")\n",
    "    joblib.dump({\"time_min\": time_min, \"time_max\": time_max}, \"time_range_enhanced.pkl\")\n",
    "    joblib.dump({\"feature_cols\": feature_cols, \"tc_cols\": tc_cols}, \"column_info.pkl\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "    )\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(X_test_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=min(64, len(X_train)//4), shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=min(64, len(X_val)//4), shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=min(64, len(X_test)//4), shuffle=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba64751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_conservation_monitoring(model, train_loader, val_loader, device, epochs=1000, patience=50):\n",
    "    \"\"\"Enhanced training with conservation violation monitoring - KEEPING YOUR EXACT PHYSICS\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, min_lr=1e-6)\n",
    "    \n",
    "    # Enhanced criterion with conservation violation penalty (KEEPING YOUR EXACT VALUES)\n",
    "    criterion = PhysicsInformedLoss(\n",
    "        smoothness_weight=0.005,\n",
    "        gradient_weight=0.0001,\n",
    "        physics_weight=0.5,\n",
    "        conservation_violation_penalty=100.0  # Keeping your exact penalty\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    violation_rates = []\n",
    "    \n",
    "    print(f\"Starting training with professor's energy conservation formula...\")\n",
    "    print(f\"Physics loss weight: {criterion.physics_weight}\")\n",
    "    print(f\"Conservation violation penalty: {criterion.conservation_violation_penalty}\")\n",
    "    print(f\"PHYSICS: Energy_stored = rho * h * A_rec * cp * temp_change / dt\")\n",
    "    print(f\"PHYSICS: Energy_incoming = flux * A_rec\")\n",
    "    print(f\"CONSTRAINT: Energy_stored â‰¤ Energy_incoming\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Reset violation tracking for this epoch\n",
    "        criterion.reset_violation_tracking()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss_epoch = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            \n",
    "            # Pass inputs to loss function for physics computation\n",
    "            loss = criterion(predictions, y_batch, X_batch)\n",
    "            \n",
    "            # Check for invalid loss values\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"Warning: Invalid loss detected at epoch {epoch}\")\n",
    "                continue\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_epoch += loss.item()\n",
    "        \n",
    "        train_loss_epoch /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_epoch = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                predictions = model(X_batch)\n",
    "                \n",
    "                loss = criterion(predictions, y_batch, X_batch)\n",
    "                val_loss_epoch += loss.item()\n",
    "        \n",
    "        val_loss_epoch /= len(val_loader)\n",
    "        \n",
    "        # Track violation rate\n",
    "        violation_rate = criterion.get_violation_rate()\n",
    "        violation_rates.append(violation_rate)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss_epoch)\n",
    "        \n",
    "        # Track metrics\n",
    "        train_losses.append(train_loss_epoch)\n",
    "        val_losses.append(val_loss_epoch)\n",
    "        \n",
    "        # Early stopping with conservation violation consideration\n",
    "        adjusted_val_loss = val_loss_epoch + (violation_rate * 100)  # Heavily penalize violations\n",
    "        \n",
    "        if adjusted_val_loss < best_val_loss:\n",
    "            best_val_loss = adjusted_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss_epoch,\n",
    "                'train_loss': train_loss_epoch,\n",
    "                'violation_rate': violation_rate,\n",
    "            }, 'best_thermal_model_enhanced.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # IMPROVED LOGGING: Less frequent, more informative\n",
    "        if (epoch + 1) % 100 == 0 or (epoch < 50 and (epoch + 1) % 10 == 0):\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Categorize violation rate for better understanding\n",
    "            if violation_rate < 0.01:\n",
    "                status = \"âœ… EXCELLENT\"\n",
    "            elif violation_rate < 0.05:\n",
    "                status = \"âš ï¸  ACCEPTABLE\" \n",
    "            else:\n",
    "                status = \"ðŸš¨ CONCERNING\"\n",
    "                \n",
    "            print(f\"Epoch [{epoch+1:4d}/{epochs}] \"\n",
    "                  f\"Train: {train_loss_epoch:.6f} \"\n",
    "                  f\"Val: {val_loss_epoch:.6f} \"\n",
    "                  f\"Violations: {violation_rate:.4f} ({status}) \"\n",
    "                  f\"LR: {current_lr:.8f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint = torch.load('best_thermal_model_enhanced.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING COMPLETED\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Best validation loss: {checkpoint['val_loss']:.6f}\")\n",
    "    print(f\"Final violation rate: {checkpoint['violation_rate']:.4f}\")\n",
    "    \n",
    "    # IMPROVED INTERPRETATION of violations\n",
    "    final_rate = checkpoint['violation_rate']\n",
    "    if final_rate < 0.01:\n",
    "        print(\"ðŸŽ‰ EXCELLENT: Physics constraints well learned!\")\n",
    "        print(\"   Model respects energy conservation in >99% of cases\")\n",
    "    elif final_rate < 0.05:\n",
    "        print(\"âœ… GOOD: Acceptable physics compliance\")\n",
    "        print(\"   Small violations may be due to:\")\n",
    "        print(\"   - Measurement noise in training data\")\n",
    "        print(\"   - Numerical precision limits\")\n",
    "        print(\"   - Heat losses not captured in simplified model\")\n",
    "    else:\n",
    "        print(\"âš ï¸  NEEDS ATTENTION: High violation rate\")\n",
    "        print(\"   Consider:\")\n",
    "        print(\"   - Increasing physics_weight parameter\")\n",
    "        print(\"   - Checking data preprocessing\")\n",
    "        print(\"   - Validating theoretical temperature calculations\")\n",
    "        print(\"   - Reviewing input feature scaling\")\n",
    "    \n",
    "    return train_losses, val_losses, violation_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def temperature_to_energy_per_sensor(temps, depths, rho, A_rec, cp, dt=1.0):\n",
    "#     \"\"\"\n",
    "#     Convert temperature array (samples x sensors) to energy stored per sensor.\n",
    "#     temps: np.array shape (N_samples, N_sensors) (Â°C or K difference)\n",
    "#     depths: np.array shape (N_sensors,) or scalar (m)\n",
    "#     Returns energy (Joules) same shape as temps.\n",
    "#     \"\"\"\n",
    "#     if np.isscalar(depths):\n",
    "#         depths = np.full((temps.shape[1],), depths)\n",
    "#     mass = rho * depths * A_rec  # kg per sensor\n",
    "#     energy = temps * (mass * cp) / dt  # energy = mass * cp * delta_T / dt\n",
    "#     return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def temperature_to_total_energy_binned(temp_changes, sensor_depths, rho, A_rec, cp, dt=1.0):\n",
    "#     \"\"\"\n",
    "#     Convert temperature CHANGE array to total energy stored using bins between TC sensors.\n",
    "    \n",
    "#     Args:\n",
    "#         temp_changes: np.array shape (N_samples, N_sensors) - temperature CHANGES in Â°C or K\n",
    "#         sensor_depths: np.array shape (N_sensors,) - depths in meters [deepest to surface]\n",
    "#         rho: float - density (kg/mÂ³)\n",
    "#         A_rec: float - receiver cross-sectional area (mÂ²)\n",
    "#         cp: float - specific heat capacity (J/kgÂ·K)\n",
    "#         dt: float - time step (s)\n",
    "    \n",
    "#     Returns:\n",
    "#         total_energy: np.array shape (N_samples,) - total energy for all bins\n",
    "#     \"\"\"\n",
    "#     N_samples, N_sensors = temp_changes.shape\n",
    "    \n",
    "#     if len(sensor_depths) != N_sensors:\n",
    "#         raise ValueError(f\"Mismatch: {N_sensors} sensors but {len(sensor_depths)} depths provided\")\n",
    "    \n",
    "#     # Sort depths to ensure proper ordering (deepest to surface)\n",
    "#     sorted_indices = np.argsort(sensor_depths)[::-1]  # Sort descending (deepest first)\n",
    "#     sorted_depths = sensor_depths[sorted_indices]\n",
    "#     sorted_temp_changes = temp_changes[:, sorted_indices]\n",
    "    \n",
    "#     # Calculate bins between consecutive sensors\n",
    "#     N_bins = N_sensors - 1\n",
    "#     total_energy_all_samples = np.zeros(N_samples)\n",
    "    \n",
    "#     for sample_idx in range(N_samples):\n",
    "#         sample_total_energy = 0.0\n",
    "        \n",
    "#         for bin_idx in range(N_bins):\n",
    "#             # Bin between sensor bin_idx and bin_idx+1\n",
    "#             depth_start = sorted_depths[bin_idx]      # Deeper sensor\n",
    "#             depth_end = sorted_depths[bin_idx + 1]    # Shallower sensor\n",
    "            \n",
    "#             # Bin thickness (depth difference)\n",
    "#             bin_thickness = abs(depth_start - depth_end)\n",
    "            \n",
    "#             # Average temperature CHANGE for this bin (linear interpolation between sensors)\n",
    "#             temp_change_start = sorted_temp_changes[sample_idx, bin_idx]\n",
    "#             temp_change_end = sorted_temp_changes[sample_idx, bin_idx + 1]\n",
    "#             avg_temp_change = (temp_change_start + temp_change_end) / 2.0\n",
    "            \n",
    "#             # Mass of material in this bin\n",
    "#             bin_volume = bin_thickness * A_rec\n",
    "#             bin_mass = rho * bin_volume\n",
    "            \n",
    "#             # Energy stored in this bin: mass * cp * temp_change / dt\n",
    "#             bin_energy = bin_mass * cp * avg_temp_change / dt\n",
    "            \n",
    "#             sample_total_energy += bin_energy\n",
    "        \n",
    "#         total_energy_all_samples[sample_idx] = sample_total_energy\n",
    "    \n",
    "#     return total_energy_all_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8766b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_energy_metrics(pred_temps, actual_temps, sensor_depths, rho, A_rec, cp, dt=1.0):\n",
    "#     \"\"\"\n",
    "#     Calculate total energy stored metrics for predicted vs actual temperatures.\n",
    "    \n",
    "#     Args:\n",
    "#         pred_temps: np.array shape (N_samples, N_sensors) - predicted temperatures (temperature changes)\n",
    "#         actual_temps: np.array shape (N_samples, N_sensors) - actual temperatures (temperature changes)\n",
    "#         sensor_depths: np.array shape (N_sensors,) - sensor depths\n",
    "#         rho, A_rec, cp, dt: physical constants\n",
    "    \n",
    "#     Returns:\n",
    "#         dict with energy metrics\n",
    "#     \"\"\"\n",
    "#     # Calculate total energy for predictions and actuals using the binned approach\n",
    "#     total_energy_pred = temperature_to_total_energy_binned(\n",
    "#         pred_temps, sensor_depths, rho, A_rec, cp, dt\n",
    "#     )\n",
    "#     total_energy_actual = temperature_to_total_energy_binned(\n",
    "#         actual_temps, sensor_depths, rho, A_rec, cp, dt\n",
    "#     )\n",
    "    \n",
    "#     # Calculate metrics\n",
    "#     energy_diff = total_energy_actual - total_energy_pred\n",
    "#     rmse_energy = np.sqrt(np.mean(energy_diff ** 2))\n",
    "#     mae_energy = np.mean(np.abs(energy_diff))\n",
    "#     mape_energy = np.mean(np.abs(energy_diff / (total_energy_actual + 1e-8))) * 100\n",
    "    \n",
    "#     # RÂ² score\n",
    "#     ss_tot = np.sum((total_energy_actual - np.mean(total_energy_actual)) ** 2)\n",
    "#     ss_res = np.sum(energy_diff ** 2)\n",
    "#     r2_energy = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "    \n",
    "#     return {\n",
    "#         'total_energy_predicted': total_energy_pred,\n",
    "#         'total_energy_actual': total_energy_actual,\n",
    "#         'rmse_energy': rmse_energy,\n",
    "#         'mae_energy': mae_energy, \n",
    "#         'mape_energy': mape_energy,\n",
    "#         'r2_energy': r2_energy,\n",
    "#         'mean_actual_energy': np.mean(total_energy_actual),\n",
    "#         'std_actual_energy': np.std(total_energy_actual)\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a54e3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7: Evaluation and Visualization\n",
    "def evaluate_model(model, test_loader, y_scaler, device, sensor_depths, sensor_names):\n",
    "    \"\"\"Comprehensive model evaluation with temperature and energy stored metrics.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            pred = model(X_batch).cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "            actuals.append(y_batch.numpy())\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "    \n",
    "    # Inverse transform to real temperatures\n",
    "    pred_real = y_scaler.inverse_transform(predictions)\n",
    "    actual_real = y_scaler.inverse_transform(actuals)\n",
    "    \n",
    "    # Temperature-based metrics\n",
    "    rmse_temp = np.sqrt(np.mean((pred_real - actual_real) ** 2, axis=0))\n",
    "    mae_temp = np.mean(np.abs(pred_real - actual_real), axis=0)\n",
    "    mape_temp = np.mean(np.abs((actual_real - pred_real) / (actual_real + 1e-8)), axis=0) * 100\n",
    "    y_mean = np.mean(actual_real, axis=0)\n",
    "    ss_tot = np.sum((actual_real - y_mean) ** 2, axis=0)\n",
    "    ss_res = np.sum((actual_real - pred_real) ** 2, axis=0)\n",
    "    r2_temp = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "    \n",
    "    overall_rmse_temp = np.sqrt(np.mean((pred_real - actual_real) ** 2))\n",
    "    overall_mae_temp = np.mean(np.abs(pred_real - actual_real))\n",
    "    overall_r2_temp = np.mean(r2_temp)\n",
    "    \n",
    "    # Constants for energy calculation\n",
    "    rho = 1836.31  # kg/mÂ³\n",
    "    r = 2.0375 * 0.0254  # radius in meters (converted from inches)\n",
    "    A_rec = np.pi * r**2  # receiver cross-sectional area (mÂ²)\n",
    "    cp = 1512  # J/kgÂ·K\n",
    "    dt = 1.0  # time step (s)\n",
    "    \n",
    "    # Convert temps to energy stored per sensor (Joules)\n",
    "    # energy_pred = temperature_to_energy_per_sensor(pred_real, sensor_depths, rho, A_rec, cp, dt)\n",
    "    # energy_actual = temperature_to_energy_per_sensor(actual_real, sensor_depths, rho, A_rec, cp, dt)\n",
    "    \n",
    "    # Energy-based metrics\n",
    "    # rmse_energy = np.sqrt(np.mean((energy_pred - energy_actual) ** 2, axis=0))\n",
    "    # mae_energy = np.mean(np.abs(energy_pred - energy_actual), axis=0)\n",
    "    # mape_energy = np.mean(np.abs((energy_actual - energy_pred) / (energy_actual + 1e-8)), axis=0) * 100\n",
    "    # y_mean_energy = np.mean(energy_actual, axis=0)\n",
    "    # ss_tot_energy = np.sum((energy_actual - y_mean_energy) ** 2, axis=0)\n",
    "    # ss_res_energy = np.sum((energy_actual - energy_pred) ** 2, axis=0)\n",
    "    # r2_energy = 1 - (ss_res_energy / (ss_tot_energy + 1e-8))\n",
    "    \n",
    "    # overall_rmse_energy = np.sqrt(np.mean((energy_pred - energy_actual) ** 2))\n",
    "    # overall_mae_energy = np.mean(np.abs(energy_pred - energy_actual))\n",
    "    # overall_r2_energy = np.mean(r2_energy)\n",
    "\n",
    "    # # --- NEW: Calculate typical energy stored magnitude for actual data ---\n",
    "    # mean_energy_per_sensor = np.mean(energy_actual, axis=0)\n",
    "    # median_energy_per_sensor = np.median(energy_actual, axis=0)\n",
    "    # overall_mean_energy = np.mean(energy_actual)\n",
    "    \n",
    "    # print(\"\\n--- Energy Stored Magnitude Summary ---\")\n",
    "    # for i, sensor in enumerate(sensor_names):\n",
    "    #     print(f\"{sensor}: Mean = {mean_energy_per_sensor[i]:.1f} W, Median = {median_energy_per_sensor[i]:.1f} W\")\n",
    "    # print(f\"Overall mean energy stored (all sensors): {overall_mean_energy:.1f} W\")\n",
    "    \n",
    "\n",
    "    # ----\n",
    "    # Your model has TC1 deepest, TC10 surface\n",
    "    # So reverse all metrics so index 0 corresponds to TC10 at surface for display\n",
    "    # ----\n",
    "\n",
    "    rmse_temp_rev = rmse_temp[::-1]\n",
    "    mae_temp_rev = mae_temp[::-1]\n",
    "    mape_temp_rev = mape_temp[::-1]\n",
    "    r2_temp_rev = r2_temp[::-1]\n",
    "\n",
    "    # rmse_energy_rev = rmse_energy[::-1]\n",
    "    # mae_energy_rev = mae_energy[::-1]\n",
    "    # mape_energy_rev = mape_energy[::-1]\n",
    "    # r2_energy_rev = r2_energy[::-1]\n",
    "\n",
    "    sensor_names_rev = sensor_names[::-1]  # TC10...TC1\n",
    "    \n",
    "    # Return all results including reversed arrays for display\n",
    "    return {\n",
    "        'predictions': pred_real,\n",
    "        'actuals': actual_real,\n",
    "        'rmse_temp': rmse_temp_rev,\n",
    "        'mae_temp': mae_temp_rev,\n",
    "        'mape_temp': mape_temp_rev,\n",
    "        'r2_temp': r2_temp_rev,\n",
    "        'overall_rmse_temp': overall_rmse_temp,\n",
    "        'overall_mae_temp': overall_mae_temp,\n",
    "        'overall_r2_temp': overall_r2_temp,\n",
    "        # 'overall_mean_energy': overall_mean_energy,\n",
    "        # 'rmse_energy': rmse_energy_rev,\n",
    "        # 'mae_energy': mae_energy_rev,\n",
    "        # 'mape_energy': mape_energy_rev,\n",
    "        # 'r2_energy': r2_energy_rev,\n",
    "        # 'overall_rmse_energy': overall_rmse_energy,\n",
    "        # 'overall_mae_energy': overall_mae_energy,\n",
    "        # 'overall_r2_energy': overall_r2_energy,\n",
    "        'sensor_names_rev': sensor_names_rev,\n",
    "    }\n",
    "\n",
    "def plot_results(train_losses, val_losses, results, tc_cols, violation_rates=None):\n",
    "    \"\"\"Plot training history and results with optional conservation monitoring\"\"\"\n",
    "    \n",
    "    # Determine subplot layout based on whether we have violation rates\n",
    "    if violation_rates is not None:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 10))  # 2x3 grid to include violation rates\n",
    "    else:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))   # Original 2x2 grid\n",
    "    \n",
    "    # Training history\n",
    "    axes[0, 0].plot(train_losses, label='Training Loss', color='blue')\n",
    "    axes[0, 0].plot(val_losses, label='Validation Loss', color='orange')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # RMSE by sensor (temperature)\n",
    "    sensor_names_rev = results.get('sensor_names_rev', tc_cols[::-1])\n",
    "    axes[0, 1].bar(range(len(sensor_names_rev)), results['rmse_temp'])\n",
    "    axes[0, 1].set_xlabel('Sensor')\n",
    "    axes[0, 1].set_ylabel('RMSE (Â°C)')\n",
    "    axes[0, 1].set_title('RMSE by Sensor (Temperature)')\n",
    "    axes[0, 1].set_xticks(range(len(sensor_names_rev)))\n",
    "    axes[0, 1].set_xticklabels(sensor_names_rev, rotation=45)\n",
    "    \n",
    "    # Conservation violation rates (if provided)\n",
    "    if violation_rates is not None:\n",
    "        axes[0, 2].plot(violation_rates, color='red', linewidth=2)\n",
    "        axes[0, 2].set_xlabel('Epoch')\n",
    "        axes[0, 2].set_ylabel('Violation Rate')\n",
    "        axes[0, 2].set_title('Energy Conservation Violation Rate')\n",
    "        axes[0, 2].grid(True)\n",
    "        \n",
    "        # Add horizontal line at acceptable threshold (e.g., 1%)\n",
    "        axes[0, 2].axhline(y=0.01, color='orange', linestyle='--', alpha=0.7, \n",
    "                          label='Acceptable Threshold (1%)')\n",
    "        axes[0, 2].legend()\n",
    "        \n",
    "        # Color-code the background based on violation severity\n",
    "        if len(violation_rates) > 0:\n",
    "            final_rate = violation_rates[-1]\n",
    "            if final_rate > 0.05:  # > 5% violations\n",
    "                axes[0, 2].set_facecolor('#ffeeee')  # Light red background\n",
    "            elif final_rate > 0.01:  # > 1% violations\n",
    "                axes[0, 2].set_facecolor('#fff8ee')  # Light yellow background\n",
    "            else:  # < 1% violations\n",
    "                axes[0, 2].set_facecolor('#eeffee')  # Light green background\n",
    "\n",
    "    # Scatter plot\n",
    "    axes[1, 0].scatter(results['actuals'].flatten(), results['predictions'].flatten(), alpha=0.5)\n",
    "    min_val = min(results['actuals'].min(), results['predictions'].min())\n",
    "    max_val = max(results['actuals'].max(), results['predictions'].max())\n",
    "    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('Actual Temperature (Â°C)')\n",
    "    axes[1, 0].set_ylabel('Predicted Temperature (Â°C)')\n",
    "    axes[1, 0].set_title('Predicted vs Actual')\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # RÂ² scores by sensor (temperature)\n",
    "    axes[1, 1].bar(range(len(sensor_names_rev)), results['r2_temp'])\n",
    "    axes[1, 1].set_xlabel('Sensor')\n",
    "    axes[1, 1].set_ylabel('RÂ² Score')\n",
    "    axes[1, 1].set_title('RÂ² Score by Sensor (Temperature)')\n",
    "    axes[1, 1].set_xticks(range(len(sensor_names_rev)))\n",
    "    axes[1, 1].set_xticklabels(sensor_names_rev, rotation=45)\n",
    "    \n",
    "    # Additional conservation analysis (if violation rates provided)\n",
    "    if violation_rates is not None:\n",
    "        violation_stats = {\n",
    "            'Final Rate': violation_rates[-1] if violation_rates else 0,\n",
    "            'Max Rate': max(violation_rates) if violation_rates else 0,\n",
    "            'Avg Rate': np.mean(violation_rates) if violation_rates else 0,\n",
    "            'Min Rate': min(violation_rates) if violation_rates else 0\n",
    "        }\n",
    "        \n",
    "        bars = axes[1, 2].bar(violation_stats.keys(), violation_stats.values())\n",
    "        axes[1, 2].set_ylabel('Violation Rate')\n",
    "        axes[1, 2].set_title('Conservation Violation Statistics')\n",
    "        axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        for i, (key, value) in enumerate(violation_stats.items()):\n",
    "            if value > 0.05:\n",
    "                bars[i].set_color('red')\n",
    "            elif value > 0.01:\n",
    "                bars[i].set_color('orange')\n",
    "            else:\n",
    "                bars[i].set_color('green')\n",
    "        \n",
    "        for i, (key, value) in enumerate(violation_stats.items()):\n",
    "            axes[1, 2].text(i, value + max(violation_stats.values()) * 0.01, \n",
    "                           f'{value:.4f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if violation_rates is not None and len(violation_rates) > 0:\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"CONSERVATION MONITORING SUMMARY\")\n",
    "        print(\"=\"*40)\n",
    "        final_rate = violation_rates[-1]\n",
    "        max_rate = max(violation_rates)\n",
    "        avg_rate = np.mean(violation_rates)\n",
    "        \n",
    "        print(f\"Final violation rate: {final_rate:.4f} ({final_rate*100:.2f}%)\")\n",
    "        print(f\"Maximum violation rate: {max_rate:.4f} ({max_rate*100:.2f}%)\")\n",
    "        print(f\"Average violation rate: {avg_rate:.4f} ({avg_rate*100:.2f}%)\")\n",
    "        \n",
    "        if final_rate < 0.01:\n",
    "            print(\"âœ… EXCELLENT: Energy conservation successfully learned!\")\n",
    "        elif final_rate < 0.05:\n",
    "            print(\"âš ï¸  WARNING: Moderate violation rate. Consider tuning physics_weight.\")\n",
    "        else:\n",
    "            print(\"âŒ CRITICAL: High violation rate. Increase conservation_violation_penalty!\")\n",
    "        print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3c3333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8: Inference Functions\n",
    "def predict_temperature(model, X_scaler, y_scaler, time_range_data, column_info, \n",
    "                       time, h, flux, abs_val, surf, theoretical_temps, device):\n",
    "    \"\"\"\n",
    "    Inference function to predict actual temperatures from input parameters\n",
    "    \"\"\"\n",
    "    # Validate inputs - Updated to be dynamic based on the model's expected features\n",
    "    theoretical_temp_cols = [col for col in column_info['feature_cols'] if col.startswith('Theoretical_Temps_')]\n",
    "    expected_num_theoretical = len(theoretical_temp_cols)\n",
    "\n",
    "    if len(theoretical_temps) != expected_num_theoretical:\n",
    "        raise ValueError(\"Expected {} theoretical temperatures, got {}\".format(\n",
    "            expected_num_theoretical, len(theoretical_temps)))\n",
    "\n",
    "\n",
    "    # Get time normalization parameters\n",
    "    time_min = time_range_data['time_min']\n",
    "    time_max = time_range_data['time_max']\n",
    "    \n",
    "    # Normalize time\n",
    "    time_norm = (time - time_min) / (time_max - time_min)\n",
    "    \n",
    "    # Create enhanced time features\n",
    "    time_squared = time_norm ** 2\n",
    "    time_cubed = time_norm ** 3\n",
    "    time_sin = np.sin(2 * np.pi * time_norm)\n",
    "    time_cos = np.cos(2 * np.pi * time_norm)\n",
    "    \n",
    "    # Create interaction features\n",
    "    flux_abs_interaction = flux * abs_val\n",
    "    h_flux_interaction = h * flux\n",
    "    \n",
    "    # Prepare input features\n",
    "    input_features = [\n",
    "        time_norm, time_squared, time_cubed, time_sin, time_cos,\n",
    "        h, flux, abs_val, surf, flux_abs_interaction, h_flux_interaction\n",
    "    ]\n",
    "    input_features.extend(theoretical_temps)\n",
    "    \n",
    "    # Convert to numpy array and reshape\n",
    "    input_array = np.array(input_features).reshape(1, -1)\n",
    "    \n",
    "    # Scale the input features\n",
    "    input_scaled = X_scaler.transform(input_array)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction_scaled = model(input_tensor).cpu().numpy()\n",
    "    \n",
    "    # Inverse transform the prediction\n",
    "    prediction_real = y_scaler.inverse_transform(prediction_scaled)\n",
    "    \n",
    "    # Get TC column names\n",
    "    tc_cols = column_info['tc_cols']\n",
    "    \n",
    "    # Create result dictionary\n",
    "    result = {\n",
    "        'input_parameters': {\n",
    "            'time': time,\n",
    "            'h': h,\n",
    "            'flux': flux,\n",
    "            'abs': abs_val,\n",
    "            'surf': surf,\n",
    "            'theoretical_temps': theoretical_temps\n",
    "        },\n",
    "        'predicted_temperatures': {}\n",
    "    }\n",
    "    \n",
    "    # Map predictions to TC sensor names\n",
    "    for i, tc_name in enumerate(tc_cols):\n",
    "        result['predicted_temperatures'][tc_name] = float(prediction_real[0, i])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def load_inference_components():\n",
    "    \"\"\"Load all necessary components for inference\"\"\"\n",
    "    try:\n",
    "        # Load scalers and metadata\n",
    "        X_scaler = joblib.load(\"X_scaler_enhanced.pkl\")\n",
    "        y_scaler = joblib.load(\"y_scaler_enhanced.pkl\")\n",
    "        time_range_data = joblib.load(\"time_range_enhanced.pkl\")\n",
    "        column_info = joblib.load(\"column_info.pkl\")\n",
    "        \n",
    "        # Set device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load model\n",
    "        checkpoint = torch.load('best_thermal_model_enhanced.pth', map_location=device)\n",
    "        \n",
    "        # Recreate model\n",
    "        input_size = len(column_info['feature_cols'])\n",
    "        output_size = len(column_info['tc_cols'])\n",
    "        \n",
    "        model = EnhancedThermalNet(\n",
    "            input_size=input_size,\n",
    "            output_size=output_size,\n",
    "            hidden_dims=[512, 256, 256, 128],\n",
    "            dropout_rate=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        # Load trained weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        print(\"Inference components loaded successfully!\")\n",
    "        print(f\"Model input size: {input_size}\")\n",
    "        print(f\"Model output size: {output_size}\")\n",
    "        print(f\"TC sensors: {column_info['tc_cols']}\")\n",
    "        \n",
    "        return model, X_scaler, y_scaler, time_range_data, column_info, device\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading inference components: {e}\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "def example_inference():\n",
    "    \"\"\"Example of how to use the inference function\"\"\"\n",
    "    # Load inference components\n",
    "    model, X_scaler, y_scaler, time_range_data, column_info, device = load_inference_components()\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"Failed to load inference components. Make sure the model is trained and saved.\")\n",
    "        return\n",
    "    \n",
    "    # Example input parameters\n",
    "    time = 0  # 30 minutes\n",
    "    h = 0.1575   # Heat transfer coefficient\n",
    "    flux = 25900  # Heat flux\n",
    "    abs_val = 20  # Absorption coefficient\n",
    "    surf = 0.98   # Surface emissivity\n",
    "    \n",
    "    # Example theoretical temperatures (10 values)\n",
    "    theoretical_temps = [322.346598107413,344.707379405421,344.707598403347,342.463078051269,332.928870144283,324.216781541098,318.02660925491,315.821244548393,315.821244548393,315.821244548393]\n",
    "    \n",
    "    try:\n",
    "        # Make prediction\n",
    "        result = predict_temperature(\n",
    "            model, X_scaler, y_scaler, time_range_data, column_info,\n",
    "            time, h, flux, abs_val, surf, theoretical_temps, device\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TEMPERATURE PREDICTION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Time: {result['input_parameters']['time']} seconds\")\n",
    "        print(f\"Heat transfer coefficient (h): {result['input_parameters']['h']}\")\n",
    "        print(f\"Heat flux: {result['input_parameters']['flux']}\")\n",
    "        print(f\"Absorption coefficient: {result['input_parameters']['abs']}\")\n",
    "        print(f\"Surface emissivity: {result['input_parameters']['surf']}\")\n",
    "        print(\"\\nPredicted TC Temperatures:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for tc_name, temp in result['predicted_temperatures'].items():\n",
    "            print(f\"{tc_name}: {temp:.2f} Â°C\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "        return None\n",
    "\n",
    "def batch_predict_temperature(model, X_scaler, y_scaler, time_range_data, column_info, \n",
    "                            input_data, device):\n",
    "    \"\"\"Batch inference function for multiple predictions\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for data_point in input_data:\n",
    "        try:\n",
    "            result = predict_temperature(\n",
    "                model, X_scaler, y_scaler, time_range_data, column_info,\n",
    "                data_point['time'], data_point['h'], data_point['flux'],\n",
    "                data_point['abs'], data_point['surf'], data_point['theoretical_temps'],\n",
    "                device\n",
    "            )\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data point: {e}\")\n",
    "            results.append(None)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79dc7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_distribution(results, tc_cols):\n",
    "    \"\"\"\n",
    "    Plots a violin plot of the model's prediction errors (residuals),\n",
    "    ensuring the sensors are displayed in numerical order (TC1, TC2, ...).\n",
    "\n",
    "    Args:\n",
    "        results (dict): The dictionary returned by the evaluate_model function.\n",
    "        tc_cols (list): List of the TC sensor column names.\n",
    "    \"\"\"\n",
    "    print(\"\\nGenerating ordered error distribution plot...\")\n",
    "    \n",
    "    # --- 1. Ensure Correct Sorting ---\n",
    "    # Sort the column names to ensure they appear in order, e.g., TC1, TC2, ..., TC10.\n",
    "    # This is crucial for clear interpretation in a journal figure.\n",
    "    try:\n",
    "        # This \"natural sort\" correctly handles numbers in strings.\n",
    "        sorted_tc_cols = sorted(tc_cols, key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "    except (AttributeError, ValueError):\n",
    "        # If the above fails for any reason, fall back to a simple alphabetical sort.\n",
    "        print(\"Warning: Natural sort failed. Falling back to alphabetical sort for TC columns.\")\n",
    "        sorted_tc_cols = sorted(tc_cols)\n",
    "\n",
    "    # --- 2. Calculate and Organize Residuals ---\n",
    "    residuals = results['actuals'] - results['predictions']\n",
    "    \n",
    "    # Create a DataFrame using the original tc_cols to match the data\n",
    "    residuals_df = pd.DataFrame(residuals, columns=tc_cols)\n",
    "    \n",
    "    # Now, reorder the DataFrame columns according to our sorted list\n",
    "    residuals_df = residuals_df[sorted_tc_cols]\n",
    "\n",
    "    # --- 3. Plotting ---\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    \n",
    "    sns.violinplot(data=residuals_df, inner='quartile', palette='viridis', cut=0)\n",
    "    plt.axhline(0, color='red', linestyle='--', linewidth=1.5, label='Zero Error')\n",
    "    \n",
    "    plt.title('Model Prediction Error Distribution by Sensor', fontsize=18, weight='bold')\n",
    "    plt.xlabel('Thermocouple Sensor Location', fontsize=14)\n",
    "    plt.ylabel('Prediction Error (Â°C) [Actual - Predicted]', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(\"figure_error_distribution_ordered.png\", dpi=300)\n",
    "    print(\"âœ“ Ordered plot saved as 'figure_error_distribution_ordered.png'\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b03d2535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data directory: data/TC11data\n",
      "Loading data from: data/TC11data\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv (566 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv (621 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv (826 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv (795 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv (932 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv (912 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv (1087 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv (790 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv (585 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv (611 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv (554 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv (589 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv (970 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv (803 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv (1411 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv (787 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv (531 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv (865 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv (550 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv (887 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv (790 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv (794 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv (4087 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv (849 rows)\n",
      "\n",
      "==================================================\n",
      "FILE PROCESSING SUMMARY\n",
      "==================================================\n",
      "Total files scanned     : 25\n",
      "Files successfully loaded: 25\n",
      "Files skipped (filtered): 0\n",
      "Files skipped (unmatched): 0\n",
      "==================================================\n",
      "\n",
      "Total data loaded: 22706 rows, 27 columns\n",
      "Starting data preprocessing...\n",
      "Features: 21, Targets: 11\n",
      "Theory columns: 10, TC columns: 11\n",
      "Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10', 'TC11']\n",
      "After removing missing values: 22706 samples\n",
      "Training samples: 15894\n",
      "Validation samples: 3406\n",
      "Test samples: 3406\n",
      "Using device: cpu\n",
      "Model created with 409678 parameters\n",
      "Starting training with professor's energy conservation formula...\n",
      "Physics loss weight: 0.5\n",
      "Conservation violation penalty: 100.0\n",
      "PHYSICS: Energy_stored = rho * h * A_rec * cp * temp_change / dt\n",
      "PHYSICS: Energy_incoming = flux * A_rec\n",
      "CONSTRAINT: Energy_stored â‰¤ Energy_incoming\n",
      "Epoch [  10/1000] Train: 0.004995 Val: 0.003083 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  20/1000] Train: 0.002535 Val: 0.001625 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  30/1000] Train: 0.001705 Val: 0.001217 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  40/1000] Train: 0.001225 Val: 0.000968 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  50/1000] Train: 0.001071 Val: 0.000670 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [ 100/1000] Train: 0.000929 Val: 0.000518 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [ 200/1000] Train: 0.000653 Val: 0.000370 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00025000\n",
      "Epoch [ 300/1000] Train: 0.000609 Val: 0.000349 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00003125\n",
      "Epoch [ 400/1000] Train: 0.000605 Val: 0.000351 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00000781\n",
      "Early stopping at epoch 442\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "============================================================\n",
      "Best validation loss: 0.000346\n",
      "Final violation rate: 0.0000\n",
      "ðŸŽ‰ EXCELLENT: Physics constraints well learned!\n",
      "   Model respects energy conservation in >99% of cases\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS (Temperature Â°C)\n",
      "==================================================\n",
      "Sensor          RMSE (Â°C)    MAE (Â°C)     MAPE (%)     RÂ² Score    \n",
      "-----------------------------------------------------------------\n",
      "TC11            1.582        0.871        0.30         0.999       \n",
      "TC10            1.133        0.630        0.18         0.995       \n",
      "TC9             0.661        0.485        0.13         0.995       \n",
      "TC8             0.403        0.254        0.07         0.997       \n",
      "TC7             0.382        0.245        0.07         0.998       \n",
      "TC6             0.348        0.243        0.07         0.998       \n",
      "TC5             0.322        0.221        0.06         0.998       \n",
      "TC4             0.301        0.192        0.05         0.999       \n",
      "TC3             0.285        0.177        0.05         0.999       \n",
      "TC2             0.298        0.215        0.06         0.999       \n",
      "TC1             0.339        0.237        0.07         0.998       \n",
      "-----------------------------------------------------------------\n",
      "Overall         0.683        0.343                     0.998       \n",
      "\n",
      "==================================================\n",
      "PHYSICS CONSERVATION MONITORING\n",
      "==================================================\n",
      "Final energy conservation violation rate: 0.0000\n",
      "âœ… Energy conservation constraint successfully learned!\n",
      "\n",
      "Generating ordered error distribution plot...\n",
      "âœ“ Ordered plot saved as 'figure_error_distribution_ordered.png'\n"
     ]
    }
   ],
   "source": [
    "# Block 9: Main Execution [Best working]\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        # Load data (with h6 filter like in your original code)\n",
    "        DATA_DIR = get_data_directory()\n",
    "        data = load_data(DATA_DIR)  # Only h6 = 0.1575\n",
    "        \n",
    "        # Preprocess data\n",
    "        train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols = preprocess_data(\n",
    "            data, use_enhanced_features=True\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        model = EnhancedThermalNet(\n",
    "            input_size=train_loader.dataset.tensors[0].shape[1],\n",
    "            output_size=train_loader.dataset.tensors[1].shape[1],\n",
    "            hidden_dims=[512, 256, 256, 128],\n",
    "            dropout_rate=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "        \n",
    "        # Train model with enhanced conservation monitoring\n",
    "        train_losses, val_losses, violation_rates = train_model_with_conservation_monitoring(\n",
    "            model, train_loader, val_loader, device, epochs=1000, patience=50\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        sensor_depths = np.array([0.0, 0.018, 0.035, 0.053, 0.070, 0.099, 0.105, 0.123, 0.140, 0.158])\n",
    "        # sensor names as TC1 deepest -> TC10 surface (reverse for display in evaluate_model)\n",
    "        sensor_names = [f\"TC{i}\" for i in range(1, 12)] \n",
    "        # results = evaluate_model(model, test_loader, y_scaler, device)\n",
    "        results = evaluate_model(model, test_loader, y_scaler, device, sensor_depths, sensor_names)\n",
    "\n",
    "\n",
    "       # Use reversed sensor names in print because evaluate_model returns reversed metrics for display\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EVALUATION RESULTS (Temperature Â°C)\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"{'Sensor':<15} {'RMSE (Â°C)':<12} {'MAE (Â°C)':<12} {'MAPE (%)':<12} {'RÂ² Score':<12}\")\n",
    "        print(\"-\"*65)\n",
    "        for i, name in enumerate(results['sensor_names_rev']):\n",
    "            print(f\"{name:<15} {results['rmse_temp'][i]:<12.3f} {results['mae_temp'][i]:<12.3f} \"\n",
    "                  f\"{results['mape_temp'][i]:<12.2f} {results['r2_temp'][i]:<12.3f}\")\n",
    "        print(\"-\"*65)\n",
    "        print(f\"{'Overall':<15} {results['overall_rmse_temp']:<12.3f} {results['overall_mae_temp']:<12.3f} \"\n",
    "              f\"{'':<12} {results['overall_r2_temp']:<12.3f}\")\n",
    "        \n",
    "        # print(\"\\n\" + \"=\"*50)\n",
    "        # print(\"EVALUATION RESULTS (Energy stored, Watts)\")\n",
    "        # print(\"=\"*58)\n",
    "        # print(f\"{'Sensor':<15} {'RMSE (W)':<15} {'MAE (W)':<15} {'MAPE (%)':<12} {'RÂ² Score':<12}\")\n",
    "        # print(\"-\"*58)\n",
    "        # for i, name in enumerate(results['sensor_names_rev']):\n",
    "        #     print(f\"{name:<15} {results['rmse_energy'][i]:<15.3f} {results['mae_energy'][i]:<15.3f} \"\n",
    "        #           f\"{results['mape_energy'][i]:<12.2f} {results['r2_energy'][i]:<12.3f}\")\n",
    "        # print(\"-\"*58)\n",
    "        # print(f\"{'Overall':<15} {results['overall_rmse_energy']:<15.3f} {results['overall_mae_energy']:<15.3f} \"\n",
    "        #       f\"{'':<12} {results['overall_r2_energy']:<12.3f}\")\n",
    "        \n",
    "        # Physics conservation monitoring\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"PHYSICS CONSERVATION MONITORING\")\n",
    "        print(\"=\"*50)\n",
    "        final_violation_rate = violation_rates[-1] if violation_rates else 0.0\n",
    "        print(f\"Final energy conservation violation rate: {final_violation_rate:.4f}\")\n",
    "        if final_violation_rate > 0.01:\n",
    "            print(\"âš ï¸  WARNING: High violation rate detected!\")\n",
    "            print(\"   Consider increasing conservation_violation_penalty or physics_weight\")\n",
    "        else:\n",
    "            print(\"âœ… Energy conservation constraint successfully learned!\")\n",
    "\n",
    "        if 'overall_mean_energy' in results:\n",
    "            relative_rmse = results['overall_rmse_energy'] / results['overall_mean_energy'] * 100\n",
    "            print(f\"\\nRelative RMSE error in energy stored: {relative_rmse:.2f}%\")\n",
    "        \n",
    "        # plot_results(train_losses, val_losses, results, tc_cols, violation_rates)\n",
    "        \n",
    "        return model, results, X_scaler, y_scaler, tc_cols, violation_rates\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, results, X_scaler, y_scaler, tc_cols, violation_rates = main()\n",
    "\n",
    "    if results and tc_cols:\n",
    "        plot_error_distribution(results, tc_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_vertical_profile(predicted, actual, total_height, filename=\"Sample Profile\"): #for h2\n",
    "#     \"\"\"\n",
    "#     This is the final, fully dynamic plotting function. It adapts to any\n",
    "#     number of sensors and uses the stylish colored label boxes.\n",
    "#     \"\"\"\n",
    "#     num_sensors = len(actual)\n",
    "#     if num_sensors == 0:\n",
    "#         print(f\"Warning: Cannot plot profile for {filename}, no data provided.\")\n",
    "#         return\n",
    "\n",
    "#     predicted_plot = predicted[::-1]\n",
    "#     actual_plot = actual[::-1]\n",
    "#     sensor_labels = [f\"TC{i}\" for i in range(num_sensors, 0, -1)]\n",
    "\n",
    "#     if num_sensors > 1:\n",
    "#         spacing = total_height / (num_sensors - 1)\n",
    "#         depths = [0 - i * spacing for i in range(num_sensors)]\n",
    "#     else:\n",
    "#         spacing = 0\n",
    "#         depths = [0]\n",
    "    \n",
    "#     depths_plot = depths[::-1]\n",
    "\n",
    "#     plt.figure(figsize=(7, 6))\n",
    "#     plt.plot(actual_plot, depths_plot, label=\"Actual\", color='blue', marker='o', linewidth=2)\n",
    "#     plt.plot(predicted_plot, depths_plot, label=\"Predicted\", color='red', marker='x', linestyle='--', linewidth=2)\n",
    "\n",
    "#     plt.gca().invert_yaxis()\n",
    "#     plt.yticks(depths_plot, [f\"{d:.3f}\" for d in depths_plot])\n",
    "#     if num_sensors > 1:\n",
    "#         plt.ylim(min(depths_plot) - spacing, max(depths_plot) + spacing)\n",
    "\n",
    "#     plt.xlabel(\"Temperature (Â°C)\")\n",
    "#     plt.ylabel(\"Depth (m)\")\n",
    "#     plt.title(f\"Vertical Profile: {filename}\")\n",
    "#     plt.legend(loc='upper right')\n",
    "\n",
    "#     for i, label in enumerate(sensor_labels):\n",
    "#         plt.text(\n",
    "#             actual_plot[i], depths_plot[i], label,\n",
    "#             ha='right', va='center', fontsize=8,\n",
    "#             bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#cce6ff\", ec=\"navy\", lw=1, alpha=0.7)\n",
    "#         )\n",
    "#         plt.text(\n",
    "#             predicted_plot[i], depths_plot[i], label,\n",
    "#             ha='left', va='center', fontsize=8,\n",
    "#             bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#ffcccc\", ec=\"darkred\", lw=1, alpha=0.7)\n",
    "#         )\n",
    "\n",
    "#     plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"{filename}.png\", dpi=300, bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_vertical_profile(predicted, actual, total_height, filename=\"Sample Profile\"): #for h3\n",
    "#     \"\"\"\n",
    "#     This version is fully dynamic AND includes the colored annotation boxes\n",
    "#     for the sensor labels, just like your original plot.\n",
    "#     \"\"\"\n",
    "#     # Determine the number of sensors from the length of the input data\n",
    "#     num_sensors = len(actual)\n",
    "    \n",
    "#     if num_sensors == 0:\n",
    "#         print(f\"Warning: Cannot plot profile for {filename}, no data provided.\")\n",
    "#         return\n",
    "\n",
    "#     # Reverse order so the last sensor (surface) is at the top for plotting\n",
    "#     predicted_plot = predicted[::-1]\n",
    "#     actual_plot = actual[::-1]\n",
    "    \n",
    "#     # Dynamically create labels based on the number of sensors being plotted\n",
    "#     sensor_labels = [f\"TC{i}\" for i in range(num_sensors, 0, -1)]\n",
    "\n",
    "#     # Dynamically calculate depths based on the number of sensors and total height\n",
    "#     if num_sensors > 1:\n",
    "#         spacing = total_height / (num_sensors - 1)\n",
    "#         depths = [0 - i * spacing for i in range(num_sensors)]\n",
    "#     else:\n",
    "#         spacing = 0\n",
    "#         depths = [0]\n",
    "    \n",
    "#     depths_plot = depths[::-1]\n",
    "\n",
    "#     # --- Plotting Logic ---\n",
    "#     plt.figure(figsize=(7, 6))\n",
    "#     plt.plot(actual_plot, depths_plot, label=\"Actual\", color='blue', marker='o', linewidth=2)\n",
    "#     plt.plot(predicted_plot, depths_plot, label=\"Predicted\", color='red', marker='x', linestyle='--', linewidth=2)\n",
    "\n",
    "#     plt.gca().invert_yaxis()\n",
    "#     plt.yticks(depths_plot, [f\"{d:.3f}\" for d in depths_plot])\n",
    "#     if num_sensors > 1:\n",
    "#         plt.ylim(min(depths_plot) - spacing, max(depths_plot) + spacing)\n",
    "\n",
    "#     plt.xlabel(\"Temperature (Â°C)\")\n",
    "#     plt.ylabel(\"Depth (m)\")\n",
    "#     plt.title(f\"Vertical Profile: {filename}\")\n",
    "#     plt.legend(loc='upper right')\n",
    "\n",
    "#     # --- MODIFICATION: Restored the colored label boxes from your original code ---\n",
    "#     for i, label in enumerate(sensor_labels):\n",
    "#         # Blue box for the 'Actual' temperature label\n",
    "#         plt.text(\n",
    "#             actual_plot[i], depths_plot[i], label,\n",
    "#             ha='right', va='center', fontsize=8,\n",
    "#             bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#cce6ff\", ec=\"navy\", lw=1, alpha=0.7)\n",
    "#         )\n",
    "#         # Red box for the 'Predicted' temperature label\n",
    "#         plt.text(\n",
    "#             predicted_plot[i], depths_plot[i], label,\n",
    "#             ha='left', va='center', fontsize=8,\n",
    "#             bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#ffcccc\", ec=\"darkred\", lw=1, alpha=0.7)\n",
    "#         )\n",
    "#     # --- END OF MODIFICATION ---\n",
    "\n",
    "#     plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"{filename}.png\", dpi=300, bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49288a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vertical_profile(predicted, actual, filename=\"Sample Profile\"): #OGGGGG\n",
    "    # Reverse order so TC10 (surface) is at the top\n",
    "    predicted = predicted[::-1]\n",
    "    actual = actual[::-1]\n",
    "    sensor_labels = [f\"TC{i}\" for i in range(11, 0, -1)]  # TC10 to TC1\n",
    "\n",
    "    total_height = 0.1575  # Total receiver height in meters\n",
    "    spacing = total_height / 10\n",
    "    depths = [0 - i * spacing for i in range(11)]  # TC10 at 0.0, TC1 at -total_height\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.plot(actual, depths, label=\"Actual\", color='blue', marker='o', linewidth=2)\n",
    "    plt.plot(predicted, depths, label=\"Predicted\", color='red', marker='x', linestyle='--', linewidth=2)\n",
    "\n",
    "    plt.gca().invert_yaxis()  # So 0 (surface) is at top\n",
    "\n",
    "    # Set clean numeric y-ticks\n",
    "    plt.yticks(depths, [f\"{d:.3f}\" for d in depths])\n",
    "    plt.ylim(min(depths) - spacing * 0.5, max(depths) + spacing * 0.5)\n",
    "\n",
    "    plt.xlabel(\"Temperature (Â°C)\")\n",
    "    plt.ylabel(\"Depth (m)\")\n",
    "    plt.title(f\"Vertical Profile: {filename}\")\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Add sensor labels next to each point\n",
    "    for i, label in enumerate(sensor_labels):\n",
    "        plt.text(\n",
    "            actual[i], depths[i], label,\n",
    "            ha='right', va='center', fontsize=8,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#cce6ff\", ec=\"navy\", lw=1, alpha=0.7)\n",
    "        )\n",
    "        plt.text(\n",
    "            predicted[i], depths[i], label,\n",
    "            ha='left', va='center', fontsize=8,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#ffcccc\", ec=\"darkred\", lw=1, alpha=0.7)\n",
    "        )\n",
    "\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{filename}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7e3ec0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data directory: data/new_processed_reset\n",
      "\n",
      "=== Running Standalone Test Cross-Check ===\n",
      "Attempting to load pre-trained model and components...\n",
      "âœ“ Pre-trained model loaded successfully.\n",
      "Loading data from: data/new_processed_reset\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_699s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv (622 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv (576 rows)\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_surf0_630s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf1_675s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv (576 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs0_surf0_585s - Sheet2_processed.csv (564 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_surf0_598s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs25_surf0_439s-Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs0_surf0_719s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs0_surf0_644s-Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_749s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf0_429s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv (501 rows)\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs25_surf0_660s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_692s-Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs0_surf0_431s - Sheet1_processed.csv (560 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf1_830s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs25_surf0_493s - Sheet1_processed.csv (572 rows)\n",
      "Loaded: cleaned_h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv (548 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv (703 rows)\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf1_491s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs92_surf0_115s - Sheet1_processed.csv (500 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv (540 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_651s - Sheet4_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv (508 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_surf0_628s - Sheet4_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_389s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_389s - Sheet4_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs0_surf0_825s-Sheer3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf1_613s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_747s-Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv\n",
      "\n",
      "==================================================\n",
      "FILE PROCESSING SUMMARY\n",
      "==================================================\n",
      "Total files scanned     : 77\n",
      "Files successfully loaded: 12\n",
      "Files skipped (filtered): 65\n",
      "Files skipped (unmatched): 0\n",
      "==================================================\n",
      "\n",
      "Total data loaded: 6770 rows, 26 columns\n",
      "Starting data preprocessing...\n",
      "Features: 21, Targets: 10\n",
      "Theory columns: 10, TC columns: 10\n",
      "Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10']\n",
      "After removing missing values: 6770 samples\n",
      "Training samples: 4739\n",
      "Validation samples: 1015\n",
      "Test samples: 1016\n",
      "\n",
      "=== Cross-Checking 5 Test Samples ===\n",
      "\n",
      "--- Test Sample 814 ---\n",
      "TC1_tip         | Predicted: 372.523         | Actual: 366.910         | Error: 5.613          \n",
      "TC2             | Predicted: 371.032         | Actual: 364.540         | Error: 6.492          \n",
      "TC3             | Predicted: 326.321         | Actual: 342.470         | Error: 16.149         \n",
      "TC4             | Predicted: 207.602         | Actual: 212.130         | Error: 4.528          \n",
      "TC5             | Predicted: 145.459         | Actual: 153.560         | Error: 8.101          \n",
      "TC6             | Predicted: 103.849         | Actual: 104.750         | Error: 0.901          \n",
      "TC7             | Predicted: 84.632          | Actual: 86.870          | Error: 2.238          \n",
      "TC8             | Predicted: 79.693          | Actual: 73.870          | Error: 5.823          \n",
      "TC9             | Predicted: 94.535          | Actual: 76.790          | Error: 17.745         \n",
      "TC10            | Predicted: 94.582          | Actual: 99.200          | Error: 4.618          \n",
      "\n",
      "--- Test Sample 949 ---\n",
      "TC1_tip         | Predicted: 362.242         | Actual: 379.450         | Error: 17.208         \n",
      "TC2             | Predicted: 363.884         | Actual: 377.720         | Error: 13.836         \n",
      "TC3             | Predicted: 352.791         | Actual: 357.950         | Error: 5.159          \n",
      "TC4             | Predicted: 219.954         | Actual: 226.040         | Error: 6.086          \n",
      "TC5             | Predicted: 161.634         | Actual: 170.790         | Error: 9.156          \n",
      "TC6             | Predicted: 119.851         | Actual: 129.350         | Error: 9.499          \n",
      "TC7             | Predicted: 90.221          | Actual: 114.240         | Error: 24.019         \n",
      "TC8             | Predicted: 90.656          | Actual: 107.240         | Error: 16.584         \n",
      "TC9             | Predicted: 110.489         | Actual: 125.910         | Error: 15.421         \n",
      "TC10            | Predicted: 50.364          | Actual: 160.970         | Error: 110.606        \n",
      "\n",
      "--- Test Sample 998 ---\n",
      "TC1_tip         | Predicted: 368.025         | Actual: 370.680         | Error: 2.655          \n",
      "TC2             | Predicted: 366.097         | Actual: 370.440         | Error: 4.343          \n",
      "TC3             | Predicted: 320.262         | Actual: 347.450         | Error: 27.188         \n",
      "TC4             | Predicted: 193.579         | Actual: 214.470         | Error: 20.891         \n",
      "TC5             | Predicted: 136.942         | Actual: 156.390         | Error: 19.448         \n",
      "TC6             | Predicted: 97.118          | Actual: 111.770         | Error: 14.652         \n",
      "TC7             | Predicted: 79.803          | Actual: 94.890          | Error: 15.087         \n",
      "TC8             | Predicted: 72.751          | Actual: 83.190          | Error: 10.439         \n",
      "TC9             | Predicted: 78.156          | Actual: 89.600          | Error: 11.444         \n",
      "TC10            | Predicted: 73.620          | Actual: 113.750         | Error: 40.130         \n",
      "\n",
      "--- Test Sample 299 ---\n",
      "TC1_tip         | Predicted: 364.950         | Actual: 375.210         | Error: 10.260         \n",
      "TC2             | Predicted: 367.449         | Actual: 372.990         | Error: 5.541          \n",
      "TC3             | Predicted: 344.785         | Actual: 352.650         | Error: 7.865          \n",
      "TC4             | Predicted: 216.436         | Actual: 221.670         | Error: 5.234          \n",
      "TC5             | Predicted: 162.704         | Actual: 166.910         | Error: 4.206          \n",
      "TC6             | Predicted: 120.839         | Actual: 124.500         | Error: 3.661          \n",
      "TC7             | Predicted: 90.853          | Actual: 107.520         | Error: 16.667         \n",
      "TC8             | Predicted: 86.071          | Actual: 101.370         | Error: 15.299         \n",
      "TC9             | Predicted: 109.783         | Actual: 120.930         | Error: 11.147         \n",
      "TC10            | Predicted: 63.367          | Actual: 158.420         | Error: 95.053         \n",
      "\n",
      "--- Test Sample 800 ---\n",
      "TC1_tip         | Predicted: 365.985         | Actual: 370.480         | Error: 4.495          \n",
      "TC2             | Predicted: 363.432         | Actual: 367.400         | Error: 3.968          \n",
      "TC3             | Predicted: 321.777         | Actual: 344.390         | Error: 22.613         \n",
      "TC4             | Predicted: 205.021         | Actual: 214.330         | Error: 9.309          \n",
      "TC5             | Predicted: 152.805         | Actual: 156.800         | Error: 3.995          \n",
      "TC6             | Predicted: 110.322         | Actual: 110.720         | Error: 0.398          \n",
      "TC7             | Predicted: 76.368          | Actual: 94.830          | Error: 18.462         \n",
      "TC8             | Predicted: 72.420          | Actual: 80.530          | Error: 8.110          \n",
      "TC9             | Predicted: 98.315          | Actual: 79.880          | Error: 18.435         \n",
      "TC10            | Predicted: 62.698          | Actual: 97.440          | Error: 34.742         \n",
      "\n",
      "--- Generating 5 Plots ---\n",
      "âœ“ Plot saved for sample 814\n",
      "âœ“ Plot saved for sample 949\n",
      "âœ“ Plot saved for sample 998\n",
      "âœ“ Plot saved for sample 299\n",
      "âœ“ Plot saved for sample 800\n"
     ]
    }
   ],
   "source": [
    "# # Block 11: Standalone Testing (MODIFIED)\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import re # Import the regex library for sorting\n",
    "\n",
    "# def cross_check_test_predictions(data_dir, h_filter=0.084, num_samples=5, filter_condition=None):\n",
    "#     \"\"\"Cross-check model predictions with rows from the test dataset.\"\"\"\n",
    "#     print(\"Attempting to load pre-trained model and components...\")\n",
    "#     global pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device\n",
    "    \n",
    "#     if 'pred_model' not in globals() or pred_model is None:\n",
    "#         pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device = load_for_prediction()\n",
    "\n",
    "#     if pred_model is None:\n",
    "#         print(\"\\nERROR: Could not load pre-trained model. Please run the main training script first.\")\n",
    "#         raise ValueError(\"Could not load pre-trained model for testing.\")\n",
    "#     else:\n",
    "#         print(\"âœ“ Pre-trained model loaded successfully.\")\n",
    "\n",
    "#     data = load_data(data_dir, h_filter=h_filter)\n",
    "#     train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols = preprocess_data(data, use_enhanced_features=True)\n",
    "    \n",
    "#     # --- FIX: Sort the TC columns to ensure consistent order ---\n",
    "#     if tc_cols:\n",
    "#         tc_cols = sorted(tc_cols, key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "#     # --- END OF FIX ---\n",
    "\n",
    "#     X_test_scaled = np.concatenate([batch[0].numpy() for batch in test_loader], axis=0)\n",
    "#     X_test = X_scaler.inverse_transform(X_test_scaled)\n",
    "    \n",
    "#     feature_cols = X_scaler.feature_names_in_\n",
    "#     test_df = pd.DataFrame(X_test, columns=feature_cols)\n",
    "#     test_df[tc_cols] = y_scaler.inverse_transform(np.concatenate([batch[1].numpy() for batch in test_loader], axis=0))\n",
    "    \n",
    "#     time_min = pred_time_range['time_min'] if pred_time_range else 0\n",
    "#     time_max = pred_time_range['time_max'] if pred_time_range else 7200\n",
    "#     test_df['Time'] = test_df['Time_norm'] * (time_max - time_min) + time_min\n",
    "    \n",
    "#     if filter_condition:\n",
    "#         test_df = test_df.query(filter_condition)\n",
    "    \n",
    "#     num_samples = min(num_samples, len(test_df))\n",
    "#     if num_samples == 0:\n",
    "#         raise ValueError(\"No test samples available after filtering!\")\n",
    "#     sample_rows = test_df.sample(n=num_samples, random_state=33) if filter_condition is None else test_df.head(num_samples)\n",
    "    \n",
    "#     print(f\"\\n=== Cross-Checking {len(sample_rows)} Test Samples ===\")\n",
    "#     results = []\n",
    "    \n",
    "#     for idx, row in sample_rows.iterrows():\n",
    "#         try:\n",
    "#             time = row['Time']\n",
    "#             h = row['h']\n",
    "#             flux = row['flux']\n",
    "#             abs_val = row['abs']\n",
    "#             surf = row['surf']\n",
    "#             theoretical_temps = [row[col] for col in feature_cols if col.startswith('Theoretical_Temps_')]\n",
    "            \n",
    "#             input_features = row[feature_cols].values.reshape(1, -1)\n",
    "#             input_scaled = X_scaler.transform(input_features)\n",
    "#             input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(pred_device)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 prediction_scaled = pred_model(input_tensor).cpu().numpy()\n",
    "            \n",
    "#             prediction_real = y_scaler.inverse_transform(prediction_scaled)\n",
    "            \n",
    "#             pred_temps_dict = {col: prediction_real[0, i] for i, col in enumerate(tc_cols)}\n",
    "#             actual_temps = {col: row[col] for col in tc_cols}\n",
    "            \n",
    "#             comparison = {\n",
    "#                 'index': idx,\n",
    "#                 'inputs': {'time': time, 'h': h, 'flux': flux, 'abs': abs_val, 'surf': surf},\n",
    "#                 'predicted_temps': pred_temps_dict,\n",
    "#                 'actual_temps': actual_temps,\n",
    "#                 'errors': {col: abs(pred_temps_dict[col] - actual_temps[col]) for col in tc_cols}\n",
    "#             }\n",
    "#             results.append(comparison)\n",
    "            \n",
    "#             print(f\"\\n--- Test Sample {idx} ---\")\n",
    "#             for col in tc_cols:\n",
    "#                 print(f\"{col:<15} | Predicted: {pred_temps_dict[col]:<15.3f} | Actual: {actual_temps[col]:<15.3f} | Error: {comparison['errors'][col]:<15.3f}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing test sample {idx}: {e}\")\n",
    "\n",
    "#     return results, tc_cols\n",
    "\n",
    "# def run_test_cross_check(data_dir, h_filter=0.084, num_samples=5, filter_condition=None):\n",
    "#     \"\"\"Run cross-checking on test data without requiring main execution.\"\"\"\n",
    "#     try:\n",
    "#         print(\"\\n=== Running Standalone Test Cross-Check ===\")\n",
    "#         results, tc_cols = cross_check_test_predictions(data_dir, h_filter, num_samples, filter_condition)\n",
    "        \n",
    "#         print(f\"\\n--- Generating {len(results)} Plots ---\")\n",
    "#         for result in results:\n",
    "#             try:\n",
    "#                 # This gets the full list of 10 temperatures\n",
    "#                 full_predicted = [result['predicted_temps'][col] for col in tc_cols]\n",
    "#                 full_actual = [result['actual_temps'][col] for col in tc_cols]\n",
    "\n",
    "#                 # --- SLICE THE DATA to get only TC1-TC5 ---\n",
    "#                 predicted_to_plot = full_predicted[:3]\n",
    "#                 actual_to_plot = full_actual[:3]\n",
    "                \n",
    "#                 # The height for 5 sensors on an h3 material is 0.084\n",
    "#                 height_for_5_sensors = 0.084 \n",
    "                \n",
    "#                 filename = (f\"h{h_filter}_flux{result['inputs']['flux']}_\"\n",
    "#                             f\"abs{result['inputs']['abs']}_surf{result['inputs']['surf']}_\"\n",
    "#                             f\"time{int(result['inputs']['time'])}s\")\n",
    "                \n",
    "#                 # Call the plotting function with the sliced data and correct height\n",
    "#                 plot_vertical_profile(predicted_to_plot, actual_to_plot, height_for_5_sensors, filename=f\"Sample_{result.get('index')}_{filename}\")\n",
    "                \n",
    "#                 print(f\"âœ“ Plot saved for sample {result.get('index')}\")\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"âœ— Error generating plot for sample {result.get('index')}: {e}\")\n",
    "        \n",
    "#         return results\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in standalone test cross-check: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     DATA_DIR = get_data_directory()\n",
    "#     #run_test_cross_check(DATA_DIR, h_filter=0.084, num_samples=5)\n",
    "#     run_test_cross_check(DATA_DIR, h_filter=h_map[2], num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vertical_profile_old(predicted, actual, filename=\"Sample Profile\"): #OGGG\n",
    "    # Reverse order so TC10 (surface) is at the top\n",
    "    predicted = predicted[::-1]\n",
    "    actual = actual[::-1]\n",
    "    sensor_labels = [f\"TC{i}\" for i in range(11, 0, -1)]  # TC10 to TC1\n",
    "\n",
    "    total_height = 0.1575  # Total receiver height in meters\n",
    "    spacing = total_height / 10\n",
    "    depths = [0 - i * spacing for i in range(11)]  # TC10 at 0.0, TC1 at -total_height\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.plot(actual, depths, label=\"Actual\", color='blue', marker='o', linewidth=2)\n",
    "    plt.plot(predicted, depths, label=\"Predicted\", color='red', marker='x', linestyle='--', linewidth=2)\n",
    "\n",
    "    plt.gca().invert_yaxis()  # So 0 (surface) is at top\n",
    "\n",
    "    # Set clean numeric y-ticks\n",
    "    plt.yticks(depths, [f\"{d:.3f}\" for d in depths])\n",
    "    plt.ylim(min(depths) - spacing * 0.5, max(depths) + spacing * 0.5)\n",
    "\n",
    "    plt.xlabel(\"Temperature (Â°C)\")\n",
    "    plt.ylabel(\"Depth (m)\")\n",
    "    plt.title(f\"Vertical Profile: {filename}\")\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Add sensor labels next to each point\n",
    "    for i, label in enumerate(sensor_labels):\n",
    "        plt.text(\n",
    "            actual[i], depths[i], label,\n",
    "            ha='right', va='center', fontsize=8,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#cce6ff\", ec=\"navy\", lw=1, alpha=0.7)\n",
    "        )\n",
    "        plt.text(\n",
    "            predicted[i], depths[i], label,\n",
    "            ha='left', va='center', fontsize=8,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#ffcccc\", ec=\"darkred\", lw=1, alpha=0.7)\n",
    "        )\n",
    "\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{filename}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3363f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vertical_profile(predicted, actual, filename=\"Sample Profile\"):\n",
    "    \"\"\"\n",
    "    Generate temperature profile plots with the new design but keeping old parameters.\n",
    "    Shows 11 thermocouples (TC1 to TC11).\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Number of sensors (11 for TC1 to TC11)\n",
    "    num_sensors = 11\n",
    "    \n",
    "    # Get the h value from your typical setup (assuming h=0.1575 for TC11)\n",
    "    h = 0.1575  # Total receiver height in meters\n",
    "    \n",
    "    # Calculate physical depths for 11 sensors evenly spaced\n",
    "    physical_depths = np.linspace(0, h, num_sensors)  # From surface (0) to bottom (h)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 10))\n",
    "    \n",
    "    # Plot actual and predicted temperatures\n",
    "    ax.plot(actual, physical_depths, 'o-', label='Actual', color='blue', markersize=6, linewidth=2)\n",
    "    ax.plot(predicted, physical_depths, 's--', label='Predicted', color='red', markersize=5, linewidth=2)\n",
    "    \n",
    "    # Add sensor labels for each thermocouple\n",
    "    for i in range(num_sensors):\n",
    "        sensor_label = f'TC{i+1}'\n",
    "        # Position labels near the actual temperature points\n",
    "        ax.annotate(sensor_label, \n",
    "                  (actual[i], physical_depths[i]), \n",
    "                  textcoords=\"offset points\", \n",
    "                  xytext=(5, -5), \n",
    "                  ha='left',\n",
    "                  fontsize=10)\n",
    "    \n",
    "    # Calculate MAE for the plot title\n",
    "    mae = np.mean(np.abs(np.array(actual) - np.array(predicted)))\n",
    "    \n",
    "    # Extract parameters from filename if available (parse from filename string)\n",
    "    # Assuming filename format like: \"h0.1575_flux25900.0_abs20.0_surf0.9800000190734863_time1631s\"\n",
    "    import re\n",
    "    h_val = 0.1575  # Default\n",
    "    flux_val = 0    # Default\n",
    "    abs_val = 0     # Default\n",
    "    surf_val = 0    # Default\n",
    "    \n",
    "    # Parse parameters from filename if it follows the expected pattern\n",
    "    match = re.search(r'h([\\d.]+)_flux([\\d.]+)_abs([\\d.]+)_surf([\\d.]+)_time(\\d+)s', filename)\n",
    "    if match:\n",
    "        h_val = float(match.group(1))\n",
    "        flux_val = float(match.group(2))\n",
    "        abs_val = float(match.group(3))\n",
    "        surf_val = float(match.group(4))\n",
    "    else:\n",
    "        # If parsing fails, try to extract from the filename string differently\n",
    "        # This handles cases like \"Sample 5432 - h0.1575_flux25900_abs20_surf0.98_time1631s\"\n",
    "        match = re.search(r'h([\\d.]+)_flux([\\d.]+)_abs([\\d.]+)_surf([\\d.]+)', filename)\n",
    "        if match:\n",
    "            h_val = float(match.group(1))\n",
    "            flux_val = float(match.group(2))\n",
    "            abs_val = float(match.group(3))\n",
    "            surf_val = float(match.group(4))\n",
    "    \n",
    "    title = f'h={h_val:.4f}m, flux={flux_val:.0f}, abs={abs_val:.2f}, surf={surf_val:.2f} | MAE: {mae:.2f}Â°C'\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Temperature (Â°C)')\n",
    "    ax.set_ylabel('Depth (m)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_filename = f\"{filename}.png\"\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved profile plot: {plot_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85bb0083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction components loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Block 10: Standalone Prediction Functionality (USE THIS IF U WANT TO RANDOMELY TAKE SOME VALUES  AND PUT IT HERE TO GET THE PREDCITED FOR IT)\n",
    "def load_for_prediction():\n",
    "    \"\"\"Load all components needed for prediction\"\"\"\n",
    "    try:\n",
    "        # Load scalers and metadata\n",
    "        X_scaler = joblib.load(\"X_scaler_enhanced.pkl\")\n",
    "        y_scaler = joblib.load(\"y_scaler_enhanced.pkl\")\n",
    "        time_range_data = joblib.load(\"time_range_enhanced.pkl\")\n",
    "        column_info = joblib.load(\"column_info.pkl\")\n",
    "        \n",
    "        # Set device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load model\n",
    "        model = EnhancedThermalNet(\n",
    "            input_size=len(column_info['feature_cols']),\n",
    "            output_size=len(column_info['tc_cols']),\n",
    "            hidden_dims=[512, 256, 256, 128],\n",
    "            dropout_rate=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        checkpoint = torch.load('best_thermal_model_enhanced.pth', map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"Prediction components loaded successfully!\")\n",
    "        return model, X_scaler, y_scaler, time_range_data, column_info, device\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prediction components: {e}\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "def make_prediction(time, h, flux, abs_val, surf, theoretical_temps):\n",
    "    \"\"\"\n",
    "    Make a temperature prediction with given parameters\n",
    "    \n",
    "    Args:\n",
    "        time: Time value in seconds\n",
    "        h: Heat transfer coefficient (0.0375, 0.084, or 0.1575)\n",
    "        flux: Heat flux (19400, 21250, or 25900)\n",
    "        abs_val: Absorption coefficient (3 or 100)\n",
    "        surf: Surface emissivity (0.76 or 0.98)\n",
    "        theoretical_temps: List of 10 theoretical temperatures\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with predicted temperatures for each TC sensor\n",
    "    \"\"\"\n",
    "    # Load components if not already loaded\n",
    "    if not all(var in globals() for var in ['pred_model', 'pred_X_scaler', 'pred_y_scaler', \n",
    "                                           'pred_time_range', 'pred_column_info', 'pred_device']):\n",
    "        global pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device\n",
    "        pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device = load_for_prediction()\n",
    "        if pred_model is None:\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        # Validate inputs - Updated to be dynamic based on the model's expected features\n",
    "        theoretical_temp_cols = [col for col in pred_column_info['feature_cols'] if col.startswith('Theoretical_Temps_')]\n",
    "        expected_num_theoretical = len(theoretical_temp_cols)\n",
    "\n",
    "        if len(theoretical_temps) != expected_num_theoretical:\n",
    "            raise ValueError(\"Expected {} theoretical temperatures, got {}\".format(\n",
    "                expected_num_theoretical, len(theoretical_temps)))\n",
    "\n",
    "        if h not in [0.0375, 0.084, 0.1575]:\n",
    "            print(f\"Warning: h value {h} not in expected values [0.0375, 0.084, 0.1575]\")\n",
    "            \n",
    "        if flux not in [19400, 21250, 25900]:\n",
    "            print(f\"Warning: flux value {flux} not in expected values [19400, 21250, 25900]\")\n",
    "            \n",
    "        if abs_val not in [3, 100]:\n",
    "            print(f\"Warning: abs value {abs_val} not in expected values [3, 100]\")\n",
    "            \n",
    "        if surf not in [0.76, 0.98]:\n",
    "            print(f\"Warning: surf value {surf} not in expected values [0.76, 0.98]\")\n",
    "        \n",
    "        # Make prediction\n",
    "        result = predict_temperature(\n",
    "            pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info,\n",
    "            time, h, flux, abs_val, surf, theoretical_temps, pred_device\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Prediction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (run this block after training once to load components)\n",
    "pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device = load_for_prediction()\n",
    "\n",
    "# # Now you can make predictions anytime using:\n",
    "# prediction = make_prediction(\n",
    "#     time=44,\n",
    "#     h=0.1575,\n",
    "#     flux=21250,\n",
    "#     abs_val=3,\n",
    "#     surf=0.98,\n",
    "#     theoretical_temps=[303.991791613348,324.34732303606,324.34732303606,324.34732303606,324.34732303606,324.34732303606,324.34732303606,324.34732303606,324.34732303606,324.34732303606]\n",
    "    \n",
    "# )\n",
    "# temp_arr=[]\n",
    "# if prediction:\n",
    "#     print(\"\\nPredicted Temperatures:\")\n",
    "#     for tc_name, temp in prediction['predicted_temperatures'].items():\n",
    "#         print(f\"{tc_name}: {temp:.3f} Â°C\")\n",
    "#         temp_arr.append(temp)\n",
    "\n",
    "# print(temp_arr)\n",
    "\n",
    "# Test batch inference\n",
    "        # print(\"\\n3. Testing batch prediction...\")\n",
    "        # try:\n",
    "        #     if model_inf is not None:\n",
    "        #         # Create batch of test data\n",
    "        #         batch_data = [\n",
    "        #             {\n",
    "        #                 'time': 1800,  # 30 minutes\n",
    "        #                 'h': 0.1575,\n",
    "        #                 'flux': 25900,\n",
    "        #                 'abs': 100,\n",
    "        #                 'surf': 0.98,\n",
    "        #                 'theoretical_temps': [25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0]\n",
    "        #             },\n",
    "        #             {\n",
    "        #                 'time': 3600,  # 60 minutes\n",
    "        #                 'h': 0.1575,\n",
    "        #                 'flux': 19400,\n",
    "        #                 'abs': 3,\n",
    "        #                 'surf': 0.76,\n",
    "        #                 'theoretical_temps': [28.0, 32.5, 37.2, 41.8, 46.5, 51.1, 55.8, 60.4, 65.1, 69.7]\n",
    "        #             }\n",
    "        #         ]\n",
    "                \n",
    "        #         batch_results = batch_predict_temperature(\n",
    "        #             model_inf, X_scaler_inf, y_scaler_inf, time_range_inf, column_info_inf,\n",
    "        #             batch_data, device_inf\n",
    "        #         )\n",
    "                \n",
    "        #         print(f\"Batch prediction completed for {len(batch_results)} samples\")\n",
    "                \n",
    "        #         for i, result in enumerate(batch_results):\n",
    "        #             if result:\n",
    "        #                 print(f\"\\nBatch Sample {i+1}:\")\n",
    "        #                 print(f\"  Time: {result['input_parameters']['time']/60:.1f} min\")\n",
    "        #                 print(f\"  Flux: {result['input_parameters']['flux']}\")\n",
    "        #                 print(f\"  Average predicted temp: {np.mean(list(result['predicted_temperatures'].values())):.2f} Â°C\")\n",
    "                \n",
    "        #         print(\"âœ“ Batch prediction test successful!\")\n",
    "                \n",
    "        #     else:\n",
    "        #         print(\"âœ— Batch prediction test failed - model not loaded\")\n",
    "                \n",
    "        # except Exception as e:\n",
    "        #     print(f\"âœ— Batch prediction test failed: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46eb77cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data directory: data/TC11data\n",
      "\n",
      "=== Running Standalone Test Cross-Check ===\n",
      "Loading data from: data/TC11data\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv (566 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv (621 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv (826 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv (795 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv (932 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv (912 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv (1087 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv (790 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv (585 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv (611 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv (554 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv (589 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv (970 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv (803 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv (1411 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv (787 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv (531 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv (865 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv (550 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv (887 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv (790 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv (794 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv (4087 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv (849 rows)\n",
      "\n",
      "==================================================\n",
      "FILE PROCESSING SUMMARY\n",
      "==================================================\n",
      "Total files scanned     : 25\n",
      "Files successfully loaded: 25\n",
      "Files skipped (filtered): 0\n",
      "Files skipped (unmatched): 0\n",
      "==================================================\n",
      "\n",
      "Total data loaded: 22706 rows, 27 columns\n",
      "Starting data preprocessing...\n",
      "Features: 21, Targets: 11\n",
      "Theory columns: 10, TC columns: 11\n",
      "Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10', 'TC11']\n",
      "After removing missing values: 22706 samples\n",
      "Training samples: 15894\n",
      "Validation samples: 3406\n",
      "Test samples: 3406\n",
      "\n",
      "=== Cross-Checking 10 Test Samples ===\n",
      "\n",
      "Test Sample 1884:\n",
      "Inputs: Time=192.00s, h=0.1575, flux=25900, abs=3, surf=0.76\n",
      "Theoretical Temps: ['317.69', '335.74', '335.74', '335.74', '335.74', '335.74', '335.74', '335.74', '335.74', '335.74']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         363.378         363.240         0.138          \n",
      "TC2             361.531         361.300         0.231          \n",
      "TC3             362.930         362.940         0.010          \n",
      "TC4             363.401         363.120         0.281          \n",
      "TC5             363.159         362.960         0.199          \n",
      "TC6             364.823         365.020         0.197          \n",
      "TC7             363.977         364.000         0.023          \n",
      "TC8             364.695         364.800         0.105          \n",
      "TC9             364.204         364.190         0.014          \n",
      "TC10            349.168         348.980         0.188          \n",
      "TC11            243.677         241.550         2.127          \n",
      "\n",
      "Test Sample 3100:\n",
      "Inputs: Time=473.00s, h=0.1575, flux=25900, abs=3, surf=0.98\n",
      "Theoretical Temps: ['333.27', '356.88', '356.88', '356.88', '356.88', '356.88', '356.88', '356.88', '356.88', '356.88']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         361.582         361.730         0.148          \n",
      "TC2             360.834         360.980         0.146          \n",
      "TC3             361.989         362.320         0.331          \n",
      "TC4             363.292         363.740         0.448          \n",
      "TC5             362.982         362.860         0.122          \n",
      "TC6             363.846         364.110         0.264          \n",
      "TC7             363.865         364.470         0.605          \n",
      "TC8             364.833         365.470         0.637          \n",
      "TC9             361.110         360.520         0.590          \n",
      "TC10            350.506         348.040         2.466          \n",
      "TC11            351.259         351.830         0.571          \n",
      "\n",
      "Test Sample 1522:\n",
      "Inputs: Time=880.00s, h=0.1575, flux=25900, abs=3, surf=0.76\n",
      "Theoretical Temps: ['356.39', '378.22', '378.22', '378.22', '378.22', '378.22', '378.22', '378.22', '378.22', '378.22']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         363.779         363.780         0.001          \n",
      "TC2             362.541         362.240         0.301          \n",
      "TC3             364.714         364.630         0.084          \n",
      "TC4             364.976         364.710         0.266          \n",
      "TC5             364.688         364.350         0.338          \n",
      "TC6             366.200         366.480         0.280          \n",
      "TC7             364.890         364.820         0.070          \n",
      "TC8             365.259         365.340         0.081          \n",
      "TC9             361.143         361.050         0.093          \n",
      "TC10            342.179         342.650         0.471          \n",
      "TC11            246.469         246.920         0.451          \n",
      "\n",
      "Test Sample 1586:\n",
      "Inputs: Time=816.00s, h=0.1575, flux=21250, abs=3, surf=0.98\n",
      "Theoretical Temps: ['330.14', '353.39', '353.39', '353.39', '353.39', '353.39', '353.39', '353.39', '353.39', '353.39']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         352.921         353.050         0.129          \n",
      "TC2             352.511         352.610         0.099          \n",
      "TC3             353.049         353.040         0.009          \n",
      "TC4             353.472         353.310         0.162          \n",
      "TC5             352.998         352.640         0.358          \n",
      "TC6             354.275         354.410         0.135          \n",
      "TC7             352.709         352.670         0.039          \n",
      "TC8             353.175         353.040         0.135          \n",
      "TC9             349.058         349.420         0.362          \n",
      "TC10            324.397         324.900         0.503          \n",
      "TC11            342.717         343.770         1.053          \n",
      "\n",
      "Test Sample 165:\n",
      "Inputs: Time=126.00s, h=0.1575, flux=25900, abs=100, surf=0.98\n",
      "Theoretical Temps: ['417.21', '426.11', '346.34', '311.63', '302.52', '300.52', '300.11', '300.02', '300.00', '300.00']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         354.571         354.780         0.209          \n",
      "TC2             354.999         355.100         0.101          \n",
      "TC3             357.148         357.270         0.122          \n",
      "TC4             358.264         358.200         0.064          \n",
      "TC5             357.957         357.840         0.117          \n",
      "TC6             359.392         359.700         0.308          \n",
      "TC7             358.036         357.660         0.376          \n",
      "TC8             359.068         358.720         0.348          \n",
      "TC9             370.055         369.470         0.585          \n",
      "TC10            368.180         368.460         0.280          \n",
      "TC11            385.491         387.000         1.509          \n",
      "\n",
      "Test Sample 1627:\n",
      "Inputs: Time=1846.00s, h=0.1575, flux=25900, abs=3, surf=0.98\n",
      "Theoretical Temps: ['379.68', '409.15', '409.15', '409.15', '409.15', '409.15', '409.15', '409.15', '409.15', '409.15']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         372.966         372.200         0.766          \n",
      "TC2             371.086         370.970         0.116          \n",
      "TC3             373.660         373.650         0.010          \n",
      "TC4             374.021         373.860         0.161          \n",
      "TC5             373.786         373.450         0.336          \n",
      "TC6             375.212         375.020         0.192          \n",
      "TC7             375.121         374.720         0.401          \n",
      "TC8             375.655         375.610         0.045          \n",
      "TC9             374.400         374.070         0.330          \n",
      "TC10            367.169         366.600         0.569          \n",
      "TC11            354.381         355.000         0.619          \n",
      "\n",
      "Test Sample 3327:\n",
      "Inputs: Time=132.00s, h=0.1575, flux=21250, abs=3, surf=0.98\n",
      "Theoretical Temps: ['309.93', '330.92', '330.92', '330.92', '330.92', '330.92', '330.92', '330.92', '330.92', '330.92']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         360.918         360.990         0.072          \n",
      "TC2             359.436         359.450         0.014          \n",
      "TC3             360.548         360.340         0.208          \n",
      "TC4             361.136         361.060         0.076          \n",
      "TC5             360.824         360.630         0.194          \n",
      "TC6             362.233         362.440         0.207          \n",
      "TC7             361.436         361.290         0.146          \n",
      "TC8             362.063         362.340         0.277          \n",
      "TC9             360.671         361.340         0.669          \n",
      "TC10            345.320         345.350         0.030          \n",
      "TC11            345.082         345.650         0.568          \n",
      "\n",
      "Test Sample 1499:\n",
      "Inputs: Time=2606.00s, h=0.1575, flux=25900, abs=3, surf=0.98\n",
      "Theoretical Temps: ['402.65', '435.34', '435.34', '435.34', '435.34', '435.34', '435.34', '435.34', '435.34', '435.34']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         378.568         378.570         0.002          \n",
      "TC2             376.605         376.460         0.145          \n",
      "TC3             379.298         379.550         0.252          \n",
      "TC4             379.680         379.550         0.130          \n",
      "TC5             379.467         379.330         0.137          \n",
      "TC6             380.854         380.710         0.144          \n",
      "TC7             380.772         380.660         0.112          \n",
      "TC8             381.229         381.340         0.111          \n",
      "TC9             380.481         380.670         0.189          \n",
      "TC10            373.595         372.950         0.645          \n",
      "TC11            354.493         355.000         0.507          \n",
      "\n",
      "Test Sample 2229:\n",
      "Inputs: Time=45.00s, h=0.1575, flux=21250, abs=3, surf=0.76\n",
      "Theoretical Temps: ['302.90', '319.63', '319.63', '319.63', '319.63', '319.63', '319.63', '319.63', '319.63', '319.63']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         362.191         362.270         0.079          \n",
      "TC2             361.158         361.390         0.232          \n",
      "TC3             361.554         361.310         0.244          \n",
      "TC4             362.116         362.230         0.114          \n",
      "TC5             361.809         361.790         0.019          \n",
      "TC6             363.329         363.700         0.371          \n",
      "TC7             361.960         361.900         0.060          \n",
      "TC8             362.557         362.520         0.037          \n",
      "TC9             360.146         360.040         0.106          \n",
      "TC10            336.043         335.250         0.793          \n",
      "TC11            224.999         227.320         2.321          \n",
      "\n",
      "Test Sample 2451:\n",
      "Inputs: Time=1327.00s, h=0.1575, flux=25900, abs=3, surf=0.98\n",
      "Theoretical Temps: ['360.06', '386.94', '386.94', '386.94', '386.94', '386.94', '386.94', '386.94', '386.94', '386.94']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         342.808         342.640         0.168          \n",
      "TC2             345.523         346.010         0.487          \n",
      "TC3             346.720         346.400         0.320          \n",
      "TC4             350.921         350.970         0.049          \n",
      "TC5             352.176         352.640         0.464          \n",
      "TC6             354.795         355.040         0.245          \n",
      "TC7             355.921         356.150         0.229          \n",
      "TC8             358.313         358.330         0.017          \n",
      "TC9             356.488         356.330         0.158          \n",
      "TC10            344.386         344.440         0.054          \n",
      "TC11            252.629         252.650         0.021          \n",
      "\n",
      "=== Average Errors (Test Set) ===\n",
      "Sensor          Avg Error (Â°C) \n",
      "------------------------------\n",
      "TC1_tip         0.171          \n",
      "TC2             0.187          \n",
      "TC3             0.159          \n",
      "TC4             0.175          \n",
      "TC5             0.228          \n",
      "TC6             0.234          \n",
      "TC7             0.206          \n",
      "TC8             0.179          \n",
      "TC9             0.310          \n",
      "TC10            0.600          \n",
      "TC11            0.975          \n",
      "Saved profile plot: Sample 1884 - h0.1575_flux25900.0_abs3.000000238418579_surf0.7599999904632568_time192s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.000000238418579_surf0.7599999904632568_time192s.png\n",
      "Saved profile plot: Sample 3100 - h0.1575_flux25900.0_abs3.000000238418579_surf0.9800000190734863_time473s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.000000238418579_surf0.9800000190734863_time473s.png\n",
      "Saved profile plot: Sample 1522 - h0.1575_flux25900.0_abs3.000000238418579_surf0.7599999904632568_time880s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.000000238418579_surf0.7599999904632568_time880s.png\n",
      "Saved profile plot: Sample 1586 - h0.1575_flux21250.0_abs3.000000238418579_surf0.9800000190734863_time816s.png\n",
      "Plot saved: h0.1575_flux21250.0_abs3.000000238418579_surf0.9800000190734863_time816s.png\n",
      "Saved profile plot: Sample 165 - h0.1575_flux25900.0_abs100.0_surf0.9800000190734863_time126s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs100.0_surf0.9800000190734863_time126s.png\n",
      "Saved profile plot: Sample 1627 - h0.1575_flux25900.0_abs3.000000238418579_surf0.9800000190734863_time1846s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.000000238418579_surf0.9800000190734863_time1846s.png\n",
      "Saved profile plot: Sample 3327 - h0.1575_flux21250.0_abs3.000000238418579_surf0.9800000190734863_time132s.png\n",
      "Plot saved: h0.1575_flux21250.0_abs3.000000238418579_surf0.9800000190734863_time132s.png\n",
      "Saved profile plot: Sample 1499 - h0.1575_flux25900.0_abs3.000000238418579_surf0.9800000190734863_time2606s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.000000238418579_surf0.9800000190734863_time2606s.png\n",
      "Saved profile plot: Sample 2229 - h0.1575_flux21250.0_abs3.000000238418579_surf0.7599999904632568_time45s.png\n",
      "Plot saved: h0.1575_flux21250.0_abs3.000000238418579_surf0.7599999904632568_time45s.png\n",
      "Saved profile plot: Sample 2451 - h0.1575_flux25900.0_abs3.000000238418579_surf0.9800000190734863_time1327s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.000000238418579_surf0.9800000190734863_time1327s.png\n"
     ]
    }
   ],
   "source": [
    "# Block 11: Standalone Testing. [USE THIS IF U WANT TO GET N NUM OF RANDOM PLOTS WHICH ARE TAKEN FROM DATAS WHICH WERENT TESTED ON]\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive Agg backend to avoid tkinter dependency\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def cross_check_test_predictions(data_dir, h_filter=0.1575, num_samples=5, filter_condition=None):\n",
    "    \"\"\"Cross-check model predictions with rows from the test dataset.\"\"\"\n",
    "    # Load inference components\n",
    "    global pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device\n",
    "    if not all(var in globals() for var in ['pred_model', 'pred_X_scaler', 'pred_y_scaler', \n",
    "                                           'pred_time_range', 'pred_column_info', 'pred_device']):\n",
    "        pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device = load_for_prediction()\n",
    "        if pred_model is None:\n",
    "            raise ValueError(\"Failed to load inference components. Ensure model and scalers are saved.\")\n",
    "\n",
    "    # Load and preprocess data to get test_loader\n",
    "    data = load_data(data_dir, h_filter=h_filter)\n",
    "    train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols = preprocess_data(data, use_enhanced_features=True)\n",
    "    \n",
    "    # Extract test dataset from test_loader\n",
    "    X_test_scaled = np.concatenate([batch[0].numpy() for batch in test_loader], axis=0)\n",
    "    y_test_scaled = np.concatenate([batch[1].numpy() for batch in test_loader], axis=0)\n",
    "    \n",
    "    # Inverse transform to get original feature and target values\n",
    "    X_test = pred_X_scaler.inverse_transform(X_test_scaled)\n",
    "    y_test = pred_y_scaler.inverse_transform(y_test_scaled)\n",
    "    \n",
    "    # Create DataFrame for test data\n",
    "    feature_cols = pred_column_info['feature_cols']\n",
    "    test_df = pd.DataFrame(X_test, columns=feature_cols)\n",
    "    test_df[tc_cols] = y_test\n",
    "    \n",
    "    # Add original 'Time' (before normalization) for filtering\n",
    "    time_min = pred_time_range['time_min']\n",
    "    time_max = pred_time_range['time_max']\n",
    "    test_df['Time'] = test_df['Time_norm'] * (time_max - time_min) + time_min\n",
    "    \n",
    "    # Apply filter condition if provided (e.g., specific time or flux)\n",
    "    if filter_condition is not None:\n",
    "        test_df = test_df.query(filter_condition)\n",
    "        if test_df.empty:\n",
    "            raise ValueError(f\"No test rows match the condition: {filter_condition}\")\n",
    "    \n",
    "    # Sample rows for cross-checking (no random_state for true randomness)\n",
    "    num_samples = min(num_samples, len(test_df))\n",
    "    if num_samples == 0:\n",
    "        raise ValueError(\"No test samples available after filtering!\")\n",
    "    sample_rows = test_df.sample(n=num_samples,random_state=33) if filter_condition is None else test_df.head(num_samples)\n",
    "    \n",
    "    print(f\"\\n=== Cross-Checking {len(sample_rows)} Test Samples ===\")\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in sample_rows.iterrows():\n",
    "        # Prepare input features\n",
    "        time = row['Time']\n",
    "        h = row['h']\n",
    "        flux = row['flux']\n",
    "        abs_val = row['abs']\n",
    "        surf = row['surf']\n",
    "        theoretical_temps = [row[col] for col in [c for c in feature_cols if c.startswith('Theoretical_Temps_')]]\n",
    "        \n",
    "        # Make prediction\n",
    "        try:\n",
    "            pred_result = predict_temperature(\n",
    "                pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info,\n",
    "                time, h, flux, abs_val, surf, theoretical_temps, pred_device\n",
    "            )\n",
    "            \n",
    "            # Get actual temperatures\n",
    "            actual_temps = {col: row[col] for col in tc_cols}\n",
    "            \n",
    "            # Compare predictions with actuals\n",
    "            comparison = {\n",
    "                'index': idx,\n",
    "                'inputs': {\n",
    "                    'time': time,\n",
    "                    'h': h,\n",
    "                    'flux': flux,\n",
    "                    'abs': abs_val,\n",
    "                    'surf': surf,\n",
    "                    'theoretical_temps': theoretical_temps\n",
    "                },\n",
    "                'predicted_temps': pred_result['predicted_temperatures'],\n",
    "                'actual_temps': actual_temps,\n",
    "                'errors': {col: abs(pred_result['predicted_temperatures'][col] - actual_temps[col]) \n",
    "                          for col in tc_cols}\n",
    "            }\n",
    "            \n",
    "            results.append(comparison)\n",
    "            \n",
    "            # Print comparison\n",
    "            print(f\"\\nTest Sample {idx}:\")\n",
    "            print(f\"Inputs: Time={time:.2f}s, h={h:.4f}, flux={flux:.0f}, abs={abs_val:.0f}, surf={surf:.2f}\")\n",
    "            print(\"Theoretical Temps:\", [f\"{t:.2f}\" for t in theoretical_temps])\n",
    "            print(\"Predicted vs Actual Temperatures:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"{'Sensor':<15} {'Predicted (Â°C)':<15} {'Actual (Â°C)':<15} {'Error (Â°C)':<15}\")\n",
    "            print(\"-\" * 50)\n",
    "            for col in tc_cols:\n",
    "                pred_temp = pred_result['predicted_temperatures'][col]\n",
    "                actual_temp = actual_temps[col]\n",
    "                error = comparison['errors'][col]\n",
    "                print(f\"{col:<15} {pred_temp:<15.3f} {actual_temp:<15.3f} {error:<15.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test sample {idx}: {e}\")\n",
    "    \n",
    "    # Calculate average errors\n",
    "    if results:\n",
    "        avg_errors = {col: np.mean([r['errors'][col] for r in results]) for col in tc_cols}\n",
    "        print(\"\\n=== Average Errors (Test Set) ===\")\n",
    "        print(f\"{'Sensor':<15} {'Avg Error (Â°C)':<15}\")\n",
    "        print(\"-\" * 30)\n",
    "        for col, avg_error in avg_errors.items():\n",
    "            print(f\"{col:<15} {avg_error:<15.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_test_cross_check(data_dir, h_filter=0.1575, num_samples=5, filter_condition=None):\n",
    "    \"\"\"Run cross-checking on test data without requiring main execution.\"\"\"\n",
    "    try:\n",
    "        print(\"\\n=== Running Standalone Test Cross-Check ===\")\n",
    "        results = cross_check_test_predictions(data_dir, h_filter, num_samples, filter_condition)\n",
    "        \n",
    "        # Plot results using plot_vertical_profile\n",
    "        for result in results:\n",
    "            # Extract predicted and actual temperatures in order TC1_tip to TC10\n",
    "            tc_cols = pred_column_info['tc_cols']  # This will be dynamic based on your model\n",
    "            # tc_cols = ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10','TC11']\n",
    "            predicted = [result['predicted_temps'][col] for col in tc_cols]\n",
    "            actual = [result['actual_temps'][col] for col in tc_cols]\n",
    "            # Create filename based on input conditions\n",
    "            filename = (\n",
    "                f\"h{h_filter}_flux{result['inputs']['flux']}_\"\n",
    "                f\"abs{result['inputs']['abs']}_surf{result['inputs']['surf']}_\"\n",
    "                f\"time{result['inputs']['time']:.0f}s\"\n",
    "            )\n",
    "            try:\n",
    "                plot_vertical_profile(predicted, actual, filename=f\"Sample {result['index']} - {filename}\")\n",
    "                print(f\"Plot saved: {filename}.png\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error plotting sample {result['index']}: {e}\")\n",
    "        \n",
    "        # Save results to CSV\n",
    "        # pd.DataFrame(results).to_csv('test_cross_check_results.csv')\n",
    "        # print(\"Results saved to 'test_cross_check_results.csv'\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in standalone test cross-check: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_DIR = get_data_directory()\n",
    "    run_test_cross_check(DATA_DIR, h_filter=0.1575, num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e075e3",
   "metadata": {},
   "source": [
    "# Perfect Working model till here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a3233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Block 5: Data Preprocessing (Modified)\n",
    "# def preprocess_data(data, test_size=0.3, val_size=0.5, use_enhanced_features=True, h_value=None):\n",
    "#     \"\"\"Enhanced data preprocessing with support for variable TC sensors\"\"\"\n",
    "#     print(\"Starting data preprocessing...\")\n",
    "    \n",
    "#     # Time normalization\n",
    "#     time_min = data[\"Time\"].min()\n",
    "#     time_max = data[\"Time\"].max()\n",
    "#     data[\"Time_norm\"] = (data[\"Time\"] - time_min) / (time_max - time_min)\n",
    "    \n",
    "#     # Enhanced time features\n",
    "#     if use_enhanced_features:\n",
    "#         data[\"TimeÂ²\"] = data[\"Time_norm\"] ** 2\n",
    "#         data[\"TimeÂ³\"] = data[\"Time_norm\"] ** 3\n",
    "#         data[\"Time_sin\"] = np.sin(2 * np.pi * data[\"Time_norm\"])\n",
    "#         data[\"Time_cos\"] = np.cos(2 * np.pi * data[\"Time_norm\"])\n",
    "#         data[\"flux_abs_interaction\"] = data[\"flux\"] * data[\"abs\"]\n",
    "#         data[\"h_flux_interaction\"] = data[\"h\"] * data[\"flux\"]\n",
    "#         base_features = [\"Time_norm\", \"TimeÂ²\", \"TimeÂ³\", \"Time_sin\", \"Time_cos\", \n",
    "#                          \"h\", \"flux\", \"abs\", \"surf\", \"flux_abs_interaction\", \"h_flux_interaction\"]\n",
    "#     else:\n",
    "#         data[\"TimeÂ²\"] = data[\"Time_norm\"] ** 2\n",
    "#         base_features = [\"Time_norm\", \"TimeÂ²\", \"h\", \"flux\", \"abs\", \"surf\"]\n",
    "    \n",
    "#     # Identify TC and theoretical temperature columns\n",
    "#     theory_cols = [c for c in data.columns if c.startswith(\"Theoretical_Temps_\")]\n",
    "#     tc_cols = [col for col in data.columns if (col.startswith(\"TC\") and not col.endswith(\"_rate\")) or col.startswith(\"TC_\")]\n",
    "    \n",
    "#     # Determine number of sensors based on h_value\n",
    "#     h_category = get_h_category(h_value) if h_value is not None else 'h6'\n",
    "#     if h_category == 'h2':\n",
    "#         num_sensors = 3\n",
    "#     elif h_category == 'h3':\n",
    "#         num_sensors = 5\n",
    "#     else:  # h6 or default\n",
    "#         num_sensors = 10\n",
    "    \n",
    "#     # Filter valid TC and theoretical columns\n",
    "#     tc_cols = tc_cols[:num_sensors]\n",
    "#     theory_cols = theory_cols[:num_sensors] if len(theory_cols) >= num_sensors else theory_cols\n",
    "    \n",
    "#     # Handle missing TC columns\n",
    "#     if not tc_cols:\n",
    "#         print(\"Warning: No TC columns found. Creating dummy TC columns.\")\n",
    "#         tc_cols = [f\"TC_{i+1}\" for i in range(num_sensors)]\n",
    "#         for col in tc_cols:\n",
    "#             if col not in data.columns:\n",
    "#                 data[col] = 20 + np.random.normal(0, 5, len(data))\n",
    "    \n",
    "#     # Prepare features and targets\n",
    "#     feature_cols = base_features + theory_cols\n",
    "#     feature_cols = [col for col in feature_cols if col in data.columns]\n",
    "    \n",
    "#     X = data[feature_cols].copy()\n",
    "#     y = data[tc_cols].copy()\n",
    "#     filenames = data[\"filename\"] if \"filename\" in data.columns else pd.Series([\"unknown\"] * len(data))\n",
    "    \n",
    "#     print(f\"Features: {len(feature_cols)}, Targets: {len(tc_cols)}\")\n",
    "#     print(f\"Theory columns: {len(theory_cols)}, TC columns: {len(tc_cols)}\")\n",
    "#     print(f\"Feature columns: {feature_cols}\")\n",
    "#     print(f\"Target columns: {tc_cols}\")\n",
    "    \n",
    "#     # Remove missing values\n",
    "#     mask = ~(X.isnull().any(axis=1) | y.isnull().any(axis=1))\n",
    "#     X = X[mask].reset_index(drop=True)\n",
    "#     y = y[mask].reset_index(drop=True)\n",
    "#     filenames = filenames[mask].reset_index(drop=True)\n",
    "    \n",
    "#     print(f\"After removing missing values: {len(X)} samples\")\n",
    "    \n",
    "#     if len(X) < 10:\n",
    "#         raise ValueError(\"Not enough valid samples for training!\")\n",
    "    \n",
    "#     # Train-test split\n",
    "#     X_train, X_temp, y_train, y_temp, _, _ = train_test_split(\n",
    "#         X, y, filenames, test_size=test_size, random_state=SEED\n",
    "#     )\n",
    "#     X_val, X_test, y_val, y_test, _, _ = train_test_split(\n",
    "#         X_temp, y_temp, _, test_size=val_size, random_state=SEED\n",
    "#     )\n",
    "    \n",
    "#     # Scaling\n",
    "#     X_scaler = StandardScaler()\n",
    "#     y_scaler = MinMaxScaler()\n",
    "    \n",
    "#     X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "#     X_val_scaled = X_scaler.transform(X_val)\n",
    "#     X_test_scaled = X_scaler.transform(X_test)\n",
    "    \n",
    "#     y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "#     y_val_scaled = y_scaler.transform(y_val)\n",
    "#     y_test_scaled = y_scaler.transform(y_test)\n",
    "    \n",
    "#     # Save scalers and metadata\n",
    "#     joblib.dump(X_scaler, f\"X_scaler_{h_category}.pkl\")\n",
    "#     joblib.dump(y_scaler, f\"y_scaler_{h_category}.pkl\")\n",
    "#     joblib.dump({\"time_min\": time_min, \"time_max\": time_max}, f\"time_range_{h_category}.pkl\")\n",
    "#     joblib.dump({\"feature_cols\": feature_cols, \"tc_cols\": tc_cols, \"num_sensors\": num_sensors}, f\"column_info_{h_category}.pkl\")\n",
    "    \n",
    "#     # Create DataLoaders\n",
    "#     train_dataset = TensorDataset(\n",
    "#         torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "#         torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "#     )\n",
    "#     val_dataset = TensorDataset(\n",
    "#         torch.tensor(X_val_scaled, dtype=torch.float32),\n",
    "#         torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "#     )\n",
    "#     test_dataset = TensorDataset(\n",
    "#         torch.tensor(X_test_scaled, dtype=torch.float32),\n",
    "#         torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "#     )\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=min(64, len(X_train)//4), shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=min(64, len(X_val)//4), shuffle=False)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=min(64, len(X_test)//4), shuffle=False)\n",
    "    \n",
    "#     print(f\"Training samples: {len(X_train)}\")\n",
    "#     print(f\"Validation samples: {len(X_val)}\")\n",
    "#     print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "#     return train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols, num_sensors\n",
    "\n",
    "# # Block 6: Physics-Informed Loss (Modified)\n",
    "# class PhysicsInformedLoss(nn.Module):\n",
    "#     \"\"\"Physics-informed loss function with support for variable TC sensors\"\"\"\n",
    "#     def __init__(self, smoothness_weight=0.005, gradient_weight=0.0001, physics_weight=0.5, \n",
    "#                  conservation_violation_penalty=100.0):\n",
    "#         super().__init__()\n",
    "#         self.mse_loss = nn.MSELoss()\n",
    "#         self.smoothness_weight = smoothness_weight\n",
    "#         self.gradient_weight = gradient_weight\n",
    "#         self.physics_weight = physics_weight\n",
    "#         self.conservation_violation_penalty = conservation_violation_penalty\n",
    "        \n",
    "#         # Physical constants\n",
    "#         self.r = 2.0375 * 0.0254\n",
    "#         self.A_rec = np.pi * (self.r ** 2)\n",
    "#         self.rho = 1836.31\n",
    "#         self.cp = 1512\n",
    "        \n",
    "#         self.A_rec_tensor = None\n",
    "#         self.rho_tensor = None\n",
    "#         self.cp_tensor = None\n",
    "        \n",
    "#         self.violation_count = 0\n",
    "#         self.total_batches = 0\n",
    "    \n",
    "#     def _init_tensors(self, device):\n",
    "#         if self.A_rec_tensor is None:\n",
    "#             self.A_rec_tensor = torch.tensor(self.A_rec, device=device, dtype=torch.float32)\n",
    "#             self.rho_tensor = torch.tensor(self.rho, device=device, dtype=torch.float32)\n",
    "#             self.cp_tensor = torch.tensor(self.cp, device=device, dtype=torch.float32)\n",
    "    \n",
    "#     def compute_physics_loss(self, predictions, targets, inputs, num_sensors):\n",
    "#         device = predictions.device\n",
    "#         self._init_tensors(device)\n",
    "        \n",
    "#         batch_size = predictions.shape[0]\n",
    "#         self.total_batches += 1\n",
    "        \n",
    "#         try:\n",
    "#             flux = inputs[:, 6]\n",
    "#             h = inputs[:, 5]\n",
    "#             incoming_energy = flux * self.A_rec_tensor\n",
    "#             mass = self.rho_tensor * h * self.A_rec_tensor\n",
    "            \n",
    "#             theoretical_start_idx = 11\n",
    "#             if inputs.shape[1] >= theoretical_start_idx + num_sensors:\n",
    "#                 theoretical_temps = inputs[:, theoretical_start_idx:theoretical_start_idx + num_sensors]\n",
    "#                 temp_change = torch.mean(predictions - theoretical_temps, dim=1)\n",
    "#             else:\n",
    "#                 temp_change = torch.mean(predictions - targets, dim=1)\n",
    "            \n",
    "#             dt = torch.ones_like(temp_change)\n",
    "#             total_energy_stored = mass * self.cp_tensor * temp_change / dt\n",
    "            \n",
    "#             conservation_loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "            \n",
    "#             violation_mask = total_energy_stored > incoming_energy\n",
    "#             if torch.any(violation_mask):\n",
    "#                 excess = total_energy_stored[violation_mask] - incoming_energy[violation_mask]\n",
    "#                 relative_violation = excess / (incoming_energy[violation_mask] + 1e-6)\n",
    "#                 conservation_loss = conservation_loss + self.conservation_violation_penalty * torch.mean(relative_violation ** 2)\n",
    "#                 self.violation_count += torch.sum(violation_mask).item()\n",
    "            \n",
    "#             margin = 0.05 * incoming_energy\n",
    "#             soft_violation_mask = (total_energy_stored > incoming_energy - margin) & (~violation_mask)\n",
    "#             if torch.any(soft_violation_mask):\n",
    "#                 soft_excess = total_energy_stored[soft_violation_mask] - incoming_energy[soft_violation_mask]\n",
    "#                 soft_relative_violation = soft_excess / (incoming_energy[soft_violation_mask] + 1e-6)\n",
    "#                 soft_penalty = torch.mean(soft_relative_violation ** 2)\n",
    "#                 conservation_loss = conservation_loss + 0.1 * self.conservation_violation_penalty * soft_penalty\n",
    "            \n",
    "#             acceptable_mask = total_energy_stored <= incoming_energy\n",
    "#             if torch.any(acceptable_mask):\n",
    "#                 energy_difference = incoming_energy[acceptable_mask] - total_energy_stored[acceptable_mask]\n",
    "#                 efficiency_penalty = torch.mean(energy_difference) * 0.01\n",
    "#                 conservation_loss = conservation_loss + efficiency_penalty\n",
    "            \n",
    "#             balance_tolerance = 0.01 * torch.abs(incoming_energy)\n",
    "#             energy_difference = torch.abs(incoming_energy - total_energy_stored)\n",
    "#             balance_mask = energy_difference <= balance_tolerance\n",
    "#             if torch.any(balance_mask):\n",
    "#                 balance_reward = torch.mean(energy_difference[balance_mask])\n",
    "#                 conservation_loss = conservation_loss - 0.05 * balance_reward\n",
    "            \n",
    "#             return conservation_loss\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Warning: Physics loss computation failed: {e}\")\n",
    "#             return torch.tensor(100.0, device=device, requires_grad=True)\n",
    "    \n",
    "#     def get_violation_rate(self):\n",
    "#         if self.total_batches == 0:\n",
    "#             return 0.0\n",
    "#         total_samples = self.total_batches * 64\n",
    "#         return self.violation_count / total_samples if total_samples > 0 else 0.0\n",
    "    \n",
    "#     def reset_violation_tracking(self):\n",
    "#         self.violation_count = 0\n",
    "#         self.total_batches = 0\n",
    "    \n",
    "#     def forward(self, predictions, targets, inputs=None, num_sensors=None):\n",
    "#         mse_loss = self.mse_loss(predictions, targets)\n",
    "        \n",
    "#         if predictions.shape[1] > 1:\n",
    "#             smoothness_loss = torch.mean(torch.abs(predictions[:, 1:] - predictions[:, :-1]))\n",
    "#             gradient_loss = torch.mean(torch.abs(torch.diff(predictions, dim=1)))\n",
    "#         else:\n",
    "#             smoothness_loss = torch.tensor(0.0, device=predictions.device)\n",
    "#             gradient_loss = torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "#         if inputs is not None and num_sensors is not None:\n",
    "#             physics_loss = self.compute_physics_loss(predictions, targets, inputs, num_sensors)\n",
    "#         else:\n",
    "#             physics_loss = torch.tensor(50.0, device=predictions.device, requires_grad=True)\n",
    "        \n",
    "#         total_loss = (mse_loss + \n",
    "#                      self.smoothness_weight * smoothness_loss + \n",
    "#                      self.gradient_weight * gradient_loss +\n",
    "#                      self.physics_weight * physics_loss)\n",
    "        \n",
    "#         return total_loss\n",
    "\n",
    "# # Block 7: Training Function (Modified)\n",
    "# def train_model_with_conservation_monitoring(model, train_loader, val_loader, device, num_sensors, h_value, epochs=1000, patience=50):\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, min_lr=1e-6)\n",
    "    \n",
    "#     criterion = PhysicsInformedLoss(\n",
    "#         smoothness_weight=0.005,\n",
    "#         gradient_weight=0.0001,\n",
    "#         physics_weight=0.5,\n",
    "#         conservation_violation_penalty=100.0\n",
    "#     )\n",
    "    \n",
    "#     best_val_loss = float('inf')\n",
    "#     patience_counter = 0\n",
    "#     train_losses = []\n",
    "#     val_losses = []\n",
    "#     violation_rates = []\n",
    "    \n",
    "#     print(f\"Starting training with {num_sensors} sensors for h={h_value}...\")\n",
    "#     for epoch in range(epochs):\n",
    "#         criterion.reset_violation_tracking()\n",
    "#         model.train()\n",
    "#         train_loss_epoch = 0\n",
    "#         for X_batch, y_batch in train_loader:\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             predictions = model(X_batch)\n",
    "#             loss = criterion(predictions, y_batch, X_batch, num_sensors)\n",
    "#             if torch.isnan(loss) or torch.isinf(loss):\n",
    "#                 print(f\"Warning: Invalid loss detected at epoch {epoch}\")\n",
    "#                 continue\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             optimizer.step()\n",
    "#             train_loss_epoch += loss.item()\n",
    "#         train_loss_epoch /= len(train_loader)\n",
    "        \n",
    "#         model.eval()\n",
    "#         val_loss_epoch = 0\n",
    "#         with torch.no_grad():\n",
    "#             for X_batch, y_batch in val_loader:\n",
    "#                 X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#                 predictions = model(X_batch)\n",
    "#                 loss = criterion(predictions, y_batch, X_batch, num_sensors)\n",
    "#                 val_loss_epoch += loss.item()\n",
    "#         val_loss_epoch /= len(val_loader)\n",
    "        \n",
    "#         violation_rate = criterion.get_violation_rate()\n",
    "#         violation_rates.append(violation_rate)\n",
    "#         scheduler.step(val_loss_epoch)\n",
    "#         train_losses.append(train_loss_epoch)\n",
    "#         val_losses.append(val_loss_epoch)\n",
    "        \n",
    "#         adjusted_val_loss = val_loss_epoch + (violation_rate * 100)\n",
    "#         if adjusted_val_loss < best_val_loss:\n",
    "#             best_val_loss = adjusted_val_loss\n",
    "#             patience_counter = 0\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'val_loss': val_loss_epoch,\n",
    "#                 'train_loss': train_loss_epoch,\n",
    "#                 'violation_rate': violation_rate,\n",
    "#             }, f'best_thermal_model_{get_h_category(h_value)}.pth')\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "#             if patience_counter >= patience:\n",
    "#                 print(f\"Early stopping at epoch {epoch}\")\n",
    "#                 break\n",
    "        \n",
    "#         if (epoch + 1) % 100 == 0 or (epoch < 50 and (epoch + 1) % 10 == 0):\n",
    "#             current_lr = optimizer.param_groups[0]['lr']\n",
    "#             status = \"âœ… EXCELLENT\" if violation_rate < 0.01 else \"âš ï¸ ACCEPTABLE\" if violation_rate < 0.05 else \"ðŸš¨ CONCERNING\"\n",
    "#             print(f\"Epoch [{epoch+1:4d}/{epochs}] \"\n",
    "#                   f\"Train: {train_loss_epoch:.6f} \"\n",
    "#                   f\"Val: {val_loss_epoch:.6f} \"\n",
    "#                   f\"Violations: {violation_rate:.4f} ({status}) \"\n",
    "#                   f\"LR: {current_lr:.8f}\")\n",
    "    \n",
    "#     checkpoint = torch.load(f'best_thermal_model_{get_h_category(h_value)}.pth')\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(\"TRAINING COMPLETED\")\n",
    "#     print(f\"Best validation loss: {checkpoint['val_loss']:.6f}\")\n",
    "#     print(f\"Final violation rate: {checkpoint['violation_rate']:.4f}\")\n",
    "    \n",
    "#     return train_losses, val_losses, violation_rates\n",
    "\n",
    "# # Block 8: Main Execution (Modified)\n",
    "# def main():\n",
    "#     try:\n",
    "#         DATA_DIR = get_data_directory()\n",
    "#         h_values = [0.0375, 0.084, 0.1575]  # h2, h3, h6\n",
    "#         all_results = {}\n",
    "        \n",
    "#         for h_val in h_values:\n",
    "#             h_category = get_h_category(h_val)\n",
    "#             print(f\"\\nProcessing dataset for h={h_val} ({h_category})\")\n",
    "#             data = load_data(DATA_DIR, h_filter=h_val)\n",
    "            \n",
    "#             train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols, num_sensors = preprocess_data(\n",
    "#                 data, use_enhanced_features=True, h_value=h_val\n",
    "#             )\n",
    "            \n",
    "#             device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#             print(f\"Using device: {device}\")\n",
    "            \n",
    "#             model = EnhancedThermalNet(\n",
    "#                 input_size=train_loader.dataset.tensors[0].shape[1],\n",
    "#                 output_size=num_sensors,\n",
    "#                 hidden_dims=[512, 256, 256, 128],\n",
    "#                 dropout_rate=0.2\n",
    "#             ).to(device)\n",
    "            \n",
    "#             print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters for {num_sensors} sensors\")\n",
    "            \n",
    "#             train_losses, val_losses, violation_rates = train_model_with_conservation_monitoring(\n",
    "#                 model, train_loader, val_loader, device, num_sensors, h_val, epochs=1000, patience=50\n",
    "#             )\n",
    "            \n",
    "#             sensor_depths = np.array([0.0, 0.018, 0.035, 0.053, 0.070, 0.099, 0.105, 0.123, 0.140, 0.158])[:num_sensors]\n",
    "#             sensor_names = [f\"TC{i+1}\" for i in range(num_sensors)]\n",
    "#             results = evaluate_model(model, test_loader, y_scaler, device, sensor_depths, sensor_names)\n",
    "            \n",
    "#             all_results[h_val] = {\n",
    "#                 'model': model,\n",
    "#                 'results': results,\n",
    "#                 'X_scaler': X_scaler,\n",
    "#                 'y_scaler': y_scaler,\n",
    "#                 'tc_cols': tc_cols,\n",
    "#                 'violation_rates': violation_rates\n",
    "#             }\n",
    "            \n",
    "#             print(\"\\n\" + \"=\"*50)\n",
    "#             print(f\"EVALUATION RESULTS FOR {h_category} (Temperature Â°C)\")\n",
    "#             print(\"=\"*50)\n",
    "#             print(f\"{'Sensor':<15} {'RMSE (Â°C)':<12} {'MAE (Â°C)':<12} {'MAPE (%)':<12} {'RÂ² Score':<12}\")\n",
    "#             print(\"-\"*65)\n",
    "#             for i, name in enumerate(results['sensor_names_rev']):\n",
    "#                 print(f\"{name:<15} {results['rmse_temp'][i]:<12.3f} {results['mae_temp'][i]:<12.3f} \"\n",
    "#                       f\"{results['mape_temp'][i]:<12.2f} {results['r2_temp'][i]:<12.3f}\")\n",
    "#             print(\"-\"*65)\n",
    "#             print(f\"{'Overall':<15} {results['overall_rmse_temp']:<12.3f} {results['overall_mae_temp']:<12.3f} \"\n",
    "#                   f\"{'':<12} {results['overall_r2_temp']:<12.3f}\")\n",
    "            \n",
    "#             print(\"\\n\" + \"=\"*50)\n",
    "#             print(f\"PHYSICS CONSERVATION MONITORING FOR {h_category}\")\n",
    "#             print(\"=\"*50)\n",
    "#             final_violation_rate = violation_rates[-1] if violation_rates else 0.0\n",
    "#             print(f\"Final energy conservation violation rate: {final_violation_rate:.4f}\")\n",
    "        \n",
    "#         return all_results\n",
    "                \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in main execution: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     model, results, X_scaler, y_scaler, tc_cols, violation_rates = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33098429",
   "metadata": {},
   "source": [
    "# 2 code blocks below for (.)pre-processing, training and (..) prediction for Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55e86761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data directory: data/new_processed_fix_new\n",
      "\n",
      "Processing dataset for h=0.0375 (h2)\n",
      "Loading data from: data/new_processed_fix_new\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_699s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv (622 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv (576 rows)\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_surf0_630s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf1_675s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv (576 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs0_surf0_585s - Sheet2_processed.csv (564 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_surf0_598s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs25_surf0_439s-Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs0_surf0_719s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs0_surf0_644s-Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_749s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf0_429s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv (501 rows)\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs25_surf0_660s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_692s-Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs0_surf0_431s - Sheet1_processed.csv (560 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf1_830s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs25_surf0_493s - Sheet1_processed.csv (572 rows)\n",
      "Loaded: cleaned_h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv (548 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv (703 rows)\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf1_491s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs92_surf0_115s - Sheet1_processed.csv (500 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv (540 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_651s - Sheet4_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv\n",
      "Loaded: cleaned_h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv (508 rows)\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs92_surf0_628s - Sheet4_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_389s - Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_389s - Sheet4_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs0_surf0_825s-Sheer3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf1_613s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_surf0_747s-Sheet3_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv\n",
      "Skipping (not h=0.0375): cleaned_h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv\n",
      "\n",
      "==================================================\n",
      "FILE PROCESSING SUMMARY\n",
      "==================================================\n",
      "Total files scanned     : 77\n",
      "Files successfully loaded: 12\n",
      "Files skipped (filtered): 65\n",
      "Files skipped (unmatched): 0\n",
      "==================================================\n",
      "\n",
      "Total data loaded: 6770 rows, 19 columns\n",
      "Starting data preprocessing...\n",
      "Features: 14, Targets: 3\n",
      "Theory columns: 3, TC columns: 3\n",
      "Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3']\n",
      "After removing missing values: 6770 samples\n",
      "Training samples: 4739\n",
      "Validation samples: 1015\n",
      "Test samples: 1016\n",
      "Using device: cpu\n",
      "Model created with 404828 parameters for 3 sensors\n",
      "Starting training with 3 sensors for h=0.0375...\n",
      "Epoch [  10/1000] Train: 0.016810 Val: 0.011621 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  20/1000] Train: 0.013099 Val: 0.010406 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  30/1000] Train: 0.010565 Val: 0.008185 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  40/1000] Train: 0.006322 Val: 0.004419 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [  50/1000] Train: 0.004858 Val: 0.003582 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [ 100/1000] Train: 0.002771 Val: 0.001866 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00100000\n",
      "Epoch [ 200/1000] Train: 0.001697 Val: 0.001163 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00050000\n",
      "Epoch [ 300/1000] Train: 0.001369 Val: 0.001028 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00006250\n",
      "Epoch [ 400/1000] Train: 0.001352 Val: 0.001004 Violations: 0.0000 (âœ… EXCELLENT) LR: 0.00000781\n",
      "Early stopping at epoch 476\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "Best validation loss: 0.001000\n",
      "Final violation rate: 0.0000\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS FOR h2 (Temperature Â°C)\n",
      "==================================================\n",
      "Sensor          RMSE (Â°C)    MAE (Â°C)     MAPE (%)     RÂ² Score    \n",
      "-----------------------------------------------------------------\n",
      "TC3             0.824        0.531        0.16         0.998       \n",
      "TC2             0.361        0.268        0.07         0.997       \n",
      "TC1             0.254        0.175        0.05         0.999       \n",
      "-----------------------------------------------------------------\n",
      "Overall         0.540        0.325                     0.998       \n",
      "\n",
      "==================================================\n",
      "PHYSICS CONSERVATION MONITORING FOR h2\n",
      "==================================================\n",
      "Final energy conservation violation rate: 0.0000\n",
      "\n",
      "Processing dataset for h=0.1575 (h6)\n",
      "Loading data from: data/new_processed_fix_new\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_699s - Sheet1_processed.csv (717 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv (532 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv (4621 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv (566 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv (621 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_630s - Sheet2_processed.csv (618 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv (516 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_675s - Sheet3_processed.csv (856 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv (826 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv (795 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv (932 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_585s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_598s - Sheet1_processed.csv (528 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_surf0_439s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv (912 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_719s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv (1087 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_644s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv (790 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_749s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_429s - Sheet2_processed.csv (880 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv (585 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv (4247 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv (611 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv (554 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_surf0_660s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_692s-Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv (589 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_431s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv (970 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv (803 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv (1411 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_830s - Sheet2_processed.csv (528 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv (787 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_surf0_493s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv (531 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_491s - Sheet3_processed.csv (702 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv (547 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_surf0_115s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv (3952 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_651s - Sheet4_processed.csv (554 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv (865 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv (870 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv (550 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_628s - Sheet4_processed.csv (539 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_389s - Sheet3_processed.csv (537 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_389s - Sheet4_processed.csv (2139 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv (887 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_825s-Sheer3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv (790 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_613s - Sheet1_processed.csv (550 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv (794 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_747s-Sheet3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv (4087 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv (849 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv (534 rows)\n",
      "\n",
      "==================================================\n",
      "FILE PROCESSING SUMMARY\n",
      "==================================================\n",
      "Total files scanned     : 77\n",
      "Files successfully loaded: 46\n",
      "Files skipped (filtered): 31\n",
      "Files skipped (unmatched): 0\n",
      "==================================================\n",
      "\n",
      "Total data loaded: 48187 rows, 26 columns\n",
      "Starting data preprocessing...\n",
      "Features: 21, Targets: 10\n",
      "Theory columns: 10, TC columns: 10\n",
      "Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10']\n",
      "After removing missing values: 48187 samples\n",
      "Training samples: 33730\n",
      "Validation samples: 7228\n",
      "Test samples: 7229\n",
      "Using device: cpu\n",
      "Model created with 409549 parameters for 10 sensors\n",
      "Starting training with 10 sensors for h=0.1575...\n",
      "Epoch [  10/1000] Train: 50.008423 Val: 50.007409 Violations: 0.2209 (ðŸš¨ CONCERNING) LR: 0.00100000\n",
      "Epoch [  20/1000] Train: 49.912866 Val: 50.007256 Violations: 0.2209 (ðŸš¨ CONCERNING) LR: 0.00100000\n",
      "Epoch [  30/1000] Train: 49.912577 Val: 50.007036 Violations: 0.2209 (ðŸš¨ CONCERNING) LR: 0.00050000\n",
      "Epoch [  40/1000] Train: 49.912434 Val: 50.006977 Violations: 0.2209 (ðŸš¨ CONCERNING) LR: 0.00050000\n",
      "Epoch [  50/1000] Train: 49.912322 Val: 50.007017 Violations: 0.2209 (ðŸš¨ CONCERNING) LR: 0.00025000\n",
      "Epoch [ 100/1000] Train: 49.912216 Val: 50.006819 Violations: 0.2209 (ðŸš¨ CONCERNING) LR: 0.00006250\n",
      "Epoch [ 200/1000] Train: 49.912205 Val: 50.006810 Violations: 0.2209 (ðŸš¨ CONCERNING) LR: 0.00000195\n",
      "Early stopping at epoch 239\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "Best validation loss: 50.006810\n",
      "Final violation rate: 0.2209\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS FOR h6 (Temperature Â°C)\n",
      "==================================================\n",
      "Sensor          RMSE (Â°C)    MAE (Â°C)     MAPE (%)     RÂ² Score    \n",
      "-----------------------------------------------------------------\n",
      "TC10            4.257        2.814        0.91         0.991       \n",
      "TC9             2.004        1.362        0.45         0.998       \n",
      "TC8             1.890        0.929        0.37         0.998       \n",
      "TC7             1.625        0.876        0.33         0.999       \n",
      "TC6             1.470        0.790        0.30         0.999       \n",
      "TC5             1.661        0.747        0.25         0.998       \n",
      "TC4             1.397        1.001        0.32         0.999       \n",
      "TC3             1.141        0.731        0.25         0.999       \n",
      "TC2             1.419        0.968        0.34         0.999       \n",
      "TC1             1.485        1.068        0.36         0.999       \n",
      "-----------------------------------------------------------------\n",
      "Overall         2.018        1.129                     0.998       \n",
      "\n",
      "==================================================\n",
      "PHYSICS CONSERVATION MONITORING FOR h6\n",
      "==================================================\n",
      "Final energy conservation violation rate: 0.2209\n",
      "\n",
      "Training completed for all models.\n"
     ]
    }
   ],
   "source": [
    "#new\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def get_h_category(h_value):\n",
    "    \"\"\"Map h value to category\"\"\"\n",
    "    if abs(h_value - 0.0375) < 0.001:  # h2\n",
    "        return 'h2'\n",
    "    elif abs(h_value - 0.084) < 0.001:  # h3\n",
    "        return 'h3'\n",
    "    elif abs(h_value - 0.1575) < 0.001:  # h6\n",
    "        return 'h6'\n",
    "    else:\n",
    "        # Default fallback - try to determine based on available TC columns\n",
    "        return 'h6'\n",
    "\n",
    "# Your provided functions (unchanged)\n",
    "def preprocess_data(data, test_size=0.3, val_size=0.5, use_enhanced_features=True, h_value=None):\n",
    "    \"\"\"Enhanced data preprocessing with support for variable TC sensors\"\"\"\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    \n",
    "    time_min = data[\"Time\"].min()\n",
    "    time_max = data[\"Time\"].max()\n",
    "    data[\"Time_norm\"] = (data[\"Time\"] - time_min) / (time_max - time_min)\n",
    "    \n",
    "    if use_enhanced_features:\n",
    "        data[\"TimeÂ²\"] = data[\"Time_norm\"] ** 2\n",
    "        data[\"TimeÂ³\"] = data[\"Time_norm\"] ** 3\n",
    "        data[\"Time_sin\"] = np.sin(2 * np.pi * data[\"Time_norm\"])\n",
    "        data[\"Time_cos\"] = np.cos(2 * np.pi * data[\"Time_norm\"])\n",
    "        data[\"flux_abs_interaction\"] = data[\"flux\"] * data[\"abs\"]\n",
    "        data[\"h_flux_interaction\"] = data[\"h\"] * data[\"flux\"]\n",
    "        base_features = [\"Time_norm\", \"TimeÂ²\", \"TimeÂ³\", \"Time_sin\", \"Time_cos\", \n",
    "                         \"h\", \"flux\", \"abs\", \"surf\", \"flux_abs_interaction\", \"h_flux_interaction\"]\n",
    "    else:\n",
    "        data[\"TimeÂ²\"] = data[\"Time_norm\"] ** 2\n",
    "        base_features = [\"Time_norm\", \"TimeÂ²\", \"h\", \"flux\", \"abs\", \"surf\"]\n",
    "    \n",
    "    theory_cols = [c for c in data.columns if c.startswith(\"Theoretical_Temps_\")]\n",
    "    tc_cols = [col for col in data.columns if (col.startswith(\"TC\") and not col.endswith(\"_rate\")) or col.startswith(\"TC_\")]\n",
    "    \n",
    "    h_category = get_h_category(h_value) if h_value is not None else 'h6'\n",
    "    if h_category == 'h2':\n",
    "        num_sensors = 3\n",
    "    elif h_category == 'h3':\n",
    "        num_sensors = 5\n",
    "    else:\n",
    "        num_sensors = 10\n",
    "    \n",
    "    tc_cols = tc_cols[:num_sensors]\n",
    "    theory_cols = theory_cols[:num_sensors] if len(theory_cols) >= num_sensors else theory_cols\n",
    "    \n",
    "    if not tc_cols:\n",
    "        print(\"Warning: No TC columns found. Creating dummy TC columns.\")\n",
    "        tc_cols = [f\"TC_{i+1}\" for i in range(num_sensors)]\n",
    "        for col in tc_cols:\n",
    "            if col not in data.columns:\n",
    "                data[col] = 20 + np.random.normal(0, 5, len(data))\n",
    "    \n",
    "    feature_cols = base_features + theory_cols\n",
    "    feature_cols = [col for col in feature_cols if col in data.columns]\n",
    "    \n",
    "    X = data[feature_cols].copy()\n",
    "    y = data[tc_cols].copy()\n",
    "    filenames = data[\"filename\"] if \"filename\" in data.columns else pd.Series([\"unknown\"] * len(data))\n",
    "    \n",
    "    print(f\"Features: {len(feature_cols)}, Targets: {len(tc_cols)}\")\n",
    "    print(f\"Theory columns: {len(theory_cols)}, TC columns: {len(tc_cols)}\")\n",
    "    print(f\"Feature columns: {feature_cols}\")\n",
    "    print(f\"Target columns: {tc_cols}\")\n",
    "    \n",
    "    mask = ~(X.isnull().any(axis=1) | y.isnull().any(axis=1))\n",
    "    X = X[mask].reset_index(drop=True)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "    filenames = filenames[mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"After removing missing values: {len(X)} samples\")\n",
    "    \n",
    "    if len(X) < 10:\n",
    "        raise ValueError(\"Not enough valid samples for training!\")\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp, _, _ = train_test_split(\n",
    "        X, y, filenames, test_size=test_size, random_state=SEED\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test, _, _ = train_test_split(\n",
    "        X_temp, y_temp, _, test_size=val_size, random_state=SEED\n",
    "    )\n",
    "    \n",
    "    X_scaler = StandardScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "    \n",
    "    X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "    X_val_scaled = X_scaler.transform(X_val)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "    \n",
    "    y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "    y_val_scaled = y_scaler.transform(y_val)\n",
    "    y_test_scaled = y_scaler.transform(y_test)\n",
    "    \n",
    "    model_dir = os.path.join(\"models\", h_category)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    joblib.dump(X_scaler, os.path.join(model_dir, \"X_scaler.pkl\"))\n",
    "    joblib.dump(y_scaler, os.path.join(model_dir, \"y_scaler.pkl\"))\n",
    "    joblib.dump({\"time_min\": time_min, \"time_max\": time_max}, os.path.join(model_dir, \"time_range.pkl\"))\n",
    "    joblib.dump({\"feature_cols\": feature_cols, \"tc_cols\": tc_cols, \"num_sensors\": num_sensors}, os.path.join(model_dir, \"column_info.pkl\"))\n",
    "    \n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "    )\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(X_test_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=min(64, len(X_train)//4), shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=min(64, len(X_val)//4), shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=min(64, len(X_test)//4), shuffle=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols, num_sensors\n",
    "\n",
    "class PhysicsInformedLoss(nn.Module):\n",
    "    \"\"\"Physics-informed loss function with support for variable TC sensors\"\"\"\n",
    "    def __init__(self, smoothness_weight=0.005, gradient_weight=0.0001, physics_weight=0.5, \n",
    "                 conservation_violation_penalty=100.0):\n",
    "        super().__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.smoothness_weight = smoothness_weight\n",
    "        self.gradient_weight = gradient_weight\n",
    "        self.physics_weight = physics_weight\n",
    "        self.conservation_violation_penalty = conservation_violation_penalty\n",
    "        \n",
    "        self.r = 2.0375 * 0.0254\n",
    "        self.A_rec = np.pi * (self.r ** 2)\n",
    "        self.rho = 1836.31\n",
    "        self.cp = 1512\n",
    "        \n",
    "        self.A_rec_tensor = None\n",
    "        self.rho_tensor = None\n",
    "        self.cp_tensor = None\n",
    "        \n",
    "        self.violation_count = 0\n",
    "        self.total_batches = 0\n",
    "    \n",
    "    def _init_tensors(self, device):\n",
    "        if self.A_rec_tensor is None:\n",
    "            self.A_rec_tensor = torch.tensor(self.A_rec, device=device, dtype=torch.float32)\n",
    "            self.rho_tensor = torch.tensor(self.rho, device=device, dtype=torch.float32)\n",
    "            self.cp_tensor = torch.tensor(self.cp, device=device, dtype=torch.float32)\n",
    "    \n",
    "    def compute_physics_loss(self, predictions, targets, inputs, num_sensors):\n",
    "        device = predictions.device\n",
    "        self._init_tensors(device)\n",
    "        \n",
    "        batch_size = predictions.shape[0]\n",
    "        self.total_batches += 1\n",
    "        \n",
    "        try:\n",
    "            flux = inputs[:, 6]\n",
    "            h = inputs[:, 5]\n",
    "            incoming_energy = flux * self.A_rec_tensor\n",
    "            mass = self.rho_tensor * h * self.A_rec_tensor\n",
    "            \n",
    "            theoretical_start_idx = 11\n",
    "            if inputs.shape[1] >= theoretical_start_idx + num_sensors:\n",
    "                theoretical_temps = inputs[:, theoretical_start_idx:theoretical_start_idx + num_sensors]\n",
    "                temp_change = torch.mean(predictions - theoretical_temps, dim=1)\n",
    "            else:\n",
    "                temp_change = torch.mean(predictions - targets, dim=1)\n",
    "            \n",
    "            dt = torch.ones_like(temp_change)\n",
    "            total_energy_stored = mass * self.cp_tensor * temp_change / dt\n",
    "            \n",
    "            conservation_loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "            \n",
    "            violation_mask = total_energy_stored > incoming_energy\n",
    "            if torch.any(violation_mask):\n",
    "                excess = total_energy_stored[violation_mask] - incoming_energy[violation_mask]\n",
    "                relative_violation = excess / (incoming_energy[violation_mask] + 1e-6)\n",
    "                conservation_loss = conservation_loss + self.conservation_violation_penalty * torch.mean(relative_violation ** 2)\n",
    "                self.violation_count += torch.sum(violation_mask).item()\n",
    "            \n",
    "            margin = 0.05 * incoming_energy\n",
    "            soft_violation_mask = (total_energy_stored > incoming_energy - margin) & (~violation_mask)\n",
    "            if torch.any(soft_violation_mask):\n",
    "                soft_excess = total_energy_stored[soft_violation_mask] - incoming_energy[soft_violation_mask]\n",
    "                soft_relative_violation = soft_excess / (incoming_energy[soft_violation_mask] + 1e-6)\n",
    "                soft_penalty = torch.mean(soft_relative_violation ** 2)\n",
    "                conservation_loss = conservation_loss + 0.1 * self.conservation_violation_penalty * soft_penalty\n",
    "            \n",
    "            acceptable_mask = total_energy_stored <= incoming_energy\n",
    "            if torch.any(acceptable_mask):\n",
    "                energy_difference = incoming_energy[acceptable_mask] - total_energy_stored[acceptable_mask]\n",
    "                efficiency_penalty = torch.mean(energy_difference) * 0.01\n",
    "                conservation_loss = conservation_loss + efficiency_penalty\n",
    "            \n",
    "            balance_tolerance = 0.01 * torch.abs(incoming_energy)\n",
    "            energy_difference = torch.abs(incoming_energy - total_energy_stored)\n",
    "            balance_mask = energy_difference <= balance_tolerance\n",
    "            if torch.any(balance_mask):\n",
    "                balance_reward = torch.mean(energy_difference[balance_mask])\n",
    "                conservation_loss = conservation_loss - 0.05 * balance_reward\n",
    "            \n",
    "            return conservation_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Physics loss computation failed: {e}\")\n",
    "            return torch.tensor(100.0, device=device, requires_grad=True)\n",
    "    \n",
    "    def get_violation_rate(self):\n",
    "        if self.total_batches == 0:\n",
    "            return 0.0\n",
    "        total_samples = self.total_batches * 64\n",
    "        return self.violation_count / total_samples if total_samples > 0 else 0.0\n",
    "    \n",
    "    def reset_violation_tracking(self):\n",
    "        self.violation_count = 0\n",
    "        self.total_batches = 0\n",
    "    \n",
    "    def forward(self, predictions, targets, inputs=None, num_sensors=None):\n",
    "        mse_loss = self.mse_loss(predictions, targets)\n",
    "        \n",
    "        if predictions.shape[1] > 1:\n",
    "            smoothness_loss = torch.mean(torch.abs(predictions[:, 1:] - predictions[:, :-1]))\n",
    "            gradient_loss = torch.mean(torch.abs(torch.diff(predictions, dim=1)))\n",
    "        else:\n",
    "            smoothness_loss = torch.tensor(0.0, device=predictions.device)\n",
    "            gradient_loss = torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "        if inputs is not None and num_sensors is not None:\n",
    "            physics_loss = self.compute_physics_loss(predictions, targets, inputs, num_sensors)\n",
    "        else:\n",
    "            physics_loss = torch.tensor(50.0, device=predictions.device, requires_grad=True)\n",
    "        \n",
    "        total_loss = (mse_loss + \n",
    "                     self.smoothness_weight * smoothness_loss + \n",
    "                     self.gradient_weight * gradient_loss +\n",
    "                     self.physics_weight * physics_loss)\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "def train_model_with_conservation_monitoring(model, train_loader, val_loader, device, num_sensors, h_value, epochs=1000, patience=50):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, min_lr=1e-6)\n",
    "    \n",
    "    criterion = PhysicsInformedLoss(\n",
    "        smoothness_weight=0.005,\n",
    "        gradient_weight=0.0001,\n",
    "        physics_weight=0.5,\n",
    "        conservation_violation_penalty=100.0\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    violation_rates = []\n",
    "    \n",
    "    print(f\"Starting training with {num_sensors} sensors for h={h_value}...\")\n",
    "    for epoch in range(epochs):\n",
    "        criterion.reset_violation_tracking()\n",
    "        model.train()\n",
    "        train_loss_epoch = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch, X_batch, num_sensors)\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"Warning: Invalid loss detected at epoch {epoch}\")\n",
    "                continue\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss_epoch += loss.item()\n",
    "        train_loss_epoch /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss_epoch = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                predictions = model(X_batch)\n",
    "                loss = criterion(predictions, y_batch, X_batch, num_sensors)\n",
    "                val_loss_epoch += loss.item()\n",
    "        val_loss_epoch /= len(val_loader)\n",
    "        \n",
    "        violation_rate = criterion.get_violation_rate()\n",
    "        violation_rates.append(violation_rate)\n",
    "        scheduler.step(val_loss_epoch)\n",
    "        train_losses.append(train_loss_epoch)\n",
    "        val_losses.append(val_loss_epoch)\n",
    "        \n",
    "        adjusted_val_loss = val_loss_epoch + (violation_rate * 100)\n",
    "        if adjusted_val_loss < best_val_loss:\n",
    "            best_val_loss = adjusted_val_loss\n",
    "            patience_counter = 0\n",
    "            model_dir = os.path.join(\"models\", get_h_category(h_value))\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss_epoch,\n",
    "                'train_loss': train_loss_epoch,\n",
    "                'violation_rate': violation_rate,\n",
    "            }, os.path.join(model_dir, \"best_thermal_model.pth\"))\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0 or (epoch < 50 and (epoch + 1) % 10 == 0):\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            status = \"âœ… EXCELLENT\" if violation_rate < 0.01 else \"âš ï¸ ACCEPTABLE\" if violation_rate < 0.05 else \"ðŸš¨ CONCERNING\"\n",
    "            print(f\"Epoch [{epoch+1:4d}/{epochs}] \"\n",
    "                  f\"Train: {train_loss_epoch:.6f} \"\n",
    "                  f\"Val: {val_loss_epoch:.6f} \"\n",
    "                  f\"Violations: {violation_rate:.4f} ({status}) \"\n",
    "                  f\"LR: {current_lr:.8f}\")\n",
    "    \n",
    "    model_dir = os.path.join(\"models\", get_h_category(h_value))\n",
    "    checkpoint = torch.load(os.path.join(model_dir, \"best_thermal_model.pth\"))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING COMPLETED\")\n",
    "    print(f\"Best validation loss: {checkpoint['val_loss']:.6f}\")\n",
    "    print(f\"Final violation rate: {checkpoint['violation_rate']:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses, violation_rates\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "# Utility to serialize scaler to bytes\n",
    "def scaler_to_bytes(scaler):\n",
    "    buf = BytesIO()\n",
    "    joblib.dump(scaler, buf)\n",
    "    buf.seek(0)\n",
    "    return buf.read()\n",
    "\n",
    "# Utility to deserialize scaler from bytes\n",
    "def bytes_to_scaler(bytes_obj):\n",
    "    buf = BytesIO(bytes_obj)\n",
    "    scaler = joblib.load(buf)\n",
    "    return scaler\n",
    "\n",
    "# --- Your existing preprocess_data, PhysicsInformedLoss, train_model_with_conservation_monitoring, etc. unchanged ---\n",
    "\n",
    "# Updated main() with single bundle save:\n",
    "def main():\n",
    "    try:\n",
    "        DATA_DIR = get_data_directory()\n",
    "        # h_values = [0.0375, 0.084, 0.1575]  # h2, h3, h6\n",
    "        h_values = [0.0375,0.1575]  # h2, h6\n",
    "\n",
    "        all_results = {}\n",
    "        model_metadata = {}\n",
    "        \n",
    "        all_models_bundle = {}\n",
    "        \n",
    "        for h_val in h_values:\n",
    "            h_category = get_h_category(h_val)\n",
    "            print(f\"\\nProcessing dataset for h={h_val} ({h_category})\")\n",
    "            data = load_data(DATA_DIR, h_filter=h_val)\n",
    "            \n",
    "            train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols, num_sensors = preprocess_data(\n",
    "                data, use_enhanced_features=True, h_value=h_val\n",
    "            )\n",
    "            \n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            print(f\"Using device: {device}\")\n",
    "            \n",
    "            model = EnhancedThermalNet(\n",
    "                input_size=train_loader.dataset.tensors[0].shape[1],\n",
    "                output_size=num_sensors,\n",
    "                hidden_dims=[512, 256, 256, 128],\n",
    "                dropout_rate=0.2\n",
    "            ).to(device)\n",
    "            \n",
    "            print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters for {num_sensors} sensors\")\n",
    "            \n",
    "            train_losses, val_losses, violation_rates = train_model_with_conservation_monitoring(\n",
    "                model, train_loader, val_loader, device, num_sensors, h_val, epochs=1000, patience=50\n",
    "            )\n",
    "            \n",
    "            sensor_depths = np.array([0.0, 0.018, 0.035, 0.053, 0.070, 0.099, 0.105, 0.123, 0.140, 0.158])[:num_sensors]\n",
    "            sensor_names = [f\"TC{i+1}\" for i in range(num_sensors)]\n",
    "            results = evaluate_model(model, test_loader, y_scaler, device, sensor_depths, sensor_names)\n",
    "            \n",
    "            all_results[h_val] = {\n",
    "                'model': model,\n",
    "                'results': results,\n",
    "                'X_scaler': X_scaler,\n",
    "                'y_scaler': y_scaler,\n",
    "                'tc_cols': tc_cols,\n",
    "                'violation_rates': violation_rates\n",
    "            }\n",
    "            \n",
    "            # Save metadata info\n",
    "            model_metadata[h_category] = {\n",
    "                \"h_value\": h_val,\n",
    "                \"num_sensors\": num_sensors,\n",
    "                \"tc_cols\": tc_cols,\n",
    "            }\n",
    "            \n",
    "            # Save model state dict and scalers as bytes inside the bundle\n",
    "            all_models_bundle[h_category] = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'X_scaler_bytes': scaler_to_bytes(X_scaler),\n",
    "                'y_scaler_bytes': scaler_to_bytes(y_scaler),\n",
    "                'tc_cols': tc_cols,\n",
    "                'num_sensors': num_sensors,\n",
    "                'feature_dim': train_loader.dataset.tensors[0].shape[1],\n",
    "                'violation_rates': violation_rates,\n",
    "            }\n",
    "            \n",
    "            # Print evaluation summary\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"EVALUATION RESULTS FOR {h_category} (Temperature Â°C)\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"{'Sensor':<15} {'RMSE (Â°C)':<12} {'MAE (Â°C)':<12} {'MAPE (%)':<12} {'RÂ² Score':<12}\")\n",
    "            print(\"-\"*65)\n",
    "            for i, name in enumerate(results['sensor_names_rev']):\n",
    "                print(f\"{name:<15} {results['rmse_temp'][i]:<12.3f} {results['mae_temp'][i]:<12.3f} \"\n",
    "                      f\"{results['mape_temp'][i]:<12.2f} {results['r2_temp'][i]:<12.3f}\")\n",
    "            print(\"-\"*65)\n",
    "            print(f\"{'Overall':<15} {results['overall_rmse_temp']:<12.3f} {results['overall_mae_temp']:<12.3f} \"\n",
    "                  f\"{'':<12} {results['overall_r2_temp']:<12.3f}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"PHYSICS CONSERVATION MONITORING FOR {h_category}\")\n",
    "            print(\"=\"*50)\n",
    "            final_violation_rate = violation_rates[-1] if violation_rates else 0.0\n",
    "            print(f\"Final energy conservation violation rate: {final_violation_rate:.4f}\")\n",
    "        \n",
    "        # Save entire model+scaler bundle in one .pth file\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        torch.save(all_models_bundle, \"models/all_h_models_bundle.pth\")\n",
    "        \n",
    "        # Save JSON metadata separately\n",
    "        with open(os.path.join(\"models\", \"model_metadata.json\"), \"w\") as f:\n",
    "            json.dump(model_metadata, f, indent=4)\n",
    "        \n",
    "        return all_results\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# Loading example (for inference or prediction later):\n",
    "\n",
    "# def load_bundle_model_and_scalers(h_category):\n",
    "#     bundle = torch.load(\"models/all_h_models_bundle.pth\")\n",
    "#     entry = bundle[h_category]\n",
    "    \n",
    "#     model = EnhancedThermalNet(\n",
    "#         input_size=entry['feature_dim'],\n",
    "#         output_size=entry['num_sensors'],\n",
    "#         hidden_dims=[512, 256, 256, 128],\n",
    "#         dropout_rate=0.2\n",
    "#     )\n",
    "#     model.load_state_dict(entry['model_state_dict'])\n",
    "#     model.eval()\n",
    "    \n",
    "#     X_scaler = bytes_to_scaler(entry['X_scaler_bytes'])\n",
    "#     y_scaler = bytes_to_scaler(entry['y_scaler_bytes'])\n",
    "#     tc_cols = entry['tc_cols']\n",
    "    \n",
    "#     return model, X_scaler, y_scaler, tc_cols\n",
    "\n",
    "# --- Your other code remains unchanged ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()\n",
    "    if results is not None:\n",
    "        print(\"\\nTraining completed for all models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f53b66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "H3 INTERPOLATION EXAMPLE\n",
      "============================================================\n",
      "Loading h-specific models...\n",
      "Found bundled model, loading...\n",
      "âœ“ Successfully loaded h2 model\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 succeeded\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 succeeded\n",
      "âœ“ Loaded scalers for h2\n",
      "âœ“ Successfully loaded h6 model\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 succeeded\n",
      "Deserialization method 1 failed: STACK_GLOBAL requires str\n",
      "Deserialization method 2 succeeded\n",
      "âœ“ Loaded scalers for h6\n",
      "âœ“ Interpolation models loaded successfully!\n",
      "Available models: ['h2', 'h6']\n",
      "\n",
      "ðŸ” PREDICTION COMPARISON\n",
      "==================================================\n",
      "âŒ No direct h3 model available\n",
      "\n",
      "ðŸ“Š Interpolated h3 prediction (from h2 & h6):\n",
      "\n",
      "ðŸ”„ Interpolating h=0.084 using models: ['h2', 'h6']\n",
      "âœ“ Got prediction from h2 (h=0.0375)\n",
      "âœ“ Got prediction from h6 (h=0.1575)\n",
      "ðŸ“Š Linear interpolation: h2(0.0375) --[0.388]-> h6(0.1575)\n",
      "  TC1_tip: 363.375 Â°C\n",
      "  TC2: 361.764 Â°C\n",
      "  TC3: 343.794 Â°C\n",
      "\n",
      "==================================================\n",
      "DIFFERENT INTERPOLATION METHODS\n",
      "==================================================\n",
      "\n",
      "ðŸ“Š Method: linear\n",
      "\n",
      "ðŸ”„ Interpolating h=0.084 using models: ['h2', 'h6']\n",
      "âœ“ Got prediction from h2 (h=0.0375)\n",
      "âœ“ Got prediction from h6 (h=0.1575)\n",
      "ðŸ“Š Linear interpolation: h2(0.0375) --[0.388]-> h6(0.1575)\n",
      "  TC1_tip: 363.375 Â°C\n",
      "  TC2: 361.764 Â°C\n",
      "  TC3: 343.794 Â°C\n",
      "\n",
      "ðŸ“Š Method: weighted_average\n",
      "\n",
      "ðŸ”„ Interpolating h=0.084 using models: ['h2', 'h6']\n",
      "âœ“ Got prediction from h2 (h=0.0375)\n",
      "âœ“ Got prediction from h6 (h=0.1575)\n",
      "ðŸ“Š Weighted interpolation weights: {'h2': 0.714158239143367, 'h6': 0.28584176085663304}\n",
      "  TC1_tip: 364.316 Â°C\n",
      "  TC2: 362.713 Â°C\n",
      "  TC3: 341.377 Â°C\n",
      "  TC4: 102.388 Â°C\n",
      "  TC5: 102.453 Â°C\n",
      "\n",
      "============================================================\n",
      "BATCH INTERPOLATION TEST\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#new\n",
    "# Enhanced Prediction System with H-Value Interpolation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from typing import Dict, List, Union, Optional, Tuple\n",
    "from io import BytesIO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your actual EnhancedThermalNet architecture (keeping the same)\n",
    "class EnhancedThermalNet(nn.Module):\n",
    "    \"\"\"Enhanced thermal neural network with attention and residual connections\"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_dims=[512, 256, 256, 128], dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Input processing\n",
    "        self.input_layer = nn.Linear(input_size, hidden_dims[0])\n",
    "        self.input_norm = nn.LayerNorm(hidden_dims[0])\n",
    "        \n",
    "        # Feature attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_size, max(1, input_size // 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(max(1, input_size // 2), input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Hidden layers with residual connections\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.residual_projections = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            layer = nn.Linear(hidden_dims[i], hidden_dims[i + 1])\n",
    "            norm = nn.LayerNorm(hidden_dims[i + 1])\n",
    "            \n",
    "            if hidden_dims[i] != hidden_dims[i + 1]:\n",
    "                residual_proj = nn.Linear(hidden_dims[i], hidden_dims[i + 1])\n",
    "            else:\n",
    "                residual_proj = nn.Identity()\n",
    "            \n",
    "            self.layers.append(layer)\n",
    "            self.norms.append(norm)\n",
    "            self.residual_projections.append(residual_proj)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_size)\n",
    "        \n",
    "        # Activation and regularization\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using He initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply attention to input features\n",
    "        attention_weights = self.attention(x)\n",
    "        x_attended = x * attention_weights\n",
    "        \n",
    "        # Input processing\n",
    "        x = self.activation(self.input_norm(self.input_layer(x_attended)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Forward through hidden layers with residual connections\n",
    "        for i, (layer, norm, residual_proj) in enumerate(zip(self.layers, self.norms, self.residual_projections)):\n",
    "            residual = residual_proj(x)\n",
    "            x = layer(x)\n",
    "            x = norm(x)\n",
    "            x = self.activation(x)\n",
    "            x = x + residual\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "\n",
    "class HInterpolationPredictor:\n",
    "    \"\"\"Enhanced prediction system with h-value interpolation capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, models_dir=\"models\"):\n",
    "        self.models_dir = models_dir\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.metadata = {}\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # H-value to configuration mapping\n",
    "        self.h_configs = {\n",
    "            0.0375: 'h2',\n",
    "            0.084: 'h3',\n",
    "            0.1575: 'h6'\n",
    "        }\n",
    "        \n",
    "        # Expected TC counts and sensor mapping\n",
    "        self.tc_counts = {\n",
    "            'h2': 3,\n",
    "            'h3': 5,\n",
    "            'h6': 10\n",
    "        }\n",
    "        \n",
    "        self.tc_sensors = {\n",
    "            'h2': ['TC1_tip', 'TC2', 'TC3'],\n",
    "            'h3': ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5'],\n",
    "            'h6': ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10']\n",
    "        }\n",
    "        \n",
    "        # Common sensor mapping for interpolation\n",
    "        self.common_sensors = ['TC1_tip', 'TC2', 'TC3','TC4', 'TC5']\n",
    "        \n",
    "        # Default feature columns\n",
    "        self.default_feature_cols = [\n",
    "            'Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos',\n",
    "            'h', 'flux', 'abs', 'surf',\n",
    "            'flux_abs_interaction', 'h_flux_interaction'\n",
    "        ]\n",
    "    \n",
    "    def load_models(self):\n",
    "        \"\"\"Load all h-specific models and their components\"\"\"\n",
    "        print(\"Loading h-specific models...\")\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(os.path.join(self.models_dir, \"all_h_models_bundle.pth\")):\n",
    "                print(\"Found bundled model, loading...\")\n",
    "                self._load_bundled_models()\n",
    "                return len(self.models) > 0\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load bundled model: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        # Fall back to loading individual models\n",
    "        for h_val, h_name in self.h_configs.items():\n",
    "            try:\n",
    "                self._load_individual_model(h_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {h_name} model: {e}\")\n",
    "        \n",
    "        loaded_models = list(self.models.keys())\n",
    "        print(f\"âœ“ Successfully loaded models: {loaded_models}\")\n",
    "        return len(loaded_models) > 0\n",
    "    \n",
    "    def _deserialize_scaler(self, scaler_bytes):\n",
    "        \"\"\"Deserialize scaler from bytes - Multiple methods\"\"\"\n",
    "        methods = [\n",
    "            lambda: pickle.loads(scaler_bytes),\n",
    "            lambda: joblib.load(BytesIO(scaler_bytes)),  # Fixed from joblib.loads to joblib.load\n",
    "            lambda: pickle.load(BytesIO(scaler_bytes)),\n",
    "            lambda: joblib.load(BytesIO(scaler_bytes))\n",
    "        ]\n",
    "        \n",
    "        for i, method in enumerate(methods, 1):\n",
    "            try:\n",
    "                result = method()\n",
    "                print(f\"Deserialization method {i} succeeded\")\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                print(f\"Deserialization method {i} failed: {e}\")\n",
    "        \n",
    "        # Check if it's already a scaler object\n",
    "        if hasattr(scaler_bytes, 'transform'):\n",
    "            print(\"Scaler is already an object, not bytes\")\n",
    "            return scaler_bytes\n",
    "        \n",
    "        print(f\"All deserialization methods failed. Data type: {type(scaler_bytes)}\")\n",
    "        return None\n",
    "    \n",
    "    def _load_bundled_models(self):\n",
    "        \"\"\"Load from bundled model file\"\"\"\n",
    "        bundle_path = os.path.join(self.models_dir, \"all_h_models_bundle.pth\")\n",
    "        bundle = torch.load(bundle_path, map_location=self.device)\n",
    "        \n",
    "        # Load metadata if available\n",
    "        metadata_path = os.path.join(self.models_dir, \"model_metadata.json\")\n",
    "        if os.path.exists(metadata_path):\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                self.metadata = json.load(f)\n",
    "        \n",
    "        # Extract models and components from bundle\n",
    "        for h_name in ['h2', 'h3', 'h6']:\n",
    "            if h_name in bundle:\n",
    "                h_data = bundle[h_name]\n",
    "                \n",
    "                try:\n",
    "                    # Load model\n",
    "                    if 'model_state_dict' in h_data:\n",
    "                        input_size = h_data.get('feature_dim')\n",
    "                        output_size = h_data.get('num_sensors')\n",
    "                        \n",
    "                        if input_size is None or output_size is None:\n",
    "                            state_dict = h_data['model_state_dict']\n",
    "                            if 'input_layer.weight' in state_dict:\n",
    "                                input_size = state_dict['input_layer.weight'].shape[1]\n",
    "                            if 'output_layer.weight' in state_dict:\n",
    "                                output_size = state_dict['output_layer.weight'].shape[0]\n",
    "                        \n",
    "                        if input_size is not None and output_size is not None:\n",
    "                            model = EnhancedThermalNet(\n",
    "                                input_size=input_size,\n",
    "                                output_size=output_size,\n",
    "                                hidden_dims=[512, 256, 256, 128],\n",
    "                                dropout_rate=0.3\n",
    "                            ).to(self.device)\n",
    "                            \n",
    "                            model.load_state_dict(h_data['model_state_dict'])\n",
    "                            model.eval()\n",
    "                            self.models[h_name] = model\n",
    "                            print(f\"âœ“ Successfully loaded {h_name} model\")\n",
    "                        else:\n",
    "                            print(f\"âœ— Could not determine model dimensions for {h_name}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Load scalers and time range\n",
    "                    if 'X_scaler_bytes' in h_data and 'y_scaler_bytes' in h_data:\n",
    "                        X_scaler = self._deserialize_scaler(h_data['X_scaler_bytes'])\n",
    "                        y_scaler = self._deserialize_scaler(h_data['y_scaler_bytes'])\n",
    "                        time_range = h_data.get('time_range', {'time_min': 0, 'time_max': 7200})\n",
    "                        \n",
    "                        if X_scaler is not None and y_scaler is not None:\n",
    "                            # Create feature columns list\n",
    "                            feature_cols = self.default_feature_cols.copy()\n",
    "                            tc_count = self.tc_counts[h_name]\n",
    "                            for i in range(tc_count):\n",
    "                                feature_cols.append(f'Theoretical_Temps_{i+1}')\n",
    "                            \n",
    "                            # Validate feature count\n",
    "                            if hasattr(X_scaler, 'n_features_in_'):\n",
    "                                expected_features = X_scaler.n_features_in_\n",
    "                                if len(feature_cols) != expected_features:\n",
    "                                    print(f\"Warning: Feature count mismatch for {h_name}. Expected: {expected_features}, Got: {len(feature_cols)}\")\n",
    "                                    if len(feature_cols) > expected_features:\n",
    "                                        feature_cols = feature_cols[:expected_features]\n",
    "                                    else:\n",
    "                                        while len(feature_cols) < expected_features:\n",
    "                                            feature_cols.append(f'feature_{len(feature_cols)}')\n",
    "                            \n",
    "                            self.scalers[h_name] = {\n",
    "                                'X_scaler': X_scaler,\n",
    "                                'y_scaler': y_scaler,\n",
    "                                'time_range': time_range,\n",
    "                                'feature_cols': feature_cols,\n",
    "                                'tc_cols': h_data.get('tc_cols', self.tc_sensors[h_name])\n",
    "                            }\n",
    "                            print(f\"âœ“ Loaded scalers for {h_name}\")\n",
    "                        else:\n",
    "                            print(f\"âœ— Failed to deserialize scalers for {h_name}\")\n",
    "                            self._create_dummy_scalers(h_name, input_size, output_size)\n",
    "                    else:\n",
    "                        print(f\"âœ— Missing scaler bytes for {h_name}\")\n",
    "                        self._create_dummy_scalers(h_name, input_size, output_size)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âœ— Error loading {h_name}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "    \n",
    "    def _create_dummy_scalers(self, h_name, input_size, output_size):\n",
    "        \"\"\"Create dummy scalers when deserialization fails\"\"\"\n",
    "        try:\n",
    "            X_scaler = StandardScaler()\n",
    "            y_scaler = MinMaxScaler()  # Match training scaler\n",
    "            \n",
    "            # Fit with dummy data\n",
    "            X_dummy = np.random.randn(10, input_size)\n",
    "            y_dummy = np.random.randn(10, output_size)  # Shape (10, output_size)\n",
    "            \n",
    "            X_scaler.fit(X_dummy)\n",
    "            y_scaler.fit(y_dummy)  # Fit on multi-output shape\n",
    "            \n",
    "            feature_cols = self.default_feature_cols.copy()\n",
    "            tc_count = self.tc_counts[h_name]\n",
    "            for i in range(tc_count):\n",
    "                feature_cols.append(f'Theoretical_Temps_{i+1}')\n",
    "                \n",
    "            if len(feature_cols) > input_size:\n",
    "                feature_cols = feature_cols[:input_size]\n",
    "            else:\n",
    "                while len(feature_cols) < input_size:\n",
    "                    feature_cols.append(f'dummy_feature_{len(feature_cols)}')\n",
    "            \n",
    "            self.scalers[h_name] = {\n",
    "                'X_scaler': X_scaler,\n",
    "                'y_scaler': y_scaler,\n",
    "                'time_range': {'time_min': 0, 'time_max': 7200},\n",
    "                'feature_cols': feature_cols,\n",
    "                'tc_cols': self.tc_sensors[h_name]\n",
    "            }\n",
    "            print(f\"âœ“ Created dummy scalers for {h_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to create dummy scalers for {h_name}: {e}\")\n",
    "    \n",
    "    def get_h_category(self, h_value):\n",
    "        \"\"\"Get h category from h value\"\"\"\n",
    "        for h_key, category in self.h_configs.items():\n",
    "            if abs(h_value - h_key) < 0.001:\n",
    "                return category\n",
    "        \n",
    "        closest_h = min(self.h_configs.keys(), key=lambda x: abs(x - h_value))\n",
    "        category = self.h_configs[closest_h]\n",
    "        print(f\"Warning: h={h_value} not exactly matched. Using closest: h={closest_h} ({category})\")\n",
    "        return category\n",
    "    \n",
    "    def _prepare_features(self, time, h, flux, abs_val, surf, theoretical_temps, h_category):\n",
    "        \"\"\"Prepare feature vector for a specific h-category\"\"\"\n",
    "        scalers = self.scalers[h_category]\n",
    "        \n",
    "        # Validate time against training range\n",
    "        time_range = scalers['time_range']\n",
    "        if time > time_range['time_max']:\n",
    "            print(f\"Warning: Predicting beyond training time range ({time} > {time_range['time_max']})\")\n",
    "            time = time_range['time_max']\n",
    "        elif time < time_range['time_min']:\n",
    "            print(f\"Warning: Predicting before training time range ({time} < {time_range['time_min']})\")\n",
    "            time = time_range['time_min']\n",
    "        \n",
    "        time_norm = (time - time_range['time_min']) / (time_range['time_max'] - time_range['time_min'])\n",
    "        \n",
    "        # Validate theoretical temperatures\n",
    "        expected_count = self.tc_counts[h_category]\n",
    "        if len(theoretical_temps) != expected_count:\n",
    "            if len(theoretical_temps) < expected_count:\n",
    "                theoretical_temps = list(theoretical_temps) + [theoretical_temps[-1]] * (expected_count - len(theoretical_temps))\n",
    "            else:\n",
    "                theoretical_temps = theoretical_temps[:expected_count]\n",
    "        \n",
    "        # Create feature vector\n",
    "        features = {\n",
    "            'Time_norm': time_norm,\n",
    "            'TimeÂ²': time_norm ** 2,\n",
    "            'TimeÂ³': time_norm ** 3,\n",
    "            'Time_sin': np.sin(2 * np.pi * time_norm),\n",
    "            'Time_cos': np.cos(2 * np.pi * time_norm),\n",
    "            'h': h,\n",
    "            'flux': flux,\n",
    "            'abs': abs_val,\n",
    "            'surf': surf,\n",
    "            'flux_abs_interaction': flux * abs_val,\n",
    "            'h_flux_interaction': h * flux\n",
    "        }\n",
    "        \n",
    "        # Add theoretical temperatures\n",
    "        for i, temp in enumerate(theoretical_temps):\n",
    "            features[f'Theoretical_Temps_{i+1}'] = temp\n",
    "        \n",
    "        # Create DataFrame and select features\n",
    "        input_data = pd.DataFrame([features])\n",
    "        feature_cols = scalers['feature_cols']\n",
    "        \n",
    "        # Ensure all required features exist\n",
    "        for col in feature_cols:\n",
    "            if col not in input_data.columns:\n",
    "                input_data[col] = 0.0\n",
    "        \n",
    "        return input_data[feature_cols].values\n",
    "    \n",
    "    def predict_single_model(self, time, h, flux, abs_val, surf, theoretical_temps, h_category):\n",
    "        \"\"\"Make prediction using a single h-specific model\"\"\"\n",
    "        try:\n",
    "            if h_category not in self.models:\n",
    "                raise ValueError(f\"Model for {h_category} not loaded. Available: {list(self.models.keys())}\")\n",
    "            \n",
    "            model = self.models[h_category]\n",
    "            scalers = self.scalers[h_category]\n",
    "            \n",
    "            # Prepare features\n",
    "            X_input = self._prepare_features(time, h, flux, abs_val, surf, theoretical_temps, h_category)\n",
    "            \n",
    "            # Scale and predict\n",
    "            X_scaled = scalers['X_scaler'].transform(X_input)\n",
    "            X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                predictions_scaled = model(X_tensor)  # Shape (1, output_size)\n",
    "                predictions_scaled = predictions_scaled.cpu().numpy()  # Shape (1, output_size)\n",
    "            \n",
    "            # Inverse transform\n",
    "            predictions_temp = scalers['y_scaler'].inverse_transform(predictions_scaled).flatten()\n",
    "            \n",
    "            # Map to all sensor names\n",
    "            tc_cols = scalers['tc_cols']\n",
    "            predicted_temperatures = {sensor_name: temp for sensor_name, temp in zip(tc_cols, predictions_temp)}\n",
    "            \n",
    "            return predicted_temperatures\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Single model prediction failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def interpolate_h_predictions(self, time, target_h, flux, abs_val, surf, theoretical_temps, \n",
    "                                source_models=['h2', 'h6'], method='linear'):\n",
    "        \"\"\"\n",
    "        Predict for target h-value by interpolating between source models\n",
    "        \n",
    "        Args:\n",
    "            time: Time value\n",
    "            target_h: Target h-value to predict for\n",
    "            flux, abs_val, surf: Physical parameters\n",
    "            theoretical_temps: Theoretical temperatures (will be adjusted per model)\n",
    "            source_models: List of source models to use for interpolation\n",
    "            method: Interpolation method ('linear', 'weighted_average')\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with interpolated predictions\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ”„ Interpolating h={target_h} using models: {source_models}\")\n",
    "        \n",
    "        # Get predictions from source models\n",
    "        source_predictions = {}\n",
    "        source_h_values = {}\n",
    "        \n",
    "        for model_name in source_models:\n",
    "            if model_name not in self.models:\n",
    "                print(f\"âŒ Model {model_name} not available, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Get the actual h-value for this model\n",
    "            model_h = next(h for h, name in self.h_configs.items() if name == model_name)\n",
    "            source_h_values[model_name] = model_h\n",
    "            \n",
    "            # Adjust theoretical temps for this model\n",
    "            model_tc_count = self.tc_counts[model_name]\n",
    "            adjusted_temps = theoretical_temps[:model_tc_count] if len(theoretical_temps) >= model_tc_count else \\\n",
    "                            theoretical_temps + [theoretical_temps[-1]] * (model_tc_count - len(theoretical_temps))\n",
    "            \n",
    "            # Get prediction\n",
    "            pred = self.predict_single_model(time, model_h, flux, abs_val, surf, adjusted_temps, model_name)\n",
    "            if pred is not None:\n",
    "                source_predictions[model_name] = pred\n",
    "                print(f\"âœ“ Got prediction from {model_name} (h={model_h})\")\n",
    "            else:\n",
    "                print(f\"âŒ Failed to get prediction from {model_name}\")\n",
    "        \n",
    "        if len(source_predictions) < 2:\n",
    "            print(f\"âŒ Need at least 2 source predictions, got {len(source_predictions)}\")\n",
    "            return None\n",
    "        \n",
    "        # Perform interpolation\n",
    "        if method == 'linear':\n",
    "            return self._linear_interpolation(target_h, source_predictions, source_h_values)\n",
    "        elif method == 'weighted_average':\n",
    "            return self._weighted_average_interpolation(target_h, source_predictions, source_h_values)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown interpolation method: {method}\")\n",
    "    \n",
    "    def _linear_interpolation(self, target_h, source_predictions, source_h_values):\n",
    "        \"\"\"Linear interpolation between two models\"\"\"\n",
    "        model_names = list(source_predictions.keys())\n",
    "        \n",
    "        if len(model_names) == 2:\n",
    "            model1, model2 = model_names\n",
    "            h1, h2 = source_h_values[model1], source_h_values[model2]\n",
    "            \n",
    "            # Ensure h1 < h2\n",
    "            if h1 > h2:\n",
    "                model1, model2 = model2, model1\n",
    "                h1, h2 = h2, h1\n",
    "            \n",
    "            # Linear interpolation weight\n",
    "            if h2 != h1:  # Avoid division by zero\n",
    "                weight = (target_h - h1) / (h2 - h1)\n",
    "                weight = max(0, min(1, weight))  # Clamp to [0, 1]\n",
    "            else:\n",
    "                weight = 0.5\n",
    "            \n",
    "            print(f\"ðŸ“Š Linear interpolation: {model1}({h1}) --[{weight:.3f}]-> {model2}({h2})\")\n",
    "            \n",
    "            # Interpolate for common sensors\n",
    "            interpolated_temps = {}\n",
    "            pred1 = source_predictions[model1]\n",
    "            pred2 = source_predictions[model2]\n",
    "            \n",
    "            for sensor in self.common_sensors:\n",
    "                if sensor in pred1 and sensor in pred2:\n",
    "                    interpolated_temps[sensor] = (1 - weight) * pred1[sensor] + weight * pred2[sensor]\n",
    "            \n",
    "            return {\n",
    "                'predicted_temperatures': interpolated_temps,\n",
    "                'interpolation_info': {\n",
    "                    'method': 'linear',\n",
    "                    'source_models': model_names,\n",
    "                    'source_h_values': [h1, h2],\n",
    "                    'target_h': target_h,\n",
    "                    'weight': weight,\n",
    "                    'common_sensors': list(interpolated_temps.keys())\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            # For more than 2 models, use weighted average\n",
    "            return self._weighted_average_interpolation(target_h, source_predictions, source_h_values)\n",
    "    \n",
    "    def _weighted_average_interpolation(self, target_h, source_predictions, source_h_values):\n",
    "        \"\"\"Weighted average interpolation using inverse distance weighting\"\"\"\n",
    "        weights = {}\n",
    "        total_weight = 0\n",
    "        \n",
    "        # Calculate weights based on inverse distance\n",
    "        for model_name, h_val in source_h_values.items():\n",
    "            distance = abs(target_h - h_val)\n",
    "            if distance == 0:\n",
    "                # Exact match, give full weight\n",
    "                weights = {model_name: 1.0}\n",
    "                total_weight = 1.0\n",
    "                break\n",
    "            else:\n",
    "                weight = 1.0 / (distance ** 2)  # Inverse square distance\n",
    "                weights[model_name] = weight\n",
    "                total_weight += weight\n",
    "        \n",
    "        # Normalize weights\n",
    "        if total_weight > 0:\n",
    "            for model_name in weights:\n",
    "                weights[model_name] /= total_weight\n",
    "        \n",
    "        print(f\"ðŸ“Š Weighted interpolation weights: {weights}\")\n",
    "        \n",
    "        # Weighted interpolation for common sensors\n",
    "        interpolated_temps = {}\n",
    "        \n",
    "        for sensor in self.common_sensors:\n",
    "            weighted_temp = 0\n",
    "            valid_predictions = 0\n",
    "            \n",
    "            for model_name, weight in weights.items():\n",
    "                if model_name in source_predictions and sensor in source_predictions[model_name]:\n",
    "                    weighted_temp += weight * source_predictions[model_name][sensor]\n",
    "                    valid_predictions += 1\n",
    "            \n",
    "            if valid_predictions > 0:\n",
    "                interpolated_temps[sensor] = weighted_temp\n",
    "        \n",
    "        return {\n",
    "            'predicted_temperatures': interpolated_temps,\n",
    "            'interpolation_info': {\n",
    "                'method': 'weighted_average',\n",
    "                'source_models': list(weights.keys()),\n",
    "                'source_h_values': list(source_h_values.values()),\n",
    "                'target_h': target_h,\n",
    "                'weights': weights,\n",
    "                'common_sensors': list(interpolated_temps.keys())\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def predict_h3_from_h2_h6(self, time, flux, abs_val, surf, theoretical_temps, method='linear'):\n",
    "        \"\"\"\n",
    "        Convenience function to predict h3 using h2 and h6 models\n",
    "        \n",
    "        Args:\n",
    "            time: Time in seconds\n",
    "            flux: Heat flux\n",
    "            abs_val: Absorption coefficient\n",
    "            surf: Surface emissivity  \n",
    "            theoretical_temps: List of theoretical temperatures (will be adjusted per model)\n",
    "            method: Interpolation method\n",
    "        \n",
    "        Returns:\n",
    "            Interpolated predictions for h3\n",
    "        \"\"\"\n",
    "        target_h = 0.084  # h3 value\n",
    "        return self.interpolate_h_predictions(\n",
    "            time, target_h, flux, abs_val, surf, theoretical_temps,\n",
    "            source_models=['h2', 'h6'], method=method\n",
    "        )\n",
    "    \n",
    "    def compare_predictions(self, time, flux, abs_val, surf, theoretical_temps):\n",
    "        \"\"\"\n",
    "        Compare direct h3 prediction vs interpolated h3 prediction\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with comparison results\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ” PREDICTION COMPARISON\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Direct h3 prediction\n",
    "        if 'h3' in self.models:\n",
    "            print(\"ðŸ“Š Direct h3 prediction:\")\n",
    "            h3_direct = self.predict_single_model(\n",
    "                time, 0.084, flux, abs_val, surf, theoretical_temps[:5], 'h3'\n",
    "            )\n",
    "            results['direct_h3'] = h3_direct\n",
    "            \n",
    "            if h3_direct:\n",
    "                for sensor, temp in h3_direct.items():\n",
    "                    print(f\"  {sensor}: {temp:.3f} Â°C\")\n",
    "        else:\n",
    "            print(\"âŒ No direct h3 model available\")\n",
    "            results['direct_h3'] = None\n",
    "        \n",
    "        # Interpolated h3 prediction\n",
    "        print(\"\\nðŸ“Š Interpolated h3 prediction (from h2 & h6):\")\n",
    "        h3_interpolated = self.predict_h3_from_h2_h6(time, flux, abs_val, surf, theoretical_temps)\n",
    "        results['interpolated_h3'] = h3_interpolated\n",
    "        \n",
    "        if h3_interpolated and 'predicted_temperatures' in h3_interpolated:\n",
    "            for sensor, temp in h3_interpolated['predicted_temperatures'].items():\n",
    "                print(f\"  {sensor}: {temp:.3f} Â°C\")\n",
    "        \n",
    "        # Calculate differences if both exist\n",
    "        if results['direct_h3'] and results['interpolated_h3']:\n",
    "            print(\"\\nðŸ“Š Differences (Direct - Interpolated):\")\n",
    "            differences = {}\n",
    "            \n",
    "            direct_temps = results['direct_h3']\n",
    "            interp_temps = results['interpolated_h3']['predicted_temperatures']\n",
    "            \n",
    "            for sensor in self.common_sensors:\n",
    "                if sensor in direct_temps and sensor in interp_temps:\n",
    "                    diff = direct_temps[sensor] - interp_temps[sensor]\n",
    "                    differences[sensor] = diff\n",
    "                    print(f\"  {sensor}: {diff:+.3f} Â°C\")\n",
    "            \n",
    "            results['differences'] = differences\n",
    "            \n",
    "            # Calculate RMSE\n",
    "            if differences:\n",
    "                rmse = np.sqrt(np.mean([diff**2 for diff in differences.values()]))\n",
    "                print(f\"\\nðŸ“Š RMSE between direct and interpolated: {rmse:.3f} Â°C\")\n",
    "                results['rmse'] = rmse\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Global predictor instance\n",
    "global_predictor = None\n",
    "\n",
    "def load_interpolation_models():\n",
    "    \"\"\"Load models for interpolation\"\"\"\n",
    "    global global_predictor\n",
    "    global_predictor = HInterpolationPredictor()\n",
    "    success = global_predictor.load_models()\n",
    "    \n",
    "    if success:\n",
    "        print(\"âœ“ Interpolation models loaded successfully!\")\n",
    "        print(f\"Available models: {list(global_predictor.models.keys())}\")\n",
    "        return global_predictor\n",
    "    else:\n",
    "        print(\"âœ— Failed to load interpolation models\")\n",
    "        return None\n",
    "\n",
    "def predict_h3_interpolated(time, flux, abs_val, surf, theoretical_temps, method='linear'):\n",
    "    \"\"\"\n",
    "    Main function to predict h3 using h2 and h6 models\n",
    "    \n",
    "    Args:\n",
    "        time: Time value in seconds\n",
    "        flux: Heat flux\n",
    "        abs_val: Absorption coefficient\n",
    "        surf: Surface emissivity\n",
    "        theoretical_temps: List of theoretical temperatures\n",
    "        method: 'linear' or 'weighted_average'\n",
    "    \n",
    "    Returns:\n",
    "        Interpolated h3 predictions\n",
    "    \"\"\"\n",
    "    global global_predictor\n",
    "    \n",
    "    if global_predictor is None:\n",
    "        print(\"Loading interpolation models...\")\n",
    "        global_predictor = load_interpolation_models()\n",
    "        if global_predictor is None:\n",
    "            return None\n",
    "    \n",
    "    return global_predictor.predict_h3_from_h2_h6(time, flux, abs_val, surf, theoretical_temps, method)\n",
    "\n",
    "def run_interpolation_example():\n",
    "    \"\"\"Run a complete example of h3 interpolation\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"H3 INTERPOLATION EXAMPLE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load models\n",
    "    predictor = load_interpolation_models()\n",
    "    if not predictor:\n",
    "        return\n",
    "    \n",
    "    # Example parameters\n",
    "    time = 500  # 500s\n",
    "    flux = 25900\n",
    "    abs_val = 25\n",
    "    surf = 0.98\n",
    "    theoretical_temps = [374.528458491455,403.299198639828,403.299272630082,403.299346620336,403.29942061059,403.299451482664,403.291408371526,403.291408371526,403.291408371526,403.291408371526]\n",
    "    \n",
    "    # Compare predictions\n",
    "    comparison = predictor.compare_predictions(time, flux, abs_val, surf, theoretical_temps)\n",
    "    \n",
    "    # Try different interpolation methods\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"DIFFERENT INTERPOLATION METHODS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    methods = ['linear', 'weighted_average']\n",
    "    for method in methods:\n",
    "        print(f\"\\nðŸ“Š Method: {method}\")\n",
    "        result = predictor.predict_h3_from_h2_h6(time, flux, abs_val, surf, theoretical_temps, method)\n",
    "        if result and 'predicted_temperatures' in result:\n",
    "            for sensor, temp in result['predicted_temperatures'].items():\n",
    "                print(f\"  {sensor}: {temp:.3f} Â°C\")\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# Example usage and testing functions\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the interpolation example\n",
    "    run_interpolation_example()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BATCH INTERPOLATION TEST\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # # Test with multiple time points\n",
    "    # predictor = load_interpolation_models()\n",
    "    # if predictor:\n",
    "    #     time_points = [600, 1200, 1800, 2400, 3600]  # Different time points\n",
    "    #     theoretical_temps = [25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0]\n",
    "        \n",
    "    #     print(\"Time (s) | TC1_tip | TC2    | TC3    | Method\")\n",
    "    #     print(\"-\" * 50)\n",
    "        \n",
    "    #     for time in time_points:\n",
    "    #         result = predict_h3_interpolated(\n",
    "    #             time=time,\n",
    "    #             flux=21250,\n",
    "    #             abs_val=3,\n",
    "    #             surf=0.98,\n",
    "    #             theoretical_temps=theoretical_temps,\n",
    "    #             method='linear'\n",
    "    #         )\n",
    "            \n",
    "    #         if result and 'predicted_temperatures' in result:\n",
    "    #             temps = result['predicted_temperatures']\n",
    "    #             print(f\"{time:8d} | {temps.get('TC1_tip', 0):.3f}  | {temps.get('TC2', 0):.3f} | {temps.get('TC3', 0):.3f} | Linear\")\n",
    "\n",
    "def validate_interpolation_accuracy(predictor, test_points=10):\n",
    "    \"\"\"\n",
    "    Validate interpolation accuracy by testing on known h3 data\n",
    "    (if h3 model is available)\n",
    "    \"\"\"\n",
    "    if 'h3' not in predictor.models:\n",
    "        print(\"âŒ No h3 model available for validation\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"INTERPOLATION ACCURACY VALIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Generate test scenarios\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    \n",
    "    validation_results = []\n",
    "    total_error = 0\n",
    "    valid_comparisons = 0\n",
    "    \n",
    "    for i in range(test_points):\n",
    "        # Random test parameters\n",
    "        time = np.random.uniform(300, 6000)  # 5min to 100min\n",
    "        flux = np.random.uniform(15000, 25000)\n",
    "        abs_val = np.random.uniform(2, 4)\n",
    "        surf = np.random.uniform(0.95, 0.99)\n",
    "        \n",
    "        # Random theoretical temperatures (increasing with depth)\n",
    "        base_temp = np.random.uniform(20, 30)\n",
    "        theoretical_temps = [base_temp + i*5 + np.random.normal(0, 1) for i in range(10)]\n",
    "        \n",
    "        # Get direct h3 prediction\n",
    "        direct_pred = predictor.predict_single_model(\n",
    "            time, 0.084, flux, abs_val, surf, theoretical_temps[:5], 'h3'\n",
    "        )\n",
    "        \n",
    "        # Get interpolated prediction\n",
    "        interp_result = predictor.predict_h3_from_h2_h6(\n",
    "            time, flux, abs_val, surf, theoretical_temps, method='linear'\n",
    "        )\n",
    "        \n",
    "        if direct_pred and interp_result and 'predicted_temperatures' in interp_result:\n",
    "            interp_pred = interp_result['predicted_temperatures']\n",
    "            \n",
    "            # Calculate errors for common sensors\n",
    "            test_result = {\n",
    "                'test_id': i+1,\n",
    "                'time': time,\n",
    "                'direct': direct_pred,\n",
    "                'interpolated': interp_pred,\n",
    "                'errors': {}\n",
    "            }\n",
    "            \n",
    "            for sensor in predictor.common_sensors:\n",
    "                if sensor in direct_pred and sensor in interp_pred:\n",
    "                    error = abs(direct_pred[sensor] - interp_pred[sensor])\n",
    "                    test_result['errors'][sensor] = error\n",
    "                    total_error += error\n",
    "                    valid_comparisons += 1\n",
    "            \n",
    "            validation_results.append(test_result)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    if valid_comparisons > 0:\n",
    "        mean_absolute_error = total_error / valid_comparisons\n",
    "        \n",
    "        print(f\"ðŸ“Š Validation Results (n={test_points}):\")\n",
    "        print(f\"  Mean Absolute Error: {mean_absolute_error:.3f} Â°C\")\n",
    "        print(f\"  Valid Comparisons: {valid_comparisons}\")\n",
    "        \n",
    "        # Show detailed results for first few tests\n",
    "        print(f\"\\nðŸ“‹ Detailed Results (first 5 tests):\")\n",
    "        print(\"Test | Sensor  | Direct | Interp | Error\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        for i, result in enumerate(validation_results[:5]):\n",
    "            for sensor, error in result['errors'].items():\n",
    "                direct_temp = result['direct'][sensor]\n",
    "                interp_temp = result['interpolated'][sensor]\n",
    "                print(f\"{i+1:4d} | {sensor:7s} | {direct_temp:6.2f} | {interp_temp:6.2f} | {error:5.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'mean_absolute_error': mean_absolute_error,\n",
    "            'validation_results': validation_results,\n",
    "            'valid_comparisons': valid_comparisons\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        print(\"âŒ No valid comparisons could be made\")\n",
    "        return None\n",
    "\n",
    "def plot_interpolation_comparison(predictor, save_plot=False):\n",
    "    \"\"\"\n",
    "    Create a plot comparing direct vs interpolated predictions over time\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'h3' not in predictor.models:\n",
    "            print(\"âŒ No h3 model available for plotting\")\n",
    "            return\n",
    "        \n",
    "        # # Time series\n",
    "        # time_points = np.linspace(300, 6000, 20)  # 5min to 100min\n",
    "        \n",
    "        # # Fixed parameters for comparison\n",
    "        # flux = 21250\n",
    "        # abs_val = 3\n",
    "        # surf = 0.98\n",
    "        # theoretical_temps = [25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0]\n",
    "        \n",
    "        # Collect predictions\n",
    "        results = {'time': [], 'direct': {}, 'interpolated': {}}\n",
    "        \n",
    "        for sensor in predictor.common_sensors:\n",
    "            results['direct'][sensor] = []\n",
    "            results['interpolated'][sensor] = []\n",
    "        \n",
    "        for time in time_points:\n",
    "            results['time'].append(time/60)  # Convert to minutes\n",
    "            \n",
    "            # Direct prediction\n",
    "            direct_pred = predictor.predict_single_model(\n",
    "                time, 0.084, flux, abs_val, surf, theoretical_temps[:5], 'h3'\n",
    "            )\n",
    "            \n",
    "            # Interpolated prediction\n",
    "            interp_result = predictor.predict_h3_from_h2_h6(\n",
    "                time, flux, abs_val, surf, theoretical_temps, method='linear'\n",
    "            )\n",
    "            \n",
    "            for sensor in predictor.common_sensors:\n",
    "                if direct_pred and sensor in direct_pred:\n",
    "                    results['direct'][sensor].append(direct_pred[sensor])\n",
    "                else:\n",
    "                    results['direct'][sensor].append(np.nan)\n",
    "                \n",
    "                if (interp_result and 'predicted_temperatures' in interp_result and \n",
    "                    sensor in interp_result['predicted_temperatures']):\n",
    "                    results['interpolated'][sensor].append(interp_result['predicted_temperatures'][sensor])\n",
    "                else:\n",
    "                    results['interpolated'][sensor].append(np.nan)\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        colors = ['blue', 'red', 'green']\n",
    "        \n",
    "        for i, sensor in enumerate(predictor.common_sensors):\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            \n",
    "            # Plot direct predictions\n",
    "            plt.plot(results['time'], results['direct'][sensor], \n",
    "                    'o-', color=colors[i], label='Direct H3', linewidth=2, markersize=6)\n",
    "            \n",
    "            # Plot interpolated predictions\n",
    "            plt.plot(results['time'], results['interpolated'][sensor], \n",
    "                    's--', color=colors[i], alpha=0.7, label='Interpolated (H2+H6)', linewidth=2, markersize=4)\n",
    "            \n",
    "            plt.title(f'{sensor} Temperature Prediction')\n",
    "            plt.xlabel('Time (minutes)')\n",
    "            plt.ylabel('Temperature (Â°C)')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Overall comparison plot\n",
    "        plt.subplot(2, 2, 4)\n",
    "        for i, sensor in enumerate(predictor.common_sensors):\n",
    "            direct_vals = np.array(results['direct'][sensor])\n",
    "            interp_vals = np.array(results['interpolated'][sensor])\n",
    "            \n",
    "            # Remove NaN values for correlation\n",
    "            mask = ~(np.isnan(direct_vals) | np.isnan(interp_vals))\n",
    "            if np.sum(mask) > 0:\n",
    "                plt.scatter(direct_vals[mask], interp_vals[mask], \n",
    "                           color=colors[i], alpha=0.7, label=sensor, s=50)\n",
    "        \n",
    "        # Perfect prediction line\n",
    "        all_temps = []\n",
    "        for sensor in predictor.common_sensors:\n",
    "            all_temps.extend([t for t in results['direct'][sensor] if not np.isnan(t)])\n",
    "            all_temps.extend([t for t in results['interpolated'][sensor] if not np.isnan(t)])\n",
    "        \n",
    "        if all_temps:\n",
    "            temp_range = [min(all_temps), max(all_temps)]\n",
    "            plt.plot(temp_range, temp_range, 'k--', alpha=0.5, label='Perfect Prediction')\n",
    "        \n",
    "        plt.xlabel('Direct H3 Prediction (Â°C)')\n",
    "        plt.ylabel('Interpolated Prediction (Â°C)')\n",
    "        plt.title('Direct vs Interpolated Correlation')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_plot:\n",
    "            plt.savefig('h3_interpolation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "            print(\"ðŸ“Š Plot saved as 'h3_interpolation_comparison.png'\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"âŒ Matplotlib not available for plotting\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating plot: {e}\")\n",
    "\n",
    "# Advanced interpolation methods\n",
    "class AdvancedInterpolator:\n",
    "    \"\"\"Advanced interpolation techniques for neural network predictions\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def polynomial_interpolation(h_values, predictions, target_h, degree=2):\n",
    "        \"\"\"\n",
    "        Polynomial interpolation of predictions\n",
    "        \n",
    "        Args:\n",
    "            h_values: List of h-values\n",
    "            predictions: Dict of predictions for each h-value  \n",
    "            target_h: Target h-value\n",
    "            degree: Polynomial degree\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from scipy.interpolate import lagrange\n",
    "            \n",
    "            interpolated = {}\n",
    "            \n",
    "            # Get common sensors\n",
    "            common_sensors = set(predictions[list(predictions.keys())[0]].keys())\n",
    "            for pred in predictions.values():\n",
    "                common_sensors = common_sensors.intersection(set(pred.keys()))\n",
    "            \n",
    "            for sensor in common_sensors:\n",
    "                # Extract temperature values for this sensor\n",
    "                temps = [predictions[h][sensor] for h in h_values]\n",
    "                \n",
    "                # Create polynomial interpolator\n",
    "                poly = lagrange(h_values, temps)\n",
    "                interpolated[sensor] = float(poly(target_h))\n",
    "            \n",
    "            return interpolated\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"âŒ SciPy not available for polynomial interpolation\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Polynomial interpolation failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def spline_interpolation(h_values, predictions, target_h):\n",
    "        \"\"\"\n",
    "        Spline interpolation of predictions\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from scipy.interpolate import CubicSpline\n",
    "            \n",
    "            interpolated = {}\n",
    "            \n",
    "            # Get common sensors\n",
    "            common_sensors = set(predictions[list(predictions.keys())[0]].keys())\n",
    "            for pred in predictions.values():\n",
    "                common_sensors = common_sensors.intersection(set(pred.keys()))\n",
    "            \n",
    "            for sensor in common_sensors:\n",
    "                # Extract temperature values for this sensor\n",
    "                temps = [predictions[h][sensor] for h in h_values]\n",
    "                \n",
    "                # Create spline interpolator\n",
    "                if len(h_values) >= 3:  # Need at least 3 points for cubic spline\n",
    "                    spline = CubicSpline(h_values, temps)\n",
    "                    interpolated[sensor] = float(spline(target_h))\n",
    "                else:\n",
    "                    # Fall back to linear interpolation\n",
    "                    from scipy.interpolate import interp1d\n",
    "                    linear_interp = interp1d(h_values, temps, kind='linear')\n",
    "                    interpolated[sensor] = float(linear_interp(target_h))\n",
    "            \n",
    "            return interpolated\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"âŒ SciPy not available for spline interpolation\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Spline interpolation failed: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e640945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENHANCED H3 INTERPOLATION TEST\n",
      "============================================================\n",
      "Loading enhanced interpolation system...\n",
      "Bundle loaded successfully\n",
      "Available models in bundle: ['h2', 'h6']\n",
      "\n",
      "Loading h2 model...\n",
      "h2 - Input size: 14, Output size: 3\n",
      "âœ“ h2 model loaded successfully\n",
      "Scaler deserialization method 1 succeeded\n",
      "Scaler deserialization method 1 succeeded\n",
      "Creating default feature columns for h2\n",
      "âœ“ h2 scalers loaded successfully\n",
      "  Features: 14 columns\n",
      "  Outputs: 3 sensors\n",
      "\n",
      "Loading h6 model...\n",
      "h6 - Input size: 21, Output size: 10\n",
      "âœ“ h6 model loaded successfully\n",
      "Scaler deserialization method 1 succeeded\n",
      "Scaler deserialization method 1 succeeded\n",
      "Creating default feature columns for h6\n",
      "âœ“ h6 scalers loaded successfully\n",
      "  Features: 21 columns\n",
      "  Outputs: 10 sensors\n",
      "\n",
      "Successfully loaded 2 models: ['h2', 'h6']\n",
      "\n",
      "--- PHYSICS_WEIGHTED INTERPOLATION ---\n",
      "Debug h2: Expected features: 14\n",
      "Debug h2: Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3']\n",
      "Debug h2: Final feature array shape: (1, 14)\n",
      "Debug h2: Feature values: [7.58333333e-02 5.75069444e-03 4.36094329e-04 4.58649554e-01\n",
      " 8.88617233e-01]...\n",
      "Debug h6: Expected features: 21\n",
      "Debug h6: Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10']\n",
      "Debug h6: Final feature array shape: (1, 21)\n",
      "Debug h6: Feature values: [7.58333333e-02 5.75069444e-03 4.36094329e-04 4.58649554e-01\n",
      " 8.88617233e-01]...\n",
      "Linear weight: 0.388, Physics weight: 0.626\n",
      "Predicted temperatures:\n",
      "  TC1_tip: 356.88 Â°C\n",
      "  TC2: 353.31 Â°C\n",
      "  TC3: 335.30 Â°C\n",
      "Method info: {'method': 'physics_weighted', 'linear_weight': 0.38750000000000007, 'physics_weight': np.float64(0.626419153378347), 'thermal_inertia_ratio': 2.24, 'penetration_adjustment': np.float64(0.5972978834458671)}\n",
      "\n",
      "--- THERMAL_DIFFUSION INTERPOLATION ---\n",
      "Debug h2: Expected features: 14\n",
      "Debug h2: Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3']\n",
      "Debug h2: Final feature array shape: (1, 14)\n",
      "Debug h2: Feature values: [7.58333333e-02 5.75069444e-03 4.36094329e-04 4.58649554e-01\n",
      " 8.88617233e-01]...\n",
      "Debug h6: Expected features: 21\n",
      "Debug h6: Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10']\n",
      "Debug h6: Final feature array shape: (1, 21)\n",
      "Debug h6: Feature values: [7.58333333e-02 5.75069444e-03 4.36094329e-04 4.58649554e-01\n",
      " 8.88617233e-01]...\n",
      "Predicted temperatures:\n",
      "  TC1_tip: 360.41 Â°C\n",
      "  TC2: 356.81 Â°C\n",
      "  TC3: 326.11 Â°C\n",
      "Method info: {'method': 'thermal_diffusion', 'time_weight': 0.3, 'characteristic_times': {'h2': 1561.7816549999998, 'h3_target': 7836.395632127999, 'h6': 27549.828394199998}}\n",
      "\n",
      "--- LINEAR INTERPOLATION ---\n",
      "Debug h2: Expected features: 14\n",
      "Debug h2: Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3']\n",
      "Debug h2: Final feature array shape: (1, 14)\n",
      "Debug h2: Feature values: [7.58333333e-02 5.75069444e-03 4.36094329e-04 4.58649554e-01\n",
      " 8.88617233e-01]...\n",
      "Debug h6: Expected features: 21\n",
      "Debug h6: Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10']\n",
      "Debug h6: Final feature array shape: (1, 21)\n",
      "Debug h6: Feature values: [7.58333333e-02 5.75069444e-03 4.36094329e-04 4.58649554e-01\n",
      " 8.88617233e-01]...\n",
      "Predicted temperatures:\n",
      "  TC1_tip: 359.47 Â°C\n",
      "  TC2: 355.87 Â°C\n",
      "  TC3: 328.57 Â°C\n",
      "Method info: {'method': 'linear', 'weight': 0.38750000000000007}\n",
      "\n",
      "--- METHOD COMPARISON ---\n",
      "Sensor     physics_weighted thermal_diffusion linear      \n",
      "-------------------------------------------------\n",
      "TC1_tip    356.88       360.41       359.47       \n",
      "TC2        353.31       356.81       355.87       \n",
      "TC3        335.30       326.11       328.57       \n",
      "TC4        N/A          N/A          N/A          \n",
      "TC5        N/A          N/A          N/A          \n",
      "\n",
      "============================================================\n",
      "ENHANCED INTERPOLATION COMPLETED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ImprovedPhysicsLoss(nn.Module):\n",
    "    \"\"\"Enhanced physics-informed loss with better thermal modeling\"\"\"\n",
    "    def __init__(self, thermal_conductivity=2.5, density=1836.31, specific_heat=1512):\n",
    "        super().__init__()\n",
    "        self.k = thermal_conductivity  # W/(mÂ·K) - thermal conductivity\n",
    "        self.rho = density            # kg/mÂ³ - density  \n",
    "        self.cp = specific_heat       # J/(kgÂ·K) - specific heat\n",
    "        self.alpha = self.k / (self.rho * self.cp)  # thermal diffusivity\n",
    "        \n",
    "    def heat_equation_residual(self, T_pred, inputs, dt=1.0):\n",
    "        \"\"\"\n",
    "        Compute residual of 1D heat equation: âˆ‚T/âˆ‚t = Î±âˆ‡Â²T + Q/(Ïcp)\n",
    "        where Q is heat source term\n",
    "        \"\"\"\n",
    "        # Extract variables\n",
    "        time = inputs[:, 0:1].requires_grad_(True)\n",
    "        depth = inputs[:, 5:6].requires_grad_(True)  # h represents depth\n",
    "        flux = inputs[:, 6:7]\n",
    "        \n",
    "        # Compute gradients using automatic differentiation\n",
    "        T_t = torch.autograd.grad(T_pred.sum(), time, create_graph=True)[0]\n",
    "        T_h = torch.autograd.grad(T_pred.sum(), depth, create_graph=True)[0]\n",
    "        T_hh = torch.autograd.grad(T_h.sum(), depth, create_graph=True)[0]\n",
    "        \n",
    "        # Heat source term (normalized by material properties)\n",
    "        Q_source = flux / (self.rho * self.cp)\n",
    "        \n",
    "        # Heat equation residual\n",
    "        residual = T_t - self.alpha * T_hh - Q_source\n",
    "        \n",
    "        return torch.mean(residual**2)\n",
    "    \n",
    "    def thermal_continuity_loss(self, predictions):\n",
    "        \"\"\"Ensure temperature continuity across sensors\"\"\"\n",
    "        if predictions.shape[1] > 1:\n",
    "            # Temperature should change smoothly across depth\n",
    "            temp_gradients = torch.diff(predictions, dim=1)\n",
    "            # Penalize extreme gradients\n",
    "            continuity_loss = torch.mean(torch.abs(temp_gradients))\n",
    "            return continuity_loss\n",
    "        return torch.tensor(0.0, device=predictions.device)\n",
    "    \n",
    "    def energy_conservation_loss(self, predictions, targets, inputs):\n",
    "        \"\"\"Enhanced energy conservation with proper thermal modeling\"\"\"\n",
    "        batch_size = predictions.shape[0]\n",
    "        \n",
    "        # Extract parameters\n",
    "        flux = inputs[:, 6]  # Heat flux W/mÂ²\n",
    "        h_value = inputs[:, 5]  # Depth/thickness\n",
    "        \n",
    "        # Calculate material properties\n",
    "        volume = h_value * 1.0  # Assuming unit area\n",
    "        mass = self.rho * volume\n",
    "        \n",
    "        # Energy balance: Energy_in - Energy_stored = Energy_out\n",
    "        # For steady state: Energy_in â‰ˆ Energy_stored + losses\n",
    "        \n",
    "        # Energy input (from flux)\n",
    "        energy_in = flux  # W/mÂ²\n",
    "        \n",
    "        # Energy stored (temperature change * thermal capacity)\n",
    "        temp_change = torch.mean(predictions - targets, dim=1)\n",
    "        energy_stored = mass * self.cp * temp_change / 3600  # per hour\n",
    "        \n",
    "        # Conservation residual\n",
    "        conservation_residual = torch.abs(energy_in - energy_stored) / (energy_in + 1e-6)\n",
    "        \n",
    "        return torch.mean(conservation_residual**2)\n",
    "\n",
    "class EnhancedThermalInterpolator:\n",
    "    \"\"\"Improved thermal interpolation with physics constraints\"\"\"\n",
    "    \n",
    "    def __init__(self, models_dict, scalers_dict):\n",
    "        self.models = models_dict\n",
    "        self.scalers = scalers_dict\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Move models to device\n",
    "        for model in self.models.values():\n",
    "            model.to(self.device)\n",
    "            model.eval()\n",
    "    \n",
    "    def physics_aware_interpolation(self, time, target_h, flux, abs_val, surf, \n",
    "                                  theoretical_temps, method='physics_weighted'):\n",
    "        \"\"\"\n",
    "        Physics-aware interpolation between h2 and h6 models\n",
    "        \n",
    "        Args:\n",
    "            time: Time value\n",
    "            target_h: Target h-value (0.084 for h3)\n",
    "            flux, abs_val, surf: Physical parameters\n",
    "            theoretical_temps: Theoretical temperatures\n",
    "            method: Interpolation method\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get predictions from source models\n",
    "        source_predictions = {}\n",
    "        source_h_values = {'h2': 0.0375, 'h6': 0.1575}\n",
    "        \n",
    "        for model_name in ['h2', 'h6']:\n",
    "            if model_name in self.models:\n",
    "                pred = self._predict_single_model(\n",
    "                    time, source_h_values[model_name], flux, abs_val, surf, \n",
    "                    theoretical_temps, model_name\n",
    "                )\n",
    "                if pred is not None:\n",
    "                    source_predictions[model_name] = pred\n",
    "        \n",
    "        if len(source_predictions) != 2:\n",
    "            raise ValueError(f\"Need both h2 and h6 predictions, got: {list(source_predictions.keys())}\")\n",
    "        \n",
    "        # Apply physics-aware interpolation\n",
    "        if method == 'physics_weighted':\n",
    "            return self._physics_weighted_interpolation(\n",
    "                target_h, source_predictions, source_h_values, flux, abs_val\n",
    "            )\n",
    "        elif method == 'thermal_diffusion':\n",
    "            return self._thermal_diffusion_interpolation(\n",
    "                target_h, source_predictions, source_h_values, time, flux\n",
    "            )\n",
    "        else:\n",
    "            return self._standard_linear_interpolation(\n",
    "                target_h, source_predictions, source_h_values\n",
    "            )\n",
    "    \n",
    "    def _physics_weighted_interpolation(self, target_h, predictions, h_values, flux, abs_val):\n",
    "        \"\"\"\n",
    "        Weight interpolation based on thermal physics principles\n",
    "        \"\"\"\n",
    "        h1, h2 = h_values['h2'], h_values['h6']\n",
    "        \n",
    "        # Standard linear weight\n",
    "        linear_weight = (target_h - h1) / (h2 - h1)\n",
    "        \n",
    "        # Physics-based adjustment\n",
    "        # Thicker materials have higher thermal inertia\n",
    "        thermal_inertia_ratio = target_h / h1  # Relative to h2\n",
    "        \n",
    "        # Heat flux penetration - exponential decay with depth\n",
    "        penetration_h1 = np.exp(-abs_val * h1)\n",
    "        penetration_target = np.exp(-abs_val * target_h)\n",
    "        penetration_h2 = np.exp(-abs_val * h2)\n",
    "        \n",
    "        # Adjust weight based on heat penetration similarity\n",
    "        penetration_weight = (penetration_target - penetration_h1) / (penetration_h2 - penetration_h1)\n",
    "        \n",
    "        # Combine weights (60% linear, 40% physics-based)\n",
    "        physics_weight = 0.6 * linear_weight + 0.4 * penetration_weight\n",
    "        physics_weight = max(0, min(1, physics_weight))  # Clamp to [0,1]\n",
    "        \n",
    "        print(f\"Linear weight: {linear_weight:.3f}, Physics weight: {physics_weight:.3f}\")\n",
    "        \n",
    "        # Interpolate temperatures\n",
    "        interpolated_temps = {}\n",
    "        pred_h2 = predictions['h2']\n",
    "        pred_h6 = predictions['h6']\n",
    "        \n",
    "        common_sensors = ['TC1_tip', 'TC2', 'TC3','TC4', 'TC5']\n",
    "        for sensor in common_sensors:\n",
    "            if sensor in pred_h2 and sensor in pred_h6:\n",
    "                temp_interp = (1 - physics_weight) * pred_h2[sensor] + physics_weight * pred_h6[sensor]\n",
    "                interpolated_temps[sensor] = temp_interp\n",
    "        \n",
    "        return {\n",
    "            'predicted_temperatures': interpolated_temps,\n",
    "            'interpolation_info': {\n",
    "                'method': 'physics_weighted',\n",
    "                'linear_weight': linear_weight,\n",
    "                'physics_weight': physics_weight,\n",
    "                'thermal_inertia_ratio': thermal_inertia_ratio,\n",
    "                'penetration_adjustment': penetration_weight - linear_weight\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _thermal_diffusion_interpolation(self, target_h, predictions, h_values, time, flux):\n",
    "        \"\"\"\n",
    "        Interpolation based on thermal diffusion equations\n",
    "        \"\"\"\n",
    "        h1, h2 = h_values['h2'], h_values['h6']\n",
    "        \n",
    "        # Thermal diffusivity (Î± = k/(Ïcp))\n",
    "        alpha = 2.5 / (1836.31 * 1512)  # mÂ²/s\n",
    "        \n",
    "        # Characteristic diffusion time for each thickness\n",
    "        t_char_h1 = h1**2 / alpha\n",
    "        t_char_target = target_h**2 / alpha\n",
    "        t_char_h2 = h2**2 / alpha\n",
    "        \n",
    "        # Time-dependent weight based on diffusion characteristics\n",
    "        if time < t_char_target:\n",
    "            # Early time - surface effects dominate, closer to thinner model\n",
    "            time_weight = 0.3\n",
    "        else:\n",
    "            # Later time - bulk effects, standard interpolation\n",
    "            time_weight = (t_char_target - t_char_h1) / (t_char_h2 - t_char_h1)\n",
    "        \n",
    "        time_weight = max(0, min(1, time_weight))\n",
    "        \n",
    "        # Apply interpolation\n",
    "        interpolated_temps = {}\n",
    "        pred_h2 = predictions['h2']\n",
    "        pred_h6 = predictions['h6']\n",
    "        \n",
    "        common_sensors = ['TC1_tip', 'TC2', 'TC3','TC4', 'TC5']\n",
    "        for sensor in common_sensors:\n",
    "            if sensor in pred_h2 and sensor in pred_h6:\n",
    "                temp_interp = (1 - time_weight) * pred_h2[sensor] + time_weight * pred_h6[sensor]\n",
    "                interpolated_temps[sensor] = temp_interp\n",
    "        \n",
    "        return {\n",
    "            'predicted_temperatures': interpolated_temps,\n",
    "            'interpolation_info': {\n",
    "                'method': 'thermal_diffusion',\n",
    "                'time_weight': time_weight,\n",
    "                'characteristic_times': {\n",
    "                    'h2': t_char_h1,\n",
    "                    'h3_target': t_char_target,\n",
    "                    'h6': t_char_h2\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _standard_linear_interpolation(self, target_h, predictions, h_values):\n",
    "        \"\"\"Standard linear interpolation\"\"\"\n",
    "        h1, h2 = h_values['h2'], h_values['h6']\n",
    "        weight = (target_h - h1) / (h2 - h1)\n",
    "        \n",
    "        interpolated_temps = {}\n",
    "        pred_h2 = predictions['h2']\n",
    "        pred_h6 = predictions['h6']\n",
    "        \n",
    "        common_sensors = ['TC1_tip', 'TC2', 'TC3','TC4', 'TC5']\n",
    "        for sensor in common_sensors:\n",
    "            if sensor in pred_h2 and sensor in pred_h6:\n",
    "                temp_interp = (1 - weight) * pred_h2[sensor] + weight * pred_h6[sensor]\n",
    "                interpolated_temps[sensor] = temp_interp\n",
    "        \n",
    "        return {\n",
    "            'predicted_temperatures': interpolated_temps,\n",
    "            'interpolation_info': {\n",
    "                'method': 'linear',\n",
    "                'weight': weight\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _predict_single_model(self, time, h, flux, abs_val, surf, theoretical_temps, h_category):\n",
    "        \"\"\"Predict using a single model with enhanced feature engineering\"\"\"\n",
    "        try:\n",
    "            if h_category not in self.models:\n",
    "                return None\n",
    "            \n",
    "            model = self.models[h_category]\n",
    "            scaler_info = self.scalers[h_category]\n",
    "            \n",
    "            # Enhanced feature preparation with thermal physics\n",
    "            X_input = self._prepare_enhanced_features(\n",
    "                time, h, flux, abs_val, surf, theoretical_temps, h_category\n",
    "            )\n",
    "            \n",
    "            # Scale and predict\n",
    "            X_scaled = scaler_info['X_scaler'].transform(X_input)\n",
    "            X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                predictions_scaled = model(X_tensor)\n",
    "                predictions_scaled = predictions_scaled.cpu().numpy()\n",
    "            \n",
    "            # Inverse transform with bounds checking\n",
    "            predictions_temp = scaler_info['y_scaler'].inverse_transform(predictions_scaled).flatten()\n",
    "            \n",
    "            # Apply physical constraints\n",
    "            predictions_temp = self._apply_physical_constraints(\n",
    "                predictions_temp, flux, abs_val, h, theoretical_temps\n",
    "            )\n",
    "            \n",
    "            # Map to sensor names\n",
    "            tc_cols = scaler_info['tc_cols']\n",
    "            predicted_temperatures = {\n",
    "                sensor_name: temp for sensor_name, temp in zip(tc_cols, predictions_temp)\n",
    "            }\n",
    "            \n",
    "            return predicted_temperatures\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Single model prediction failed for {h_category}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _prepare_enhanced_features(self, time, h, flux, abs_val, surf, theoretical_temps, h_category):\n",
    "        \"\"\"Enhanced feature preparation with thermal physics and robust feature handling\"\"\"\n",
    "        # Get time normalization parameters\n",
    "        time_range = self.scalers[h_category]['time_range']\n",
    "        time_norm = (time - time_range['time_min']) / (time_range['time_max'] - time_range['time_min'])\n",
    "        \n",
    "        # Enhanced thermal features\n",
    "        thermal_diffusivity = 2.5 / (1836.31 * 1512)  # mÂ²/s\n",
    "        fourier_number = thermal_diffusivity * time / (h**2)  # Dimensionless time\n",
    "        biot_number = flux * h / 2.5  # Heat transfer characteristic\n",
    "        \n",
    "        # Get feature columns from scalers\n",
    "        feature_cols = self.scalers[h_category]['feature_cols']\n",
    "        \n",
    "        # Debug: print feature information\n",
    "        print(f\"Debug {h_category}: Expected features: {len(feature_cols)}\")\n",
    "        print(f\"Debug {h_category}: Feature columns: {feature_cols}\")\n",
    "        \n",
    "        # If feature_cols is empty or problematic, create default feature set\n",
    "        if not feature_cols or len(feature_cols) == 0:\n",
    "            print(f\"Warning: Empty feature_cols for {h_category}, creating default features\")\n",
    "            \n",
    "            # Create default feature set based on model input size\n",
    "            if hasattr(self.scalers[h_category]['X_scaler'], 'n_features_in_'):\n",
    "                expected_features = self.scalers[h_category]['X_scaler'].n_features_in_\n",
    "                print(f\"Expected features from scaler: {expected_features}\")\n",
    "                \n",
    "                # Create default feature column names\n",
    "                tc_count = {'h2': 3, 'h3': 5, 'h6': 10}[h_category]\n",
    "                feature_cols = [\n",
    "                    'Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos',\n",
    "                    'h', 'flux', 'abs', 'surf',\n",
    "                    'flux_abs_interaction', 'h_flux_interaction'\n",
    "                ]\n",
    "                \n",
    "                # Add theoretical temperature columns\n",
    "                for i in range(tc_count):\n",
    "                    feature_cols.append(f'Theoretical_Temps_{i+1}')\n",
    "                \n",
    "                # Adjust to match expected count\n",
    "                if len(feature_cols) > expected_features:\n",
    "                    feature_cols = feature_cols[:expected_features]\n",
    "                elif len(feature_cols) < expected_features:\n",
    "                    # Add dummy features if needed\n",
    "                    while len(feature_cols) < expected_features:\n",
    "                        feature_cols.append(f'dummy_feature_{len(feature_cols)}')\n",
    "                \n",
    "                print(f\"Created default feature_cols: {feature_cols}\")\n",
    "            else:\n",
    "                # Fallback if no information available\n",
    "                print(\"No feature information available, using minimal feature set\")\n",
    "                feature_cols = ['Time_norm', 'h', 'flux', 'abs', 'surf']\n",
    "        \n",
    "        # Prepare base features dictionary\n",
    "        features = {\n",
    "            'Time_norm': time_norm,\n",
    "            'TimeÂ²': time_norm ** 2,\n",
    "            'TimeÂ³': time_norm ** 3,\n",
    "            'Time_sin': np.sin(2 * np.pi * time_norm),\n",
    "            'Time_cos': np.cos(2 * np.pi * time_norm),\n",
    "            'h': h,\n",
    "            'flux': flux,\n",
    "            'abs': abs_val,\n",
    "            'surf': surf,\n",
    "            'flux_abs_interaction': flux * abs_val,\n",
    "            'h_flux_interaction': h * flux,\n",
    "            'fourier_number': fourier_number,\n",
    "            'biot_number': biot_number,\n",
    "            'thermal_penetration': np.sqrt(thermal_diffusivity * time)\n",
    "        }\n",
    "        \n",
    "        # Add theoretical temperatures\n",
    "        tc_count = {'h2': 3, 'h3': 5, 'h6': 10}[h_category]\n",
    "        adjusted_temps = theoretical_temps[:tc_count] if len(theoretical_temps) >= tc_count else \\\n",
    "                        theoretical_temps + [theoretical_temps[-1]] * (tc_count - len(theoretical_temps))\n",
    "        \n",
    "        for i, temp in enumerate(adjusted_temps):\n",
    "            features[f'Theoretical_Temps_{i+1}'] = temp\n",
    "        \n",
    "        # Create feature array matching expected columns\n",
    "        feature_array = []\n",
    "        for col in feature_cols:\n",
    "            if col in features:\n",
    "                feature_array.append(features[col])\n",
    "            elif 'theoretical' in col.lower() or 'temps' in col.lower():\n",
    "                # Default theoretical temperature\n",
    "                feature_array.append(adjusted_temps[0] if adjusted_temps else 300.0)\n",
    "            elif 'fourier' in col.lower():\n",
    "                feature_array.append(fourier_number)\n",
    "            elif 'biot' in col.lower():\n",
    "                feature_array.append(biot_number)\n",
    "            elif 'penetration' in col.lower():\n",
    "                feature_array.append(np.sqrt(thermal_diffusivity * time))\n",
    "            elif 'time' in col.lower():\n",
    "                feature_array.append(time_norm)\n",
    "            elif col == 'h':\n",
    "                feature_array.append(h)\n",
    "            elif col == 'flux':\n",
    "                feature_array.append(flux)\n",
    "            elif col == 'abs':\n",
    "                feature_array.append(abs_val)\n",
    "            elif col == 'surf':\n",
    "                feature_array.append(surf)\n",
    "            else:\n",
    "                # Default value for unknown features\n",
    "                feature_array.append(0.0)\n",
    "                print(f\"Warning: Unknown feature '{col}', using default value 0.0\")\n",
    "        \n",
    "        result = np.array(feature_array).reshape(1, -1)\n",
    "        print(f\"Debug {h_category}: Final feature array shape: {result.shape}\")\n",
    "        print(f\"Debug {h_category}: Feature values: {result[0][:5]}...\")  # First 5 values\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _apply_physical_constraints(self, predictions, flux, abs_val, h, theoretical_temps):\n",
    "        \"\"\"Apply physical constraints to predictions\"\"\"\n",
    "        # Temperature should not be below ambient\n",
    "        ambient_temp = min(theoretical_temps) if theoretical_temps else 20\n",
    "        predictions = np.maximum(predictions, ambient_temp)\n",
    "        \n",
    "        # Maximum temperature constraint based on heat flux\n",
    "        max_temp_estimate = ambient_temp + flux / 100  # Rough estimate\n",
    "        predictions = np.minimum(predictions, max_temp_estimate)\n",
    "        \n",
    "        # Ensure monotonic decrease with depth (for most cases)\n",
    "        if len(predictions) > 1:\n",
    "            for i in range(1, len(predictions)):\n",
    "                if predictions[i] > predictions[i-1]:\n",
    "                    predictions[i] = predictions[i-1] * 0.99  # Small decrease\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "def create_enhanced_interpolation_system(models_bundle_path=\"models/all_h_models_bundle.pth\"):\n",
    "    \"\"\"\n",
    "    Create the enhanced interpolation system with robust feature handling\n",
    "    \"\"\"\n",
    "    print(\"Loading enhanced interpolation system...\")\n",
    "    \n",
    "    try:\n",
    "        # Load bundle\n",
    "        bundle = torch.load(models_bundle_path, map_location='cpu')\n",
    "        print(\"Bundle loaded successfully\")\n",
    "        print(f\"Available models in bundle: {list(bundle.keys())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading bundle: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Deserialize function with multiple fallbacks\n",
    "    def deserialize_scaler(scaler_bytes):\n",
    "        import joblib\n",
    "        import pickle\n",
    "        from io import BytesIO\n",
    "        \n",
    "        methods = [\n",
    "            lambda: joblib.load(BytesIO(scaler_bytes)),\n",
    "            lambda: pickle.load(BytesIO(scaler_bytes)),\n",
    "            lambda: scaler_bytes  # In case it's already an object\n",
    "        ]\n",
    "        \n",
    "        for i, method in enumerate(methods, 1):\n",
    "            try:\n",
    "                result = method()\n",
    "                print(f\"Scaler deserialization method {i} succeeded\")\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                print(f\"Scaler deserialization method {i} failed: {e}\")\n",
    "        \n",
    "        print(\"All scaler deserialization methods failed\")\n",
    "        return None\n",
    "    \n",
    "    # Load models and scalers\n",
    "    models = {}\n",
    "    scalers = {}\n",
    "    \n",
    "    for h_name in ['h2', 'h6']:  # Only need h2 and h6 for h3 interpolation\n",
    "        if h_name in bundle:\n",
    "            print(f\"\\nLoading {h_name} model...\")\n",
    "            h_data = bundle[h_name]\n",
    "            \n",
    "            try:\n",
    "                # Load model\n",
    "                input_size = h_data.get('feature_dim')\n",
    "                output_size = h_data.get('num_sensors')\n",
    "                \n",
    "                print(f\"{h_name} - Input size: {input_size}, Output size: {output_size}\")\n",
    "                \n",
    "                if input_size is None or output_size is None:\n",
    "                    print(f\"Missing dimensions for {h_name}, trying to infer from state_dict...\")\n",
    "                    state_dict = h_data['model_state_dict']\n",
    "                    if 'input_layer.weight' in state_dict:\n",
    "                        input_size = state_dict['input_layer.weight'].shape[1]\n",
    "                    if 'output_layer.weight' in state_dict:\n",
    "                        output_size = state_dict['output_layer.weight'].shape[0]\n",
    "                    print(f\"Inferred - Input size: {input_size}, Output size: {output_size}\")\n",
    "                \n",
    "                # Create model (using the class definition from your code)\n",
    "                model = EnhancedThermalNet(\n",
    "                    input_size=input_size,\n",
    "                    output_size=output_size,\n",
    "                    hidden_dims=[512, 256, 256, 128],\n",
    "                    dropout_rate=0.2\n",
    "                )\n",
    "                model.load_state_dict(h_data['model_state_dict'])\n",
    "                model.eval()\n",
    "                models[h_name] = model\n",
    "                print(f\"âœ“ {h_name} model loaded successfully\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Failed to load {h_name} model: {e}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Load scalers\n",
    "                X_scaler = deserialize_scaler(h_data['X_scaler_bytes'])\n",
    "                y_scaler = deserialize_scaler(h_data['y_scaler_bytes'])\n",
    "                \n",
    "                if X_scaler is None or y_scaler is None:\n",
    "                    print(f\"Failed to deserialize scalers for {h_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Get feature information\n",
    "                feature_cols = h_data.get('feature_cols', [])\n",
    "                tc_cols = h_data.get('tc_cols', [])\n",
    "                \n",
    "                # If feature_cols is empty, create default based on input_size\n",
    "                if not feature_cols and hasattr(X_scaler, 'n_features_in_'):\n",
    "                    print(f\"Creating default feature columns for {h_name}\")\n",
    "                    expected_features = X_scaler.n_features_in_\n",
    "                    \n",
    "                    tc_count = {'h2': 3, 'h3': 5, 'h6': 10}[h_name]\n",
    "                    \n",
    "                    # Create standard feature set\n",
    "                    feature_cols = [\n",
    "                        'Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos',\n",
    "                        'h', 'flux', 'abs', 'surf',\n",
    "                        'flux_abs_interaction', 'h_flux_interaction'\n",
    "                    ]\n",
    "                    \n",
    "                    # Add theoretical temperatures\n",
    "                    for i in range(tc_count):\n",
    "                        feature_cols.append(f'Theoretical_Temps_{i+1}')\n",
    "                    \n",
    "                    # Adjust to match expected count\n",
    "                    if len(feature_cols) != expected_features:\n",
    "                        print(f\"Adjusting feature count: {len(feature_cols)} -> {expected_features}\")\n",
    "                        if len(feature_cols) > expected_features:\n",
    "                            feature_cols = feature_cols[:expected_features]\n",
    "                        else:\n",
    "                            while len(feature_cols) < expected_features:\n",
    "                                feature_cols.append(f'extra_feature_{len(feature_cols)}')\n",
    "                \n",
    "                # Create tc_cols if empty\n",
    "                if not tc_cols:\n",
    "                    output_count = output_size if output_size else 3\n",
    "                    tc_cols = [f'TC{i+1}' if i == 0 else f'TC{i+1}' for i in range(output_count)]\n",
    "                    if output_count > 0:\n",
    "                        tc_cols[0] = 'TC1_tip'  # First sensor is usually the tip\n",
    "                \n",
    "                scalers[h_name] = {\n",
    "                    'X_scaler': X_scaler,\n",
    "                    'y_scaler': y_scaler,\n",
    "                    'time_range': h_data.get('time_range', {'time_min': 0, 'time_max': 7200}),\n",
    "                    'feature_cols': feature_cols,\n",
    "                    'tc_cols': tc_cols\n",
    "                }\n",
    "                \n",
    "                print(f\"âœ“ {h_name} scalers loaded successfully\")\n",
    "                print(f\"  Features: {len(feature_cols)} columns\")\n",
    "                print(f\"  Outputs: {len(tc_cols)} sensors\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Failed to load {h_name} scalers: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"âœ— {h_name} not found in bundle\")\n",
    "    \n",
    "    if len(models) == 0:\n",
    "        print(\"No models loaded successfully\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded {len(models)} models: {list(models.keys())}\")\n",
    "    return EnhancedThermalInterpolator(models, scalers)\n",
    "\n",
    "def run_enhanced_interpolation_test():\n",
    "    \"\"\"\n",
    "    Test the enhanced interpolation system\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ENHANCED H3 INTERPOLATION TEST\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Create interpolation system\n",
    "        interpolator = create_enhanced_interpolation_system()\n",
    "        \n",
    "        # Test parameters\n",
    "        time = 546\n",
    "        flux = 25900\n",
    "        abs_val = 90\n",
    "        surf = 0.98\n",
    "        theoretical_temps = [\n",
    "            439.813583498074,478.199097590388,459.686133516536,413.671627498018,370.724663127373,340.122901242449,321.377323650719,311.186898878393,306.567327821688,305.726791259644\n",
    "        ]\n",
    "        # Test different interpolation methods\n",
    "        methods = ['physics_weighted', 'thermal_diffusion', 'linear']\n",
    "        \n",
    "        results = {}\n",
    "        for method in methods:\n",
    "            print(f\"\\n--- {method.upper()} INTERPOLATION ---\")\n",
    "            \n",
    "            result = interpolator.physics_aware_interpolation(\n",
    "                time=time,\n",
    "                target_h=0.084,  # h3\n",
    "                flux=flux,\n",
    "                abs_val=abs_val,\n",
    "                surf=surf,\n",
    "                theoretical_temps=theoretical_temps,\n",
    "                method=method\n",
    "            )\n",
    "            \n",
    "            if result and 'predicted_temperatures' in result:\n",
    "                results[method] = result\n",
    "                temps = result['predicted_temperatures']\n",
    "                \n",
    "                print(\"Predicted temperatures:\")\n",
    "                for sensor, temp in temps.items():\n",
    "                    print(f\"  {sensor}: {temp:.2f} Â°C\")\n",
    "                \n",
    "                if 'interpolation_info' in result:\n",
    "                    info = result['interpolation_info']\n",
    "                    print(f\"Method info: {info}\")\n",
    "        \n",
    "        # Compare methods\n",
    "        if len(results) > 1:\n",
    "            print(f\"\\n--- METHOD COMPARISON ---\")\n",
    "            sensors = ['TC1_tip', 'TC2', 'TC3','TC4', 'TC5']\n",
    "            \n",
    "            print(f\"{'Sensor':<10} \" + \" \".join([f\"{method:<12}\" for method in methods]))\n",
    "            print(\"-\" * (10 + 13 * len(methods)))\n",
    "            \n",
    "            for sensor in sensors:\n",
    "                row = f\"{sensor:<10} \"\n",
    "                for method in methods:\n",
    "                    if method in results and sensor in results[method]['predicted_temperatures']:\n",
    "                        temp = results[method]['predicted_temperatures'][sensor]\n",
    "                        row += f\"{temp:<12.2f} \"\n",
    "                    else:\n",
    "                        row += f\"{'N/A':<12} \"\n",
    "                print(row)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Enhanced interpolation test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Validation system\n",
    "class InterpolationValidator:\n",
    "    \"\"\"Validate interpolation accuracy against known h3 data\"\"\"\n",
    "    \n",
    "    def __init__(self, interpolator, h3_model=None, h3_scalers=None):\n",
    "        self.interpolator = interpolator\n",
    "        self.h3_model = h3_model\n",
    "        self.h3_scalers = h3_scalers\n",
    "    \n",
    "    def validate_interpolation(self, test_cases, methods=['physics_weighted', 'linear']):\n",
    "        \"\"\"\n",
    "        Validate interpolation against direct h3 predictions\n",
    "        \n",
    "        Args:\n",
    "            test_cases: List of test parameter dictionaries\n",
    "            methods: List of interpolation methods to test\n",
    "        \"\"\"\n",
    "        \n",
    "        validation_results = {method: {'errors': [], 'predictions': []} for method in methods}\n",
    "        \n",
    "        for i, test_case in enumerate(test_cases):\n",
    "            print(f\"\\nTest case {i+1}/{len(test_cases)}\")\n",
    "            \n",
    "            # Get direct h3 prediction (if available)\n",
    "            if self.h3_model and self.h3_scalers:\n",
    "                direct_h3 = self._predict_h3_direct(test_case)\n",
    "            else:\n",
    "                direct_h3 = None\n",
    "            \n",
    "            # Get interpolated predictions\n",
    "            for method in methods:\n",
    "                interp_result = self.interpolator.physics_aware_interpolation(\n",
    "                    time=test_case['time'],\n",
    "                    target_h=0.084,\n",
    "                    flux=test_case['flux'],\n",
    "                    abs_val=test_case['abs'],\n",
    "                    surf=test_case['surf'],\n",
    "                    theoretical_temps=test_case['theoretical_temps'],\n",
    "                    method=method\n",
    "                )\n",
    "                \n",
    "                if interp_result and direct_h3:\n",
    "                    # Calculate errors\n",
    "                    errors = {}\n",
    "                    interp_temps = interp_result['predicted_temperatures']\n",
    "                    \n",
    "                    for sensor in ['TC1_tip', 'TC2', 'TC3','TC4', 'TC5']:\n",
    "                        if sensor in interp_temps and sensor in direct_h3:\n",
    "                            error = abs(interp_temps[sensor] - direct_h3[sensor])\n",
    "                            errors[sensor] = error\n",
    "                    \n",
    "                    validation_results[method]['errors'].append(errors)\n",
    "                    validation_results[method]['predictions'].append(interp_temps)\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary = {}\n",
    "        for method in methods:\n",
    "            errors = validation_results[method]['errors']\n",
    "            if errors:\n",
    "                all_errors = []\n",
    "                for error_dict in errors:\n",
    "                    all_errors.extend(error_dict.values())\n",
    "                \n",
    "                summary[method] = {\n",
    "                    'mean_absolute_error': np.mean(all_errors),\n",
    "                    'max_error': np.max(all_errors),\n",
    "                    'std_error': np.std(all_errors),\n",
    "                    'num_predictions': len(all_errors)\n",
    "                }\n",
    "        \n",
    "        return validation_results, summary\n",
    "    \n",
    "    def _predict_h3_direct(self, test_case):\n",
    "        \"\"\"Direct h3 prediction for validation\"\"\"\n",
    "        # This would use your actual h3 model if available\n",
    "        # For now, return None as placeholder\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Run enhanced interpolation test\n",
    "    results = run_enhanced_interpolation_test()\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ENHANCED INTERPOLATION COMPLETED\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # The physics_weighted method should give better results\n",
    "        # by accounting for thermal penetration and material properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dcc8430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_vertical_profile(predicted, actual, filename=\"Sample Profile\"):\n",
    "    # Reverse order so TC10 (surface) is at the top\n",
    "    predicted = predicted[::-1]\n",
    "    actual = actual[::-1]\n",
    "    sensor_labels = [f\"TC{i}\" for i in range(10, 0, -1)]  # TC10 to TC1\n",
    "\n",
    "    total_height = 0.1575  # Total receiver height in meters\n",
    "    spacing = total_height / 9\n",
    "    depths = [0 - i * spacing for i in range(10)]  # TC10 at 0.0, TC1 at -total_height\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.plot(actual, depths, label=\"Actual\", color='blue', marker='o', linewidth=2)\n",
    "    plt.plot(predicted, depths, label=\"Predicted\", color='red', marker='x', linestyle='--', linewidth=2)\n",
    "\n",
    "    plt.gca().invert_yaxis()  # So 0 (surface) is at top\n",
    "\n",
    "    # Set clean numeric y-ticks\n",
    "    plt.yticks(depths, [f\"{d:.3f}\" for d in depths])\n",
    "    plt.ylim(min(depths) - spacing * 0.5, max(depths) + spacing * 0.5)\n",
    "\n",
    "    plt.xlabel(\"Temperature (Â°C)\")\n",
    "    plt.ylabel(\"Depth (m)\")\n",
    "    plt.title(f\"Vertical Profile: {filename}\")\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Add sensor labels next to each point\n",
    "    for i, label in enumerate(sensor_labels):\n",
    "        plt.text(\n",
    "            actual[i], depths[i], label,\n",
    "            ha='right', va='center', fontsize=8,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#cce6ff\", ec=\"navy\", lw=1, alpha=0.7)\n",
    "        )\n",
    "        plt.text(\n",
    "            predicted[i], depths[i], label,\n",
    "            ha='left', va='center', fontsize=8,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#ffcccc\", ec=\"darkred\", lw=1, alpha=0.7)\n",
    "        )\n",
    "\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{filename}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# actual = [361.36,360.52,360.93,362.1,361.66,363.36,361.29,361.44,360.32,337.33]\n",
    "# pred = [361.057861328125, 359.50201416015625, 360.8453063964844, 360.733642578125, 360.468505859375, 362.0776062011719, 360.5418395996094, 361.2062683105469, 358.7239990234375, 338.3153076171875]\n",
    "\n",
    "actual=[350.66,354.42,362.85,340,340,340,340,340,340,340]\n",
    "pred=[356.88,353.31,335.30,340,340,340,340,340,340,340]\n",
    "\n",
    "plot_vertical_profile(pred, actual, filename=\"h3_flux88_abs90_wr_surf0_529s-546s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164e4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data directory: data/new_processed_reset\n",
      "\n",
      "=== Running Standalone Test Cross-Check ===\n",
      "Loading data from: data/new_processed_reset\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_699s - Sheet1_processed.csv (717 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_wr_surf0_215s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_mr_surf0_617s - Sheet1_processed.csv (532 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_probeUp_590s - Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_mr_surf0_796s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_longRun_780s - Sheet1_processed.csv (4621 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_754s - Sheet3_processed.csv (566 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_648s - Sheet3_processed.csv (621 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_630s - Sheet2_processed.csv (618 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_wr_surf0 _835s- Sheet1_processed.csv (516 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_675s - Sheet3_processed.csv (856 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_newSalt_398s - Sheet1_processed.csv (826 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_wr_surf0_632s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_466s - Sheet1_processed.csv (795 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_surf0_probeUp_193s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_868s - Sheet2_processed.csv (932 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_585s - Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_598s - Sheet1_processed.csv (528 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_surf0_439s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_450s - Sheet3_processed.csv (912 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_719s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet2_processed.csv (1087 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_644s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_868s - Sheet1_processed.csv (790 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_mr_surf0_635s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_749s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_429s - Sheet2_processed.csv (880 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_800s - Sheet1_processed.csv (585 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_longRun_612s - Sheet2_processed.csv (4247 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_689s - Sheet1_processed.csv (611 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_851s - Sheet2_processed.csv (554 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_newSalt_wr_surf0_123s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_surf0_660s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_692s-Sheet2_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again3_344s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs92_mr_surf0_580s - Sheet1_processed.csv (589 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_surf0_431s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_790s - Sheet1_processed.csv (970 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_newSalt_641s - Sheet1_processed.csv (803 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_689s - Sheet3_processed.csv (1411 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_830s - Sheet2_processed.csv (528 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_523s - Sheet2_processed.csv (787 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_surf0_493s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_wr_surf0_393s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux73_abs0_surf1_660s - Sheet1_processed.csv (531 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_wr_surf0_368s - Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_640s-Sheet3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_491s - Sheet3_processed.csv (702 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_graphDisc_surf0_759s - Sheet1_processed.csv (547 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs92_surf0_115s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_558s - Sheet2_processed.csv (3952 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again_266s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs0_mr_surf0_571s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_651s - Sheet4_processed.csv (554 rows)\n",
      "Loaded: cleaned_h6_flux73_abs0_surf0_511s - Sheet3_processed.csv (865 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_wr_surf0_746s-Sheet2_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_643s - Sheet3_processed.csv (870 rows)\n",
      "Loaded: cleaned_h6_flux78_abs0_surf1_newSalt_505s - Sheet3_processed.csv (550 rows)\n",
      "Skipping (not h=0.1575): cleaned_h2_flux88_abs25_newSalt_surf0_172s - Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_wr_surf0_734s - Sheet1_processed.csv (514 rows)\n",
      "Loaded: cleaned_h6_flux88_abs92_surf0_628s - Sheet4_processed.csv (539 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_389s - Sheet3_processed.csv (537 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs25_wr_surf0_422s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_389s - Sheet4_processed.csv (2139 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_wr_surf0_529s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux78_abs0_surf0_745s - Sheet1_processed.csv (887 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_surf0_825s-Sheer3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_newSalt_573s - Sheet2_processed.csv (790 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_Redone_again2_398s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_surf1_613s - Sheet1_processed.csv (550 rows)\n",
      "Loaded: cleaned_h6_flux88_abs0_surf1_545s - Sheet2_processed.csv (794 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_surf0_747s-Sheet3_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs0_surf0_longRun_762s - Sheet1_processed.csv (4087 rows)\n",
      "Loaded: cleaned_h6_flux88_abs20_surf0_781s - Sheet2_processed.csv (849 rows)\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs0_wr_surf0_416s-Sheet1_processed.csv\n",
      "Skipping (not h=0.1575): cleaned_h3_flux88_abs90_mr_surf0_506s-Sheet1_processed.csv\n",
      "Loaded: cleaned_h6_flux88_abs20_mr_surf0_537s - Sheet1_processed.csv (534 rows)\n",
      "\n",
      "Total data loaded: 48187 rows, 26 columns\n",
      "Starting data preprocessing...\n",
      "Features: 21, Targets: 10\n",
      "Theory columns: 10, TC columns: 10\n",
      "Feature columns: ['Time_norm', 'TimeÂ²', 'TimeÂ³', 'Time_sin', 'Time_cos', 'h', 'flux', 'abs', 'surf', 'flux_abs_interaction', 'h_flux_interaction', 'Theoretical_Temps_1', 'Theoretical_Temps_2', 'Theoretical_Temps_3', 'Theoretical_Temps_4', 'Theoretical_Temps_5', 'Theoretical_Temps_6', 'Theoretical_Temps_7', 'Theoretical_Temps_8', 'Theoretical_Temps_9', 'Theoretical_Temps_10']\n",
      "Target columns: ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10']\n",
      "After removing missing values: 48187 samples\n",
      "Training samples: 33730\n",
      "Validation samples: 7228\n",
      "Test samples: 7229\n",
      "\n",
      "=== Cross-Checking 20 Test Samples ===\n",
      "\n",
      "Test Sample 5329:\n",
      "Inputs: Time=1631.00s, h=0.1575, flux=25900, abs=20, surf=0.98\n",
      "Theoretical Temps: ['400.89', '433.32', '433.32', '433.30', '420.60', '398.60', '380.57', '374.79', '374.79', '374.79']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         354.513         354.420         0.093          \n",
      "TC2             356.382         356.340         0.042          \n",
      "TC3             364.416         364.160         0.256          \n",
      "TC4             372.895         372.510         0.385          \n",
      "TC5             377.492         377.270         0.222          \n",
      "TC6             384.512         384.030         0.482          \n",
      "TC7             385.562         385.110         0.452          \n",
      "TC8             387.721         386.960         0.761          \n",
      "TC9             389.416         388.650         0.766          \n",
      "TC10            377.838         377.090         0.748          \n",
      "\n",
      "Test Sample 6382:\n",
      "Inputs: Time=315.00s, h=0.1575, flux=25900, abs=20, surf=0.98\n",
      "Theoretical Temps: ['348.50', '373.94', '373.94', '371.97', '358.60', '344.10', '333.61', '330.17', '330.17', '330.17']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         354.536         353.820         0.716          \n",
      "TC2             354.537         354.700         0.163          \n",
      "TC3             358.763         359.400         0.637          \n",
      "TC4             361.627         362.290         0.663          \n",
      "TC5             363.146         363.620         0.474          \n",
      "TC6             366.982         367.320         0.338          \n",
      "TC7             367.030         366.920         0.110          \n",
      "TC8             368.754         368.520         0.234          \n",
      "TC9             369.947         370.220         0.273          \n",
      "TC10            357.335         358.750         1.415          \n",
      "\n",
      "Test Sample 3900:\n",
      "Inputs: Time=734.00s, h=0.1575, flux=25900, abs=3, surf=0.76\n",
      "Theoretical Temps: ['350.56', '371.79', '371.79', '371.79', '371.79', '371.79', '371.79', '371.79', '371.79', '371.79']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         365.407         365.500         0.093          \n",
      "TC2             363.651         363.720         0.069          \n",
      "TC3             366.195         366.050         0.145          \n",
      "TC4             366.379         366.340         0.039          \n",
      "TC5             366.563         366.170         0.393          \n",
      "TC6             368.545         368.400         0.145          \n",
      "TC7             368.067         367.670         0.397          \n",
      "TC8             369.058         368.670         0.388          \n",
      "TC9             368.679         368.810         0.131          \n",
      "TC10            357.300         355.240         2.060          \n",
      "\n",
      "Test Sample 1027:\n",
      "Inputs: Time=294.00s, h=0.1575, flux=25900, abs=100, surf=0.98\n",
      "Theoretical Temps: ['432.94', '447.33', '360.64', '316.88', '303.87', '300.82', '300.17', '300.04', '300.01', '300.00']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         349.085         349.380         0.295          \n",
      "TC2             349.955         350.640         0.685          \n",
      "TC3             353.788         354.150         0.362          \n",
      "TC4             354.871         355.890         1.019          \n",
      "TC5             355.761         355.770         0.009          \n",
      "TC6             358.365         358.410         0.045          \n",
      "TC7             357.724         356.950         0.774          \n",
      "TC8             360.795         359.970         0.825          \n",
      "TC9             381.318         380.420         0.898          \n",
      "TC10            383.946         381.930         2.016          \n",
      "\n",
      "Test Sample 3516:\n",
      "Inputs: Time=355.00s, h=0.1575, flux=25900, abs=20, surf=0.76\n",
      "Theoretical Temps: ['366.53', '389.44', '389.44', '382.16', '364.39', '348.04', '336.66', '332.99', '332.99', '332.99']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         352.549         352.400         0.149          \n",
      "TC2             352.681         353.350         0.669          \n",
      "TC3             357.444         357.760         0.316          \n",
      "TC4             360.709         361.070         0.361          \n",
      "TC5             362.597         362.370         0.227          \n",
      "TC6             367.412         367.390         0.022          \n",
      "TC7             368.306         368.400         0.094          \n",
      "TC8             371.268         371.690         0.422          \n",
      "TC9             375.453         376.290         0.837          \n",
      "TC10            365.775         364.950         0.825          \n",
      "\n",
      "Test Sample 6744:\n",
      "Inputs: Time=228.00s, h=0.1575, flux=25900, abs=100, surf=0.98\n",
      "Theoretical Temps: ['431.75', '445.66', '359.43', '316.40', '303.74', '300.79', '300.16', '300.03', '300.01', '300.00']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         351.278         350.100         1.178          \n",
      "TC2             351.800         351.420         0.380          \n",
      "TC3             355.175         355.480         0.305          \n",
      "TC4             355.938         356.890         0.952          \n",
      "TC5             356.580         356.770         0.190          \n",
      "TC6             358.815         359.080         0.265          \n",
      "TC7             357.814         358.060         0.246          \n",
      "TC8             360.074         360.020         0.054          \n",
      "TC9             377.538         377.690         0.152          \n",
      "TC10            378.142         379.980         1.838          \n",
      "\n",
      "Test Sample 6423:\n",
      "Inputs: Time=1240.00s, h=0.1575, flux=25900, abs=3, surf=0.98\n",
      "Theoretical Temps: ['359.58', '386.40', '386.40', '386.40', '386.40', '386.40', '386.40', '386.40', '386.40', '386.40']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         368.607         367.930         0.677          \n",
      "TC2             366.479         365.330         1.149          \n",
      "TC3             369.489         368.580         0.909          \n",
      "TC4             369.630         368.900         0.730          \n",
      "TC5             369.751         368.640         1.111          \n",
      "TC6             370.929         370.150         0.779          \n",
      "TC7             370.931         370.150         0.781          \n",
      "TC8             371.826         369.670         2.156          \n",
      "TC9             370.003         368.340         1.663          \n",
      "TC10            363.076         361.510         1.566          \n",
      "\n",
      "Test Sample 6321:\n",
      "Inputs: Time=425.00s, h=0.1575, flux=25900, abs=100, surf=0.98\n",
      "Theoretical Temps: ['440.67', '458.25', '369.00', '320.40', '304.88', '301.05', '300.22', '300.05', '300.01', '300.00']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         345.605         345.630         0.025          \n",
      "TC2             347.053         347.210         0.157          \n",
      "TC3             351.585         352.320         0.735          \n",
      "TC4             353.299         354.670         1.371          \n",
      "TC5             354.540         355.090         0.550          \n",
      "TC6             358.064         358.140         0.076          \n",
      "TC7             358.300         357.920         0.380          \n",
      "TC8             363.278         363.140         0.138          \n",
      "TC9             388.127         388.430         0.303          \n",
      "TC10            391.869         392.650         0.781          \n",
      "\n",
      "Test Sample 1987:\n",
      "Inputs: Time=638.00s, h=0.1575, flux=25900, abs=3, surf=0.76\n",
      "Theoretical Temps: ['334.36', '353.98', '353.98', '353.98', '353.98', '353.98', '353.98', '353.98', '353.98', '353.98']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         363.311         363.970         0.659          \n",
      "TC2             362.173         363.310         1.137          \n",
      "TC3             364.158         365.230         1.072          \n",
      "TC4             364.517         365.300         0.783          \n",
      "TC5             364.303         364.680         0.377          \n",
      "TC6             365.818         366.660         0.842          \n",
      "TC7             364.030         364.510         0.480          \n",
      "TC8             364.440         365.070         0.630          \n",
      "TC9             359.918         360.700         0.782          \n",
      "TC10            336.963         334.970         1.993          \n",
      "\n",
      "Test Sample 2543:\n",
      "Inputs: Time=275.00s, h=0.1575, flux=25900, abs=100, surf=0.98\n",
      "Theoretical Temps: ['446.89', '467.28', '376.47', '323.82', '305.94', '301.31', '300.28', '300.06', '300.01', '300.00']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         351.982         352.350         0.368          \n",
      "TC2             352.480         353.480         1.000          \n",
      "TC3             355.629         355.900         0.271          \n",
      "TC4             356.401         357.640         1.239          \n",
      "TC5             357.083         357.420         0.337          \n",
      "TC6             359.810         359.810         0.000          \n",
      "TC7             358.905         358.040         0.865          \n",
      "TC8             361.906         360.980         0.926          \n",
      "TC9             379.928         381.360         1.432          \n",
      "TC10            377.303         375.530         1.773          \n",
      "\n",
      "Test Sample 4309:\n",
      "Inputs: Time=268.00s, h=0.1575, flux=25900, abs=3, surf=0.76\n",
      "Theoretical Temps: ['318.64', '336.78', '336.78', '336.78', '336.78', '336.78', '336.78', '336.78', '336.78', '336.78']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         363.200         363.780         0.580          \n",
      "TC2             361.452         361.830         0.378          \n",
      "TC3             363.620         364.020         0.400          \n",
      "TC4             363.760         364.470         0.710          \n",
      "TC5             363.783         364.340         0.557          \n",
      "TC6             365.515         366.540         1.025          \n",
      "TC7             364.814         365.190         0.376          \n",
      "TC8             365.622         366.250         0.628          \n",
      "TC9             364.951         366.580         1.629          \n",
      "TC10            352.519         352.380         0.139          \n",
      "\n",
      "Test Sample 5859:\n",
      "Inputs: Time=1021.00s, h=0.1575, flux=25900, abs=20, surf=0.98\n",
      "Theoretical Temps: ['379.51', '408.95', '408.95', '408.24', '393.61', '373.61', '358.22', '353.31', '353.31', '353.31']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         352.159         352.230         0.071          \n",
      "TC2             353.780         353.940         0.160          \n",
      "TC3             361.107         361.210         0.103          \n",
      "TC4             368.398         368.160         0.238          \n",
      "TC5             372.198         371.950         0.248          \n",
      "TC6             378.321         377.380         0.941          \n",
      "TC7             378.917         377.820         1.097          \n",
      "TC8             380.608         379.320         1.288          \n",
      "TC9             382.007         380.550         1.457          \n",
      "TC10            370.375         368.570         1.805          \n",
      "\n",
      "Test Sample 1644:\n",
      "Inputs: Time=230.00s, h=0.1575, flux=25900, abs=3, surf=0.98\n",
      "Theoretical Temps: ['323.99', '346.54', '346.54', '346.54', '346.54', '346.54', '346.54', '346.54', '346.54', '346.54']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         361.289         360.570         0.719          \n",
      "TC2             359.635         360.430         0.794          \n",
      "TC3             361.582         360.670         0.912          \n",
      "TC4             361.736         362.120         0.385          \n",
      "TC5             361.640         361.370         0.270          \n",
      "TC6             363.099         362.370         0.729          \n",
      "TC7             362.327         362.090         0.237          \n",
      "TC8             363.208         363.830         0.622          \n",
      "TC9             360.701         359.240         1.461          \n",
      "TC10            345.552         346.830         1.278          \n",
      "\n",
      "Test Sample 6837:\n",
      "Inputs: Time=423.00s, h=0.1575, flux=25900, abs=100, surf=0.98\n",
      "Theoretical Temps: ['443.50', '462.34', '372.31', '321.88', '305.33', '301.16', '300.24', '300.05', '300.01', '300.00']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         346.016         344.870         1.146          \n",
      "TC2             347.452         346.660         0.792          \n",
      "TC3             351.887         352.290         0.403          \n",
      "TC4             353.559         354.630         1.071          \n",
      "TC5             354.787         354.970         0.183          \n",
      "TC6             358.314         358.220         0.094          \n",
      "TC7             358.444         358.410         0.034          \n",
      "TC8             363.381         363.500         0.119          \n",
      "TC9             388.089         388.410         0.321          \n",
      "TC10            391.041         394.240         3.199          \n",
      "\n",
      "Test Sample 4060:\n",
      "Inputs: Time=1433.00s, h=0.1575, flux=25900, abs=3, surf=0.98\n",
      "Theoretical Temps: ['359.20', '385.97', '385.97', '385.97', '385.97', '385.97', '385.97', '385.97', '385.97', '385.97']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         369.236         368.650         0.586          \n",
      "TC2             367.551         367.040         0.511          \n",
      "TC3             370.325         369.720         0.605          \n",
      "TC4             370.508         370.190         0.318          \n",
      "TC5             370.629         369.840         0.789          \n",
      "TC6             372.180         371.710         0.470          \n",
      "TC7             371.873         371.700         0.173          \n",
      "TC8             372.289         371.750         0.539          \n",
      "TC9             371.091         369.710         1.381          \n",
      "TC10            362.646         361.070         1.576          \n",
      "\n",
      "Test Sample 4460:\n",
      "Inputs: Time=1130.00s, h=0.1575, flux=25900, abs=3, surf=0.98\n",
      "Theoretical Temps: ['353.19', '379.21', '379.21', '379.21', '379.21', '379.21', '379.21', '379.21', '379.21', '379.21']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         344.491         345.640         1.149          \n",
      "TC2             344.185         347.180         2.995          \n",
      "TC3             345.952         346.300         0.348          \n",
      "TC4             347.301         348.550         1.249          \n",
      "TC5             347.849         348.620         0.771          \n",
      "TC6             349.345         350.120         0.775          \n",
      "TC7             349.771         351.010         1.239          \n",
      "TC8             351.816         353.440         1.624          \n",
      "TC9             350.143         351.690         1.547          \n",
      "TC10            337.642         340.260         2.618          \n",
      "\n",
      "Test Sample 1435:\n",
      "Inputs: Time=766.00s, h=0.1575, flux=21250, abs=3, surf=0.98\n",
      "Theoretical Temps: ['325.72', '348.46', '348.46', '348.46', '348.46', '348.46', '348.46', '348.46', '348.46', '348.46']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         353.522         350.820         2.702          \n",
      "TC2             352.396         350.380         2.016          \n",
      "TC3             353.772         351.480         2.292          \n",
      "TC4             353.812         351.650         2.162          \n",
      "TC5             353.424         351.140         2.284          \n",
      "TC6             355.064         353.130         1.934          \n",
      "TC7             353.203         351.180         2.023          \n",
      "TC8             353.642         351.120         2.522          \n",
      "TC9             349.003         346.100         2.903          \n",
      "TC10            324.686         318.670         6.016          \n",
      "\n",
      "Test Sample 1454:\n",
      "Inputs: Time=144.00s, h=0.1575, flux=25900, abs=3, surf=0.76\n",
      "Theoretical Temps: ['316.82', '334.79', '334.79', '334.79', '334.79', '334.79', '334.79', '334.79', '334.79', '334.79']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         362.406         363.590         1.184          \n",
      "TC2             360.794         362.050         1.256          \n",
      "TC3             362.543         362.340         0.203          \n",
      "TC4             362.531         362.910         0.379          \n",
      "TC5             362.455         362.590         0.135          \n",
      "TC6             364.223         364.380         0.157          \n",
      "TC7             363.023         362.000         1.023          \n",
      "TC8             363.825         363.630         0.195          \n",
      "TC9             362.781         363.300         0.519          \n",
      "TC10            346.108         347.220         1.112          \n",
      "\n",
      "Test Sample 2776:\n",
      "Inputs: Time=709.00s, h=0.1575, flux=19400, abs=3, surf=0.76\n",
      "Theoretical Temps: ['332.60', '352.05', '352.05', '352.05', '352.05', '352.05', '352.05', '352.05', '352.05', '352.05']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         355.090         355.000         0.090          \n",
      "TC2             354.165         354.860         0.695          \n",
      "TC3             355.964         356.150         0.186          \n",
      "TC4             356.192         356.630         0.438          \n",
      "TC5             355.874         356.250         0.376          \n",
      "TC6             357.702         358.180         0.478          \n",
      "TC7             355.751         356.000         0.249          \n",
      "TC8             355.926         356.540         0.614          \n",
      "TC9             351.804         352.960         1.156          \n",
      "TC10            328.412         329.030         0.618          \n",
      "\n",
      "Test Sample 5969:\n",
      "Inputs: Time=365.00s, h=0.1575, flux=19400, abs=3, surf=0.76\n",
      "Theoretical Temps: ['316.56', '334.51', '334.51', '334.51', '334.51', '334.51', '334.51', '334.51', '334.51', '334.51']\n",
      "Predicted vs Actual Temperatures:\n",
      "--------------------------------------------------\n",
      "Sensor          Predicted (Â°C)  Actual (Â°C)     Error (Â°C)     \n",
      "--------------------------------------------------\n",
      "TC1_tip         359.808         360.380         0.572          \n",
      "TC2             358.249         358.230         0.019          \n",
      "TC3             360.492         360.720         0.228          \n",
      "TC4             360.672         361.090         0.418          \n",
      "TC5             360.650         360.780         0.130          \n",
      "TC6             362.485         362.570         0.085          \n",
      "TC7             361.520         360.910         0.610          \n",
      "TC8             362.138         362.010         0.128          \n",
      "TC9             361.111         361.310         0.199          \n",
      "TC10            347.328         346.840         0.488          \n",
      "\n",
      "=== Average Errors (Test Set) ===\n",
      "Sensor          Avg Error (Â°C) \n",
      "------------------------------\n",
      "TC1_tip         0.653          \n",
      "TC2             0.753          \n",
      "TC3             0.534          \n",
      "TC4             0.746          \n",
      "TC5             0.482          \n",
      "TC6             0.484          \n",
      "TC7             0.582          \n",
      "TC8             0.741          \n",
      "TC9             0.990          \n",
      "TC10            1.693          \n",
      "Plot saved: h0.1575_flux25900.0_abs20.0_surf0.9800000190734863_time1631s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs20.0_surf0.9800000190734863_time315s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.7599999904632568_time734s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs100.0_surf0.9800000190734863_time294s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs20.0_surf0.7599999904632568_time355s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs100.0_surf0.9800000190734863_time228s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.9800000190734863_time1240s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs100.0_surf0.9800000190734863_time425s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.7599999904632568_time638s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs100.0_surf0.9800000190734863_time275s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.7599999904632568_time268s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs20.0_surf0.9800000190734863_time1021s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.9800000190734863_time230s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs100.0_surf0.9800000190734863_time423s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.9800000190734863_time1433s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.9800000190734863_time1130s.png\n",
      "Plot saved: h0.1575_flux21250.0_abs3.0_surf0.9800000190734863_time766s.png\n",
      "Plot saved: h0.1575_flux25900.0_abs3.0_surf0.7599999904632568_time144s.png\n",
      "Plot saved: h0.1575_flux19400.0_abs3.0_surf0.7599999904632568_time709s.png\n",
      "Plot saved: h0.1575_flux19400.0_abs3.0_surf0.7599999904632568_time365s.png\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Block 11: Standalone Testing\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')  # Use non-interactive Agg backend to avoid tkinter dependency\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# def cross_check_test_predictions(data_dir, h_filter=0.1575, num_samples=5, filter_condition=None):\n",
    "#     \"\"\"Cross-check model predictions with rows from the test dataset.\"\"\"\n",
    "#     # Load inference components\n",
    "#     global pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device\n",
    "#     if not all(var in globals() for var in ['pred_model', 'pred_X_scaler', 'pred_y_scaler', \n",
    "#                                            'pred_time_range', 'pred_column_info', 'pred_device']):\n",
    "#         pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info, pred_device = load_for_prediction()\n",
    "#         if pred_model is None:\n",
    "#             raise ValueError(\"Failed to load inference components. Ensure model and scalers are saved.\")\n",
    "\n",
    "#     # Load and preprocess data to get test_loader\n",
    "#     data = load_data(data_dir, h_filter=h_filter)\n",
    "#     train_loader, val_loader, test_loader, X_scaler, y_scaler, tc_cols = preprocess_data(data, use_enhanced_features=True)\n",
    "    \n",
    "#     # Extract test dataset from test_loader\n",
    "#     X_test_scaled = np.concatenate([batch[0].numpy() for batch in test_loader], axis=0)\n",
    "#     y_test_scaled = np.concatenate([batch[1].numpy() for batch in test_loader], axis=0)\n",
    "    \n",
    "#     # Inverse transform to get original feature and target values\n",
    "#     X_test = pred_X_scaler.inverse_transform(X_test_scaled)\n",
    "#     y_test = pred_y_scaler.inverse_transform(y_test_scaled)\n",
    "    \n",
    "#     # Create DataFrame for test data\n",
    "#     feature_cols = pred_column_info['feature_cols']\n",
    "#     test_df = pd.DataFrame(X_test, columns=feature_cols)\n",
    "#     test_df[tc_cols] = y_test\n",
    "    \n",
    "#     # Add original 'Time' (before normalization) for filtering\n",
    "#     time_min = pred_time_range['time_min']\n",
    "#     time_max = pred_time_range['time_max']\n",
    "#     test_df['Time'] = test_df['Time_norm'] * (time_max - time_min) + time_min\n",
    "    \n",
    "#     # Apply filter condition if provided (e.g., specific time or flux)\n",
    "#     if filter_condition is not None:\n",
    "#         test_df = test_df.query(filter_condition)\n",
    "#         if test_df.empty:\n",
    "#             raise ValueError(f\"No test rows match the condition: {filter_condition}\")\n",
    "    \n",
    "#     # Sample rows for cross-checking (no random_state for true randomness)\n",
    "#     num_samples = min(num_samples, len(test_df))\n",
    "#     if num_samples == 0:\n",
    "#         raise ValueError(\"No test samples available after filtering!\")\n",
    "#     sample_rows = test_df.sample(n=num_samples,random_state=33) if filter_condition is None else test_df.head(num_samples)\n",
    "    \n",
    "#     print(f\"\\n=== Cross-Checking {len(sample_rows)} Test Samples ===\")\n",
    "#     results = []\n",
    "    \n",
    "#     for idx, row in sample_rows.iterrows():\n",
    "#         # Prepare input features\n",
    "#         time = row['Time']\n",
    "#         h = row['h']\n",
    "#         flux = row['flux']\n",
    "#         abs_val = row['abs']\n",
    "#         surf = row['surf']\n",
    "#         theoretical_temps = [row[col] for col in [c for c in feature_cols if c.startswith('Theoretical_Temps_')]]\n",
    "        \n",
    "#         # Make prediction\n",
    "#         try:\n",
    "#             pred_result = predict_temperature(\n",
    "#                 pred_model, pred_X_scaler, pred_y_scaler, pred_time_range, pred_column_info,\n",
    "#                 time, h, flux, abs_val, surf, theoretical_temps, pred_device\n",
    "#             )\n",
    "            \n",
    "#             # Get actual temperatures\n",
    "#             actual_temps = {col: row[col] for col in tc_cols}\n",
    "            \n",
    "#             # Compare predictions with actuals\n",
    "#             comparison = {\n",
    "#                 'index': idx,\n",
    "#                 'inputs': {\n",
    "#                     'time': time,\n",
    "#                     'h': h,\n",
    "#                     'flux': flux,\n",
    "#                     'abs': abs_val,\n",
    "#                     'surf': surf,\n",
    "#                     'theoretical_temps': theoretical_temps\n",
    "#                 },\n",
    "#                 'predicted_temps': pred_result['predicted_temperatures'],\n",
    "#                 'actual_temps': actual_temps,\n",
    "#                 'errors': {col: abs(pred_result['predicted_temperatures'][col] - actual_temps[col]) \n",
    "#                           for col in tc_cols}\n",
    "#             }\n",
    "            \n",
    "#             results.append(comparison)\n",
    "            \n",
    "#             # Print comparison\n",
    "#             print(f\"\\nTest Sample {idx}:\")\n",
    "#             print(f\"Inputs: Time={time:.2f}s, h={h:.4f}, flux={flux:.0f}, abs={abs_val:.0f}, surf={surf:.2f}\")\n",
    "#             print(\"Theoretical Temps:\", [f\"{t:.2f}\" for t in theoretical_temps])\n",
    "#             print(\"Predicted vs Actual Temperatures:\")\n",
    "#             print(\"-\" * 50)\n",
    "#             print(f\"{'Sensor':<15} {'Predicted (Â°C)':<15} {'Actual (Â°C)':<15} {'Error (Â°C)':<15}\")\n",
    "#             print(\"-\" * 50)\n",
    "#             for col in tc_cols:\n",
    "#                 pred_temp = pred_result['predicted_temperatures'][col]\n",
    "#                 actual_temp = actual_temps[col]\n",
    "#                 error = comparison['errors'][col]\n",
    "#                 print(f\"{col:<15} {pred_temp:<15.3f} {actual_temp:<15.3f} {error:<15.3f}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing test sample {idx}: {e}\")\n",
    "    \n",
    "#     # Calculate average errors\n",
    "#     if results:\n",
    "#         avg_errors = {col: np.mean([r['errors'][col] for r in results]) for col in tc_cols}\n",
    "#         print(\"\\n=== Average Errors (Test Set) ===\")\n",
    "#         print(f\"{'Sensor':<15} {'Avg Error (Â°C)':<15}\")\n",
    "#         print(\"-\" * 30)\n",
    "#         for col, avg_error in avg_errors.items():\n",
    "#             print(f\"{col:<15} {avg_error:<15.3f}\")\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def run_test_cross_check(data_dir, h_filter=0.1575, num_samples=5, filter_condition=None):\n",
    "#     \"\"\"Run cross-checking on test data without requiring main execution.\"\"\"\n",
    "#     try:\n",
    "#         print(\"\\n=== Running Standalone Test Cross-Check ===\")\n",
    "#         results = cross_check_test_predictions(data_dir, h_filter, num_samples, filter_condition)\n",
    "        \n",
    "#         # Plot results using plot_vertical_profile\n",
    "#         for result in results:\n",
    "#             # Extract predicted and actual temperatures in order TC1_tip to TC10\n",
    "#             tc_cols = ['TC1_tip', 'TC2', 'TC3', 'TC4', 'TC5', 'TC6', 'TC7', 'TC8', 'TC9', 'TC10']\n",
    "#             predicted = [result['predicted_temps'][col] for col in tc_cols]\n",
    "#             actual = [result['actual_temps'][col] for col in tc_cols]\n",
    "#             # Create filename based on input conditions\n",
    "#             filename = (\n",
    "#                 f\"h{h_filter}_flux{result['inputs']['flux']}_\"\n",
    "#                 f\"abs{result['inputs']['abs']}_surf{result['inputs']['surf']}_\"\n",
    "#                 f\"time{result['inputs']['time']:.0f}s\"\n",
    "#             )\n",
    "#             try:\n",
    "#                 plot_vertical_profile(predicted, actual, filename=f\"Sample {result['index']} - {filename}\")\n",
    "#                 print(f\"Plot saved: {filename}.png\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error plotting sample {result['index']}: {e}\")\n",
    "        \n",
    "#         # Save results to CSV\n",
    "#         # pd.DataFrame(results).to_csv('test_cross_check_results.csv')\n",
    "#         # print(\"Results saved to 'test_cross_check_results.csv'\")\n",
    "        \n",
    "#         return results\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in standalone test cross-check: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     DATA_DIR = get_data_directory()\n",
    "#     run_test_cross_check(DATA_DIR, h_filter=0.1575, num_samples=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
